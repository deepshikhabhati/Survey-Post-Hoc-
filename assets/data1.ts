export const data1= `

id:1  reference: P. Linardatos, V. Papastefanopoulos, and S. Kotsiantis, “Explain-
able ai: A review of machine learning interpretability methods,”
Entropy, vol. 23, no. 1, p. 18, 2020.
Bibtex: 
@article linardatos2020explainable,
  title= Explainable ai: A review of machine learning interpretability methods ,
  author= Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris ,
  journal= Entropy ,
  volume= 23 ,
  number= 1 ,
  pages= 18 ,
  year= 2020 ,
  publisher= MDPI 
 
Citation : 2275
Abstract:  Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.
Keywords: xai; machine learning; explainability; interpretability; fairness; sensitivity; black-box
APA: Linardatos, P., Papastefanopoulos, V., & Kotsiantis, S. (2020). Explainable ai: A review of machine learning interpretability methods. Entropy, 23(1), 18.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=Explainable+AI%3A+A+Review+of+Machine+Learning+Interpretability+Methods&btnG=
External link: https://www.mdpi.com/1099-4300/23/1/18
DOI:  https://doi.org/10.3390/e23010018

id:2  reference: A. B. Arrieta, N. D´ıaz-Rodr´ıguez, J. Del Ser, A. Bennetot, S. Tabik,
A. Barbado, S. Garc´ıa, S. Gil-L ´opez, D. Molina, R. Benjamins et al.,
“Explainable artificial intelligence (xai): Concepts, taxonomies,
opportunities and challenges toward responsible ai,” Information
fusion, vol. 58, pp. 82–115, 2020.
Bibtex: 
@article arrieta2020explainable,
  title= Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI ,
  author= Arrieta, Alejandro Barredo and D \'\i az-Rodr \'\i guez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garc \'\i a, Salvador and Gil-L \'o pez, Sergio and Molina, Daniel and Benjamins, Richard and others ,
  journal= Information fusion ,
  volume= 58 ,
  pages= 82--115 ,
  year= 2020 ,
  publisher= Elsevier 
 
Citation : 7987
Abstract:  In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.
Keywords
Explainable Artificial IntelligenceMachine LearningDeep LearningData FusionInterpretabilityComprehensibilityTransparencyPrivacyFairnessAccountabilityResponsible Artificial Intelligence
APA: Arrieta, A. B., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., ... & Herrera, F. (2020). Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information fusion, 58, 82-115.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+A.+B.+Arrieta%2C+N.+D%C2%B4%C4%B1az-Rodr%C2%B4%C4%B1guez%2C+J.+Del+Ser%2C+A.+Bennetot%2C+S.+Tabik%2C+A.+Barbado%2C+S.+Garc%C2%B4%C4%B1a%2C+S.+Gil-L+%C2%B4opez%2C+D.+Molina%2C+R.+Benjamins+et+al.%2C+%E2%80%9CExplainable+artificial+intelligence+%28xai%29%3A+Concepts%2C+taxonomies%2C+opportunities+and+challenges+toward+responsible+ai%2C%E2%80%9D+Information+fusion%2C+vol.+58%2C+pp.+82%E2%80%93115%2C+2020.&btnG=
External link: https://www.sciencedirect.com/science/article/pii/S1566253519308103?casa_token=B5OVfyaKbpMAAAAA:G7e8Ul3cfBzKx4GLnSizTt8VYkwwFIL-FHbvKrh18ws0i5aq7yWAqmki9F6kOw5wA2g8L9bF
DOI:  https://doi.org/10.1016/j.inffus.2019.12.012

id:3  reference: A. Adadi and M. Berrada, “Peeking inside the black-box: a survey
on explainable artificial intelligence (xai),” IEEE access, vol. 6, pp.
52 138–52 160, 2018.
Bibtex: 
@article adadi2018peeking,
  title= Peeking inside the black-box: a survey on explainable artificial intelligence (XAI) ,
  author= Adadi, Amina and Berrada, Mohammed ,
  journal= IEEE access ,
  volume= 6 ,
  pages= 52138--52160 ,
  year= 2018 ,
  publisher= IEEE 
 
Citation : 5679
Abstract:  At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.
Keyword: Explainable artificial intelligence, interpretable machine learning, black-box models. 
APA: Adadi, A., & Berrada, M. (2018). Peeking inside the black-box: a survey on explainable artificial intelligence (XAI). IEEE access, 6, 52138-52160.
Google scholar link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+A.+Adadi+and+M.+Berrada%2C+%E2%80%9CPeeking+inside+the+black-box%3A+a+survey+on+explainable+artificial+intelligence+%28xai%29%2C%E2%80%9D+IEEE+access%2C+vol.+6%2C+pp.+52+138%E2%80%9352+160%2C+2018.&btnG=
External link: https://ieeexplore.ieee.org/abstract/document/8466590
DOI:https://ieeexplore.ieee.org/document/8466590

id:4  reference: G. Alicioglu and B. Sun, “A survey of visual analytics for ex-
plainable artificial intelligence methods,” Computers & Graphics,
vol. 102, pp. 502–520, 2022.
Bibtex: 
@article alicioglu2022survey,
  title= A survey of visual analytics for explainable artificial intelligence methods ,
  author= Alicioglu, Gulsum and Sun, Bo ,
  journal= Computers \& Graphics ,
  volume= 102 ,
  pages= 502--520 ,
  year= 2022 ,
  publisher= Elsevier 
 
Citation : 177
Abstract:  Deep learning (DL) models have achieved impressive performance in various domains such as medicine, finance, and autonomous vehicle systems with advances in computing power and technologies. However, due to the black-box structure of DL models, the decisions of these learning models often need to be explained to end-users. Explainable Artificial Intelligence (XAI) provides explanations of black-box models to reveal the behavior and underlying decision-making mechanisms of the models through tools, techniques, and algorithms. Visualization techniques help to present model and prediction explanations in a more understandable, explainable, and interpretable way. This survey paper aims to review current trends and challenges of visual analytics in interpreting DL models by adopting XAI methods and present future research directions in this area. We reviewed literature based on two different aspects, model usage and visual approaches. We addressed several research questions based on our findings and then discussed missing points, research gaps, and potential future research directions. This survey provides guidelines to develop a better interpretation of neural networks through XAI methods in the field of visual analytics.
Keywords
Explainable Artificial IntelligenceInterpretable neural networksVisual analyticsBlack-box models
APA: Alicioglu, G., & Sun, B. (2022). A survey of visual analytics for explainable artificial intelligence methods. Computers & Graphics, 102, 502-520
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=%5B4%5D+G.+Alicioglu+and+B.+Sun%2C+%E2%80%9CA+survey+of+visual+analytics+for+ex-+plainable+artificial+intelligence+methods%2C%E2%80%9D+Computers+%26+Graphics%2C+vol.+102%2C+pp.+502%E2%80%93520%2C+2022.&btnG=#d=gs_cit&t=1728098290613&u=%2Fscholar%3Fq%3Dinfo%3ALnE6d80lMvsJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Den
External link:
https://www.sciencedirect.com/science/article/pii/S0097849321001886?casa_token=k-WklxEadVIAAAAA:Q0fxn64f7imly9ZDoBlWZUQDAyjBHoXDIB22SD9WDag3fQNjsvcZPnlwywcHgh428Xicoi-h
DOI: https://www.sciencedirect.com/science/article/abs/pii/S0097849321001886?via%3Dihub

id:5  reference: A. Das and P. Rad, “Opportunities and challenges in ex-
plainable artificial intelligence (xai): A survey,” arXiv preprint
arXiv:2006.11371, 2020.
Bibtex: 
@article das2020opportunities,
  title= Opportunities and challenges in explainable artificial intelligence (xai): A survey ,
  author= Das, Arun and Rad, Paul ,
  journal= arXiv preprint arXiv:2006.11371 ,
  year= 2020 
 
Citation : 860
Abstract: Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.
Keywords: explainable ai, xai, interpretable deep learning, machine learning, computer vision, neural network. 
APA: Das, A., & Rad, P. (2020). Opportunities and challenges in explainable artificial intelligence (xai): A survey. arXiv preprint arXiv:2006.11371.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Das+and+P.+Rad%2C+%E2%80%9COpportunities+and+challenges+in+ex-+plainable+artificial+intelligence+%28xai%29%3A+A+survey%2C%E2%80%9D+arXiv+preprint+arXiv%3A2006.11371%2C+2020.&btnG=#d=gs_cit&t=1728098769721&u=%2Fscholar%3Fq%3Dinfo%3AJMFUJCFILOsJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Den
External link:
https://arxiv.org/abs/2006.11371
DOI: 
https://doi.org/10.48550/arXiv.2006.11371
	

id:6  reference: G. Schwalbe and B. Finzel, “A comprehensive taxonomy for
explainable artificial intelligence: a systematic survey of surveys
on methods and concepts,” Data Mining and Knowledge Discovery,
pp. 1–59, 2023.
Bibtex: 
@article schwalbe2023comprehensive,
  title= A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts ,
  author= Schwalbe, Gesina and Finzel, Bettina ,
  journal= Data Mining and Knowledge Discovery ,
  pages= 1--59 ,
  year= 2023 ,
  publisher= Springer 
 
Citation : 171
Abstract: In the meantime, a wide variety of terminologies, motivations, approaches, and evaluation criteria have been developed within the research field of explainable artificial intelligence (XAI). With the amount of XAI methods vastly growing, a taxonomy of methods is needed by researchers as well as practitioners: To grasp the breadth of the topic, compare methods, and to select the right XAI method based on traits required by a specific use-case context. Many taxonomies for XAI methods of varying level of detail and depth can be found in the literature. While they often have a different focus, they also exhibit many points of overlap. This paper unifies these efforts and provides a complete taxonomy of XAI methods with respect to notions present in the current state of research. In a structured literature analysis and meta-study, we identified and reviewed more than 50 of the most cited and current surveys on XAI methods, metrics, and method traits. After summarizing them in a survey of surveys, we merge terminologies and concepts of the articles into a unified structured taxonomy. Single concepts therein are illustrated by more than 50 diverse selected example methods in total, which we categorize accordingly. The taxonomy may serve both beginners, researchers, and practitioners as a reference and wide-ranging overview of XAI method traits and aspects. Hence, it provides foundations for targeted, use-case-oriented, and context-sensitive future research.
Keywords Explainable artificial intelligence · Interpretability · Taxonomy · Meta-analysis · Survey-of-surveys · Review
APA: Schwalbe, G., & Finzel, B. (2023). A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts. Data Mining and Knowledge Discovery, 1-59.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=G.+Schwalbe+and+B.+Finzel%2C+%E2%80%9CA+comprehensive+taxonomy+for+explainable+artificial+intelligence%3A+a+systematic+survey+of+surveys+on+methods+and+concepts%2C%E2%80%9D+Data+Mining+and+Knowledge+Discovery%2C+pp.+1%E2%80%9359%2C+2023.&btnG=#d=gs_cit&t=1728099139081&u=%2Fscholar%3Fq%3Dinfo%3A1tFqoIRayvQJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Den
External link:
https://link.springer.com/article/10.1007/S10618-022-00867-8
DOI: https://doi.org/10.1007/s10618-022-00867-8


id:7  reference: A. Chaddad, J. Peng, J. Xu, and A. Bouridane, “Survey of explain-
able ai techniques in healthcare,” Sensors, vol. 23, no. 2, p. 634,
2023.
Bibtex: 
@article chaddad2023survey,
  title= Survey of explainable AI techniques in healthcare ,
  author= Chaddad, Ahmad and Peng, Jihao and Xu, Jian and Bouridane, Ahmed ,
  journal= Sensors ,
  volume= 23 ,
  number= 2 ,
  pages= 634 ,
  year= 2023 ,
  publisher= MDPI 
 
Citation : 214
Abstract: Artificial intelligence (AI) with deep learning models has been widely applied in numerous domains, including medical imaging and healthcare tasks. In the medical field, any judgment or decision is fraught with risk. A doctor will carefully judge whether a patient is sick before forming a reasonable explanation based on the patient’s symptoms and/or an examination. Therefore, to be a viable and accepted tool, AI needs to mimic human judgment and interpretation skills. Specifically, explainable AI (XAI) aims to explain the information behind the black-box model of deep learning that reveals how the decisions are made. This paper provides a survey of the most recent XAI techniques used in healthcare and related medical imaging applications. We summarize and categorize the XAI types, and highlight the algorithms used to increase interpretability in medical imaging topics. In addition, we focus on the challenging XAI problems in medical applications and provide guidelines to develop better interpretations of deep learning models using XAI concepts in medical image and text analysis. Furthermore, this survey provides future directions to guide developers and researchers for future prospective investigations on clinical topics, particularly on applications with medical imaging.
Keywords: explainable AI; medical imaging; deep learning; radiomics
APA: Chaddad, A., Peng, J., Xu, J., & Bouridane, A. (2023). Survey of explainable AI techniques in healthcare. Sensors, 23(2), 634.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Chaddad%2C+J.+Peng%2C+J.+Xu%2C+and+A.+Bouridane%2C+%E2%80%9CSurvey+of+explain-+able+ai+techniques+in+healthcare%2C%E2%80%9D+Sensors%2C+vol.+23%2C+no.+2%2C+p.+634%2C+2023.&btnG=#d=gs_cit&t=1728099489412&u=%2Fscholar%3Fq%3Dinfo%3AS8SCrawJEBgJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Den
External link:
https://www.mdpi.com/1424-8220/23/2/634
DOI: https://doi.org/10.3390/s23020634

id:8  reference: Z. Salahuddin, H. C. Woodruff, A. Chatterjee, and P. Lambin,
“Transparency of deep neural networks for medical image anal-
ysis: A review of interpretability methods,” Computers in biology
and medicine, vol. 140, p. 105111, 2022.
Bib tex: 
@article salahuddin2022transparency,
  title= Transparency of deep neural networks for medical image analysis: A review of interpretability methods ,
  author= Salahuddin, Zohaib and Woodruff, Henry C and Chatterjee, Avishek and Lambin, Philippe ,
  journal= Computers in biology and medicine ,
  volume= 140 ,
  pages= 105111 ,
  year= 2022 ,
  publisher= Elsevier 
 
Citation : 301
Abstract: Artificial Intelligence (AI) has emerged as a useful aid in numerous clinical applications for diagnosis and treatment decisions. Deep neural networks have shown the same or better performance than clinicians in many tasks owing to the rapid increase in the available data and computational power. In order to conform to the principles of trustworthy AI, it is essential that the AI system be transparent, robust, fair, and ensure accountability. Current deep neural solutions are referred to as black-boxes due to a lack of understanding of the specifics concerning the decision-making process. Therefore, there is a need to ensure the interpretability of deep neural networks before they can be incorporated into the routine clinical workflow. In this narrative review, we utilized systematic keyword searches and domain expertise to identify nine different types of interpretability methods that have been used for understanding deep learning models for medical image analysis applications based on the type of generated explanations and technical similarities. Furthermore, we report the progress made towards evaluating the explanations produced by various interpretability methods. Finally, we discuss limitations, provide guidelines for using interpretability methods and future directions concerning the interpretability of deep neural networks for medical imaging analysis.
Keywords
Explainable artificial intelligenceMedical imagingExplainabilityInterpretabilityDeep neural networks
APA: Salahuddin, Z., Woodruff, H. C., Chatterjee, A., & Lambin, P. (2022). Transparency of deep neural networks for medical image analysis: A review of interpretability methods. Computers in biology and medicine, 140, 105111.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+Z.+Salahuddin%2C+H.+C.+Woodruff%2C+A.+Chatterjee%2C+and+P.+Lambin%2C+%E2%80%9CTransparency+of+deep+neural+networks+for+medical+image+anal-+ysis%3A+A+review+of+interpretability+methods%2C%E2%80%9D+Computers+in+biology+and+medicine%2C+vol.+140%2C+p.+105111%2C+2022.&btnG=
External link:
https://www.sciencedirect.com/science/article/pii/S0010482521009057
DOI: https://doi.org/10.1016/j.compbiomed.2021.105111

id:9  reference: A. M. Antoniadi, Y. Du, Y. Guendouz, L. Wei, C. Mazo, B. A.
Becker, and C. Mooney, “Current challenges and future opportu-
nities for xai in machine learning-based clinical decision support
systems: a systematic review,” Applied Sciences, vol. 11, no. 11, p.
5088, 2021.
Bibtex: 
@article antoniadi2021current,
  title= Current challenges and future opportunities for XAI in machine learning-based clinical decision support systems: a systematic review ,
  author= Antoniadi, Anna Markella and Du, Yuhan and Guendouz, Yasmine and Wei, Lan and Mazo, Claudia and Becker, Brett A and Mooney, Catherine ,
  journal= Applied Sciences ,
  volume= 11 ,
  number= 11 ,
  pages= 5088 ,
  year= 2021 ,
  publisher= MDPI 
 
Citation : 446
Abstract: Machine Learning and Artificial Intelligence (AI) more broadly have great immediate and future potential for transforming almost all aspects of medicine. However, in many applications, even outside medicine, a lack of transparency in AI applications has become increasingly problematic. This is particularly pronounced where users need to interpret the output of AI systems. Explainable AI (XAI) provides a rationale that allows users to understand why a system has produced a given output. The output can then be interpreted within a given context. One area that is in great need of XAI is that of Clinical Decision Support Systems (CDSSs). These systems support medical practitioners in their clinic decision-making and in the absence of explainability may lead to issues of under or over-reliance. Providing explanations for how recommendations are arrived at will allow practitioners to make more nuanced, and in some cases, life-saving decisions. The need for XAI in CDSS, and the medical field in general, is amplified by the need for ethical and fair decision-making and the fact that AI trained with historical data can be a reinforcement agent of historical actions and biases that should be uncovered. We performed a systematic literature review of work to-date in the application of XAI in CDSS. Tabular data processing XAI-enabled systems are the most common, while XAI-enabled CDSS for text analysis are the least common in literature. There is more interest in developers for the provision of local explanations, while there was almost a balance between post-hoc and ante-hoc explanations, as well as between model-specific and model-agnostic techniques. Studies reported benefits of the use of XAI such as the fact that it could enhance decision confidence for clinicians, or generate the hypothesis about causality, which ultimately leads to increased trustworthiness and acceptability of the system and potential for its incorporation in the clinical workflow. However, we found an overall distinct lack of application of XAI in the context of CDSS and, in particular, a lack of user studies exploring the needs of clinicians. We propose some guidelines for the implementation of XAI in CDSS and explore some opportunities, challenges, and future research needs.
Keywords: artificial intelligence; explainable AI; XAI; clinical decision support systems; CDSS; medicine; machine learning; deep learning; explainability; transparency; interpretability
APA: Antoniadi, A. M., Du, Y., Guendouz, Y., Wei, L., Mazo, C., Becker, B. A., & Mooney, C. (2021). Current challenges and future opportunities for XAI in machine learning-based clinical decision support systems: a systematic review. Applied Sciences, 11(11), 5088.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+M.+Antoniadi%2C+Y.+Du%2C+Y.+Guendouz%2C+L.+Wei%2C+C.+Mazo%2C+B.+A.+Becker%2C+and+C.+Mooney%2C+%E2%80%9CCurrent+challenges+and+future+opportu-+nities+for+xai+in+machine+learning-based+clinical+decision+support+systems%3A+a+systematic+review%2C%E2%80%9D+Applied+Sciences%2C+vol.+11%2C+no.+11%2C+p.+5088%2C+2021.&btnG=#d=gs_cit&t=1728100191781&u=%2Fscholar%3Fq%3Dinfo%3AEOrHw2ENuVoJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Den
External link:
https://www.mdpi.com/2076-3417/11/11/5088
DOI: https://doi.org/10.3390/app11115088

id:10  reference: A. Singh, S. Sengupta, and V. Lakshminarayanan, “Explainable
deep learning models in medical image analysis,” Journal of imag-
ing, vol. 6, no. 6, p. 52, 2020.
Bibtex: 
@article singh2020explainable,
  title= Explainable deep learning models in medical image analysis ,
  author= Singh, Amitojdeep and Sengupta, Sourya and Lakshminarayanan, Vasudevan ,
  journal= Journal of imaging ,
  volume= 6 ,
  number= 6 ,
  pages= 52 ,
  year= 2020 ,
  publisher= MDPI 
 
Citation : 615
Abstract: Deep learning methods have been very effective for a variety of medical diagnostic tasks and have even outperformed human experts on some of those. However, the black-box nature of the algorithms has restricted their clinical use. Recent explainability studies aim to show the features that influence the decision of a model the most. The majority of literature reviews of this area have focused on taxonomy, ethics, and the need for explanations. A review of the current applications of explainable deep learning for different medical imaging tasks is presented here. The various approaches, challenges for clinical deployment, and the areas requiring further research are discussed here from a practical standpoint of a deep learning researcher designing a system for the clinical end-users.
Keywords: explainability; explainable AI; XAI; deep learning; medical imaging; diagnosis
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+A.+Singh%2C+S.+Sengupta%2C+and+V.+Lakshminarayanan%2C+%E2%80%9CExplainable+deep+learning+models+in+medical+image+analysis%2C%E2%80%9D+Journal+of+imag-+ing%2C+vol.+6%2C+no.+6%2C+p.+52%2C+2020.&btnG=#d=gs_cit&t=1728100381625&u=%2Fscholar%3Fq%3Dinfo%3AgWPvy_iZ_xwJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Den
External link: https://www.mdpi.com/2313-433X/6/6/52
DOI: https://doi.org/10.3390/jimaging6060052

id:11  reference: B. H. Van der Velden, H. J. Kuijf, K. G. Gilhuijs, and
M. A. Viergever, “Explainable artificial intelligence (xai) in deep
learning-based medical image analysis,” Medical Image Analysis,
vol. 79, p. 102470, 2022.
Bibtex: 
@article van2022explainable,
  title= Explainable artificial intelligence (XAI) in deep learning-based medical image analysis ,
  author= Van der Velden, Bas HM and Kuijf, Hugo J and Gilhuijs, Kenneth GA and Viergever, Max A ,
  journal= Medical Image Analysis ,
  volume= 79 ,
  pages= 102470 ,
  year= 2022 ,
  publisher= Elsevier 
 
Citation : 660
Abstract: With an increase in deep learning-based methods, the call for explainability of such methods grows, especially in high-stakes decision making areas such as medical image analysis. This survey presents an overview of explainable artificial intelligence (XAI) used in deep learning-based medical image analysis. A framework of XAI criteria is introduced to classify deep learning-based medical image analysis methods. Papers on XAI techniques in medical image analysis are then surveyed and categorized according to the framework and according to anatomical location. The paper concludes with an outlook of future opportunities for XAI in medical image analysis.
Keywords
Explainable artificial intelligenceInterpretable deep learningMedical image analysisDeep learningSurvey
APA: Van der Velden, B. H., Kuijf, H. J., Gilhuijs, K. G., & Viergever, M. A. (2022). Explainable artificial intelligence (XAI) in deep learning-based medical image analysis. Medical Image Analysis, 79, 102470.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+B.+H.+Van+der+Velden%2C+H.+J.+Kuijf%2C+K.+G.+Gilhuijs%2C+and+M.+A.+Viergever%2C+%E2%80%9CExplainable+artificial+intelligence+%28xai%29+in+deep+learning-based+medical+image+analysis%2C%E2%80%9D+Medical+Image+Analysis%2C+vol.+79%2C+p.+102470%2C+2022.&btnG=#d=gs_cit&t=1728100545987&u=%2Fscholar%3Fq%3Dinfo%3Am8uDFwkg3Q8J%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Den
External link:
https://www.sciencedirect.com/science/article/pii/S1361841522001177
DOI: https://doi.org/10.1016/j.media.2022.102470

id:12  reference: S. Nazir, D. M. Dickson, and M. U. Akram, “Survey of explainable
artificial intelligence techniques for biomedical imaging with deep
neural networks,” Computers in Biology and Medicine, vol. 156, p.
106668, 2023.
Bibtex: 
@article nazir2023survey,
  title= Survey of explainable artificial intelligence techniques for biomedical imaging with deep neural networks ,
  author= Nazir, Sajid and Dickson, Diane M and Akram, Muhammad Usman ,
  journal= Computers in Biology and Medicine ,
  volume= 156 ,
  pages= 106668 ,
  year= 2023 ,
  publisher= Elsevier 
 
Citation : 109
Abstract: Artificial Intelligence (AI) techniques of deep learning have revolutionized the disease diagnosis with their outstanding image classification performance. In spite of the outstanding results, the widespread adoption of these techniques in clinical practice is still taking place at a moderate pace. One of the major hindrance is that a trained Deep Neural Networks (DNN) model provides a prediction, but questions about why and how that prediction was made remain unanswered. This linkage is of utmost importance for the regulated healthcare domain to increase the trust in the automated diagnosis system by the practitioners, patients and other stakeholders. The application of deep learning for medical imaging has to be interpreted with caution due to the health and safety concerns similar to blame attribution in the case of an accident involving autonomous cars. The consequences of both a false positive and false negative cases are far reaching for patients' welfare and cannot be ignored. This is exacerbated by the fact that the state-of-the-art deep learning algorithms comprise of complex interconnected structures, millions of parameters, and a ‘black box’ nature, offering little understanding of their inner working unlike the traditional machine learning algorithms. Explainable AI (XAI) techniques help to understand model predictions which help develop trust in the system, accelerate the disease diagnosis, and meet adherence to regulatory requirements.
This survey provides a comprehensive review of the promising field of XAI for biomedical imaging diagnostics. We also provide a categorization of the XAI techniques, discuss the open challenges, and provide future directions for XAI which would be of interest to clinicians, regulators and model developers.
Keywords
Interpretable AIBlackboxFeaturesSupervised learningPredictive modelsNeural networksDiagnostic imagingBackpropagation
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=S.+Nazir%2C+D.+M.+Dickson%2C+and+M.+U.+Akram%2C+%E2%80%9CSurvey+of+explainable+artificial+intelligence+techniques+for+biomedical+imaging+with+deep+neural+networks%2C%E2%80%9D+Computers+in+Biology+and+Medicine%2C+vol.+156%2C+p.+106668%2C+2023.&btnG=
External link:
https://www.sciencedirect.com/science/article/pii/S0010482523001336
DOI: https://doi.org/10.1016/j.compbiomed.2023.106668

id:13  reference: E. Tjoa and C. Guan, “A survey on explainable artificial intel-
ligence (xai): Toward medical xai,” IEEE transactions on neural
networks and learning systems, vol. 32, no. 11, pp. 4793–4813, 2020.
Bibtex: 
@article tjoa2020survey,
  title= A survey on explainable artificial intelligence (xai): Toward medical xai ,
  author= Tjoa, Erico and Guan, Cuntai ,
  journal= IEEE transactions on neural networks and learning systems ,
  volume= 32 ,
  number= 11 ,
  pages= 4793--4813 ,
  year= 2020 ,
  publisher= IEEE 
 
Citation : 1780
Abstract: Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.
Keywords : Explainable artificial intelligence (XAI), interpretability, machine learning (ML), medical information system, survey.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=E.+Tjoa+and+C.+Guan%2C+%E2%80%9CA+survey+on+explainable+artificial+intel-+ligence+%28xai%29%3A+Toward+medical+xai%2C%E2%80%9D+IEEE+transactions+on+neural+networks+and+learning+systems%2C+vol.+32%2C+no.+11%2C+pp.+4793%E2%80%934813%2C+2020&btnG=
External link: https://ieeexplore.ieee.org/abstract/document/9233366
DOI: 10.1109/TNNLS.2020.3027314

id:14  reference: X. Huang, D. Kroening, W. Ruan, J. Sharp, Y. Sun, E. Thamo,
M. Wu, and X. Yi, “A survey of safety and trustworthiness of
deep neural networks: Verification, testing, adversarial attack and
defence, and interpretability,” Computer Science Review, vol. 37, p.
100270, 2020.
Bibtex: 
@article huang2020survey,
  title= A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability ,
  author= Huang, Xiaowei and Kroening, Daniel and Ruan, Wenjie and Sharp, James and Sun, Youcheng and Thamo, Emese and Wu, Min and Yi, Xinping ,
  journal= Computer Science Review ,
  volume= 37 ,
  pages= 100270 ,
  year= 2020 ,
  publisher= Elsevier 
 
Citation : 530
Abstract: In the past few years, significant progress has been made on deep neural networks (DNNs) in achieving human-level performance on several long-standing tasks. With the broader deployment of DNNs on various applications, the concerns over their safety and trustworthiness have been raised in public, especially after the widely reported fatal incidents involving self-driving cars. Research to address these concerns is particularly active, with a significant number of papers released in the past few years. This survey paper conducts a review of the current research effort into making DNNs safe and trustworthy, by focusing on four aspects: verification, testing, adversarial attack and defence, and interpretability. In total, we survey 202 papers, most of which were published after 2017.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=X.+Huang%2C+D.+Kroening%2C+W.+Ruan%2C+J.+Sharp%2C+Y.+Sun%2C+E.+Thamo%2C+M.+Wu%2C+and+X.+Yi%2C+%E2%80%9CA+survey+of+safety+and+trustworthiness+of+deep+neural+networks%3A+Verification%2C+testing%2C+adversarial+attack+and+defence%2C+and+interpretability%2C%E2%80%9D+Computer+Science+Review%2C+vol.+37%2C+p.+100270%2C+2020.&btnG=
External link:
https://www.sciencedirect.com/science/article/pii/S1574013719302527?casa_token=0LuY65fsR_4AAAAA:wc9_vnpwkrR-6BwzEx7gMUUVzesxkQk90np9zZNoCp4pxvnWy5a2H3mk9TzE-t3-bHubLKCD
DOI: https://doi.org/10.1016/j.cosrev.2020.100270

id:15  reference: M. Reyes, R. Meier, S. Pereira, C. A. Silva, F.-M. Dahlweid,
H. v. Tengg-Kobligk, R. M. Summers, and R. Wiest, “On the
interpretability of artificial intelligence in radiology: challenges
and opportunities,” Radiology: artificial intelligence, vol. 2, no. 3,
p. e190043, 2020.
Bibtex: 
@article reyes2020interpretability,
  title= On the interpretability of artificial intelligence in radiology: challenges and opportunities ,
  author= Reyes, Mauricio and Meier, Raphael and Pereira, S \'e rgio and Silva, Carlos A and Dahlweid, Fried-Michael and Tengg-Kobligk, Hendrik von and Summers, Ronald M and Wiest, Roland ,
  journal= Radiology: artificial intelligence ,
  volume= 2 ,
  number= 3 ,
  pages= e190043 ,
  year= 2020 ,
  publisher= Radiological Society of North America 
 
Citation : 411
Abstract:  As artificial intelligence (AI) systems begin to make their way into clinical radiology practice, it is crucial to assure that they function correctly and that they gain the trust of experts. Toward this goal, approaches to make AI “interpretable” have gained attention to enhance the understanding of a machine learning algorithm, despite its complexity. This article aims to provide insights into the current state of the art of interpretability methods for radiology AI. This review discusses radiologists’ opinions on the topic and suggests trends and challenges that need to be addressed to effectively streamline interpretability methods in clinical practice.
Keywords: Convolutional Neural Network (CNN), Informatics, Radiomics, Supervised learning, Technology Assessment 
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=M.+Reyes%2C+R.+Meier%2C+S.+Pereira%2C+C.+A.+Silva%2C+F.-M.+Dahlweid%2C+H.+v.+Tengg-Kobligk%2C+R.+M.+Summers%2C+and+R.+Wiest%2C+%E2%80%9COn+the+interpretability+of+artificial+intelligence+in+radiology%3A+challenges+and+opportunities%2C%E2%80%9D+Radiology%3A+artificial+intelligence%2C+vol.+2%2C+no.+3%2C+p.+e190043%2C+2020.&btnG=
External link:
https://pubs.rsna.org/doi/full/10.1148/ryai.2020190043
DOI: https://doi.org/10.1148/ryai.2020190043

id:16  reference: T. T. Nguyen, Q. V. H. Nguyen, D. T. Nguyen, S. Yang, P. W.
Eklund, T. Huynh-The, T. T. Nguyen, Q.-V. Pham, I. Razzak, and
E. B. Hsu, “Artificial intelligence in the battle against coronavirus
(covid-19): a survey and future research directions,” arXiv preprint
arXiv:2008.07343, 2020.
Bibtex: 
@article nguyen2020artificial,
  title= Artificial intelligence in the battle against coronavirus (COVID-19): a survey and future research directions ,
  author= Nguyen, Thanh Thi and Nguyen, Quoc Viet Hung and Nguyen, Dung Tien and Yang, Samuel and Eklund, Peter W and Huynh-The, Thien and Nguyen, Thanh Tam and Pham, Quoc-Viet and Razzak, Imran and Hsu, Edbert B ,
  journal= arXiv preprint arXiv:2008.07343 ,
  year= 2020 
 
Citation : 287
Abstract: Artificial intelligence (AI) has been applied widely in our daily lives in a variety of ways with numerous success stories. AI has also contributed to dealing with the coronavirus disease (COVID-19) pandemic, which has been happening around the globe. This paper presents a survey of AI methods being used in various applications in the fight against the COVID-19 outbreak and outlines the crucial role of AI research in this unprecedented battle. We touch on areas where AI plays as an essential component, from medical image processing, data analytics, text mining and natural language processing, the Internet of Things, to computational biology and medicine. A summary of COVID-19 related data sources that are available for research purposes is also presented. Research directions on exploring the potential of AI and enhancing its capability and power in the pandemic battle are thoroughly discussed. We identify 13 groups of problems related to the COVID-19 pandemic and highlight promising AI methods and tools that can be used to address these problems. It is envisaged that this study will provide AI researchers and the wider community with an overview of the current status of AI applications, and motivate researchers to harness AI's potential in the fight against COVID-19.
Google scholar link: https://scholar.googleusercontent.com/scholar.bib?q=info:4XfatXFpnRoJ:scholar.google.com/&output=citation&scisdr=ClH65O8MEMCp87a5TpQ:AFWwaeYAAAAAZwC_VpRv8UrmNDwIvBTbjZMb_AY&scisig=AFWwaeYAAAAAZwC_Voz67ldjoZWFvmsNv_udm3I&scisf=4&ct=citation&cd=-1&hl=en
External link:
https://arxiv.org/abs/2008.07343
DOI: https://arxiv.org/abs/2008.07343

id:17  reference: R. Karthik, R. Menaka, M. Hariharan, and G. Kathiresan, “Ai for
covid-19 detection from radiographs: Incisive analysis of state of
the art techniques, key challenges and future directions,” IRBM,
vol. 43, no. 5, pp. 486–510, 2022.
Bibtex: 
@article karthik2022ai,
  title= Ai for COVID-19 detection from radiographs: Incisive analysis of state of the art techniques, key challenges and future directions ,
  author= Karthik, R and Menaka, R and Hariharan, M and Kathiresan, GS ,
  journal= IRBM ,
  volume= 43 ,
  number= 5 ,
  pages= 486--510 ,
  year= 2022 ,
  publisher= Elsevier 
 
Citation : 19
Abstract: Background and objective: In recent years, Artificial Intelligence has had an evident impact on the way research addresses challenges in different domains. It has proven to be a huge asset, especially in the medical field, allowing for time-efficient and reliable solutions. This research aims to spotlight the impact of deep learning and machine learning models in the detection of COVID-19 from medical images. This is achieved by conducting a review of the state-of-the-art approaches proposed by the recent works in this field. Methods: The main focus of this study is the recent developments of classification and segmentation approaches to image-based COVID-19 detection. The study reviews 140 research papers published in different academic research databases. These papers have been screened and filtered based on specified criteria, to acquire insights prudent to image-based COVID-19 detection. Results: The methods discussed in this review include different types of imaging modality, predominantly X-rays and CT scans. These modalities are used for classification and segmentation tasks as well. This review seeks to categorize and discuss the different deep learning and machine learning architectures employed for these tasks, based on the imaging modality utilized. It also hints at other possible deep learning and machine learning architectures that can be proposed for better results towards COVID-19 detection. Along with that, a detailed overview of the emerging trends and breakthroughs in Artificial Intelligence-based COVID-19 detection has been discussed as well. Conclusion: This work concludes by stipulating the technical and non-technical challenges faced by researchers and illustrates the advantages of image-based COVID-19 detection with Artificial Intelligence techniques. 
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=R.+Karthik%2C+R.+Menaka%2C+M.+Hariharan%2C+and+G.+Kathiresan%2C+%E2%80%9CAi+for+covid-19+detection+from+radiographs%3A+Incisive+analysis+of+state+of+the+art+techniques%2C+key+challenges+and+future+directions%2C%E2%80%9D+IRBM%2C+vol.+43%2C+no.+5%2C+pp.+486%E2%80%93510%2C+2022.&btnG=
External link:
https://www.sciencedirect.com/science/article/pii/S1959031821000956?casa_token=OC1fUfZllFkAAAAA:_dXS9P6n1nx5jfdaTj-JQlbHjuU7x3QSMfKLRjywEGHrHG6rMqQ_SIzSQpMOLCbOywRGIdk2
DOI: https://doi.org/10.1016/j.irbm.2021.07.002

id:18  reference: P. Weber, K. V. Carl, and O. Hinz, “Applications of explainable
artificial intelligence in finance—a systematic review of finance,
information systems, and computer science literature,” Manage-
ment Review Quarterly, vol. 74, no. 2, pp. 867–907, 2024.
Bibtex: 
@article weber2024applications,
  title= Applications of explainable artificial intelligence in finance—a systematic review of finance, information systems, and computer science literature ,
  author= Weber, Patrick and Carl, K Valerie and Hinz, Oliver ,
  journal= Management Review Quarterly ,
  volume= 74 ,
  number= 2 ,
  pages= 867--907 ,
  year= 2024 ,
  publisher= Springer 
 
Citation : 110
Abstract: Digitalization and technologization affect numerous domains, promising advantages but also entailing risks. Hence, when decision-makers in highly-regulated domains like Finance implement these technological advances—especially Artificial Intelligence—regulators prescribe high levels of transparency, assuring the traceability of decisions for third parties. Explainable Artificial Intelligence (XAI) is of tremendous importance in this context. We provide an overview of current research on XAI in Finance with a systematic literature review screening 2,022 articles from leading Finance, Information Systems, and Computer Science outlets. We identify a set of 60 relevant articles, classify them according to the used XAI methods and goals that they aim to achieve, and provide an overview of XAI methods used in different Finance areas. Areas like risk management, portfolio optimization, and applications around the stock market are well-researched, while anti-money laundering is understudied. Researchers implement both transparent models and post-hoc explainability, while they recently favored the latter.
Keywords Explainable artifcial intelligence · Finance · Systematic literature review · Machine learning · Review 
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+P.+Weber%2C+K.+V.+Carl%2C+and+O.+Hinz%2C+%E2%80%9CApplications+of+explainable+artificial+intelligence+in+finance%E2%80%94a+systematic+review+of+finance%2C+information+systems%2C+and+computer+science+literature%2C%E2%80%9D+Manage-+ment+Review+Quarterly%2C+vol.+74%2C+no.+2%2C+pp.+867%E2%80%93907%2C+2024.&btnG=
External link:
https://link.springer.com/article/10.1007/S11301-023-00320-0
DOI: https://link.springer.com/article/10.1007/S11301-023-00320-0

id:19  reference: E. Owens, B. Sheehan, M. Mullins, M. Cunneen, J. Ressel, and
G. Castignani, “Explainable artificial intelligence (xai) in insur-
ance,” Risks, vol. 10, no. 12, p. 230, 2022.
Bibtex: 
@article owens2022explainable,
  title= Explainable artificial intelligence (xai) in insurance ,
  author= Owens, Emer and Sheehan, Barry and Mullins, Martin and Cunneen, Martin and Ressel, Juliane and Castignani, German ,
  journal= Risks ,
  volume= 10 ,
  number= 12 ,
  pages= 230 ,
  year= 2022 ,
  publisher= MDPI 
 
Citation : 34
Abstract: Explainable Artificial Intelligence (XAI) models allow for a more transparent and understandable relationship between humans and machines. The insurance industry represents a fundamental opportunity to demonstrate the potential of XAI, with the industry’s vast stores of sensitive data on policyholders and centrality in societal progress and innovation. This paper analyses current Artificial Intelligence (AI) applications in insurance industry practices and insurance research to assess their degree of explainability. Using search terms representative of (X)AI applications in insurance, 419 original research articles were screened from IEEE Xplore, ACM Digital Library, Scopus, Web of Science and Business Source Complete and EconLit. The resulting 103 articles (between the years 2000–2021) representing the current state-of-the-art of XAI in insurance literature are analysed and classified, highlighting the prevalence of XAI methods at the various stages of the insurance value chain. The study finds that XAI methods are particularly prevalent in claims management, underwriting and actuarial pricing practices. Simplification methods, called knowledge distillation and rule extraction, are identified as the primary XAI technique used within the insurance value chain. This is important as the combination of large models to create a smaller, more manageable model with distinct association rules aids in building XAI models which are regularly understandable. XAI is an important evolution of AI to ensure trust, transparency and moral values are embedded within the system’s ecosystem. The assessment of these XAI foci in the context of the insurance industry proves a worthwhile exploration into the unique advantages of XAI, highlighting to industry professionals, regulators and XAI developers where particular focus should be directed in the further development of XAI. This is the first study to analyse XAI’s current applications within the insurance industry, while simultaneously contributing to the interdisciplinary understanding of applied XAI. Advancing the literature on adequate XAI definitions, the authors propose an adapted definition of XAI informed by the systematic review of XAI literature in insurance.
Keywords: Explainable Artificial Intelligence; machine learning; insurance value chain; risk management; data governance
Google scholar link: https://scholar.googleusercontent.com/scholar.bib?q=info:4XfatXFpnRoJ:scholar.google.com/&output=citation&scisdr=ClH65O8MEMCp87a5TpQ:AFWwaeYAAAAAZwC_VpRv8UrmNDwIvBTbjZMb_AY&scisig=AFWwaeYAAAAAZwC_Voz67ldjoZWFvmsNv_udm3I&scisf=4&ct=citation&cd=-1&hl=en
External link:
https://www.mdpi.com/2227-9091/10/12/230
DOI: https://doi.org/10.3390/risks10120230

id:20  reference: E. Mohamed, K. Sirlantzis, and G. Howells, “A review of
visualisation-as-explanation techniques for convolutional neural
networks and their evaluation,” Displays, vol. 73, p. 102239, 2022.
Bibtex: 
@article mohamed2022review,
  title= A review of visualisation-as-explanation techniques for convolutional neural networks and their evaluation ,
  author= Mohamed, Elhassan and Sirlantzis, Konstantinos and Howells, Gareth ,
  journal= Displays ,
  volume= 73 ,
  pages= 102239 ,
  year= 2022 ,
  publisher= Elsevier 
 
Citation : 43
Abstract: Visualisation techniques are powerful tools to understand the behaviour of Artificial Intelligence (AI) systems. They can be used to identify important features contributing to the network decisions, investigate biases in datasets, and find weaknesses in the system's structure (e.g., network architectures). Lawmakers and regulators may not allow the use of smart systems if these systems cannot explain the logic underlying a decision or action taken. These systems are required to offer a high level of 'transparency' to be approved for deployment. Model transparency is vital for safety–critical applications such as autonomous navigation and operation systems (e.g., autonomous trains or cars), where prediction errors may have serious implications. Thus, being highly accurate without explaining the basis of their performance is not enough to satisfy regulatory requirements. The lack of system interpretability is a major obstacle to the wider adoption of AI in safety–critical applications. Explainable Artificial Intelligence (XAI) techniques applied to intelligent systems to justify their decisions offers a possible solution. In this review, we present state-of-the-art explanation techniques in detail. We focus our presentation and critical discussion on visualisation methods for the most adopted architecture in use, the Convolutional Neural Networks (CNNs), applied to the domain of image classification. Further, we discuss the evaluation techniques for different explanation methods, which shows that some of the most visually appealing methods are unreliable and can be considered a simple feature or edge detector. In contrast, robust methods can give insights into the model behaviour, which helps to enhance the model performance and boost the confidence in the model's predictions. Besides, the applications of XAI techniques show their importance in many fields such as medicine and industry. We hope that this review proves a valuable contribution for researchers in the field of XAI.
Keywords
Activation heatmapsArchitecture understandingBlack-box representationsCNN visualisationConvolutional neural networksExplainable AIFeature visualisationInterpretable neural networksSaliency mapsXAI
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+E.+Mohamed%2C+K.+Sirlantzis%2C+and+G.+Howells%2C+%E2%80%9CA+review+of+visualisation-as-explanation+techniques+for+convolutional+neural+networks+and+their+evaluation%2C%E2%80%9D+Displays%2C+vol.+73%2C+p.+102239%2C+2022.&btnG=
External link:
https://www.sciencedirect.com/science/article/pii/S014193822200066X
DOI: https://doi.org/10.1016/j.displa.2022.102239

id:21  reference: W. Samek, G. Montavon, A. Vedaldi, L. K. Hansen, and K.-R.
M ¨uller, Explainable AI: interpreting, explaining and visualizing deep
learning. Springer Nature, 2019, vol. 11700.
Bibtex: 
@book samek2019explainable,
  title= Explainable AI: interpreting, explaining and visualizing deep learning ,
  author= Samek, Wojciech and Montavon, Gr \'e goire and Vedaldi, Andrea and Hansen, Lars Kai and M \"u ller, Klaus-Robert ,
  volume= 11700 ,
  year= 2019 ,
  publisher= Springer Nature 
 
Citation : 1328
Abstract: The development of “intelligent” systems that can take decisions and perform autonomously might lead to faster and more consistent decisions. A limiting factor for a broader adoption of AI technology is the inherent risks that come with giving up human control and oversight to “intelligent” machines. For sensitive tasks involving critical infrastructures and affecting human well-being or health, it is crucial to limit the possibility of improper, non-robust and unsafe decisions and actions. Before deploying an AI system, we see a strong need to validate its behavior, and thus establish guarantees that it will continue to perform as expected when deployed in a real-world environment. In pursuit of that objective, ways for humans to verify the agreement between the AI decision structure and their own ground-truth knowledge have been explored. Explainable AI (XAI) has developed as a subfield of AI, focused on exposing complex AI models to humans in a systematic and interpretable manner. The 22 chapters included in this book provide a timely snapshot of algorithms, theory, and applications of interpretable and explainable AI and AI techniques that have been proposed recently reflecting the current discourse in this field and providing directions of future development. The book is organized in six parts: towards AI transparency; methods for interpreting AI systems; explaining the decisions of AI systems; evaluating interpretability and explanations; applications of explainable AI; and software for explainable AI.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+W.+Samek%2C+G.+Montavon%2C+A.+Vedaldi%2C+L.+K.+Hansen%2C+and+K.-R.+M+%C2%A8uller%2C+Explainable+AI%3A+interpreting%2C+explaining+and+visualizing+deep+learning.+Springer+Nature%2C+2019%2C+vol.+11700.&btnG=
External link:
https://books.google.com/books?hl=en&lr=&id=j5yuDwAAQBAJ&oi=fnd&pg=PR5&dq=+W.+Samek,+G.+Montavon,+A.+Vedaldi,+L.+K.+Hansen,+and+K.-R.+M+%C2%A8uller,+Explainable+AI:+interpreting,+explaining+and+visualizing+deep+learning.+Springer+Nature,+2019,+vol.+11700.&ots=Ir4UPC7R6F&sig=swaOPzfOwJbAVonOfsCT46Z1dZc#v=onepage&q=W.%20Samek%2C%20G.%20Montavon%2C%20A.%20Vedaldi%2C%20L.%20K.%20Hansen%2C%20and%20K.-R.%20M%20%C2%A8uller%2C%20Explainable%20AI%3A%20interpreting%2C%20explaining%20and%20visualizing%20deep%20learning.%20Springer%20Nature%2C%202019%2C%20vol.%2011700.&f=false

id:22  reference: W. Samek, G. Montavon, S. Lapuschkin, C. J. Anders, and K.-R.
M ¨uller, “Explaining deep neural networks and beyond: A review
of methods and applications,” Proceedings of the IEEE, vol. 109,
no. 3, pp. 247–278, 2021.
Bibtex: 
@article samek2021explaining,
  title= Explaining deep neural networks and beyond: A review of methods and applications ,
  author= Samek, Wojciech and Montavon, Gr \'e goire and Lapuschkin, Sebastian and Anders, Christopher J and M \"u ller, Klaus-Robert ,
  journal= Proceedings of the IEEE ,
  volume= 109 ,
  number= 3 ,
  pages= 247--278 ,
  year= 2021 ,
  publisher= IEEE 
 
Citation : 1066
Abstract: With the broader and highly successful usage of machine learning (ML) in industry and the sciences, there has been a growing demand for explainable artificial intelligence (XAI). Interpretability and explanation methods for gaining a better understanding of the problem-solving abilities and strategies of nonlinear ML, in particular, deep neural networks, are, therefore, receiving increased attention. In this work, we aim to: 1) provide a timely overview of this active emerging field, with a focus on “post hoc” explanations, and explain its theoretical foundations; 2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations; 3) outline best practice aspects, i.e., how to best include interpretation methods into the standard usage of ML; and 4) demonstrate successful usage of XAI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of ML.
KEYWORDS | Black-box models; deep learning; explainable artificial intelligence (XAI); Interpretability; model transparency; neural networks
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=W.+Samek%2C+G.+Montavon%2C+S.+Lapuschkin%2C+C.+J.+Anders%2C+and+K.-R.+M+%C2%A8uller%2C+%E2%80%9CExplaining+deep+neural+networks+and+beyond%3A+A+review+of+methods+and+applications%2C%E2%80%9D+Proceedings+of+the+IEEE%2C+vol.+109%2C+no.+3%2C+pp.+247%E2%80%93278%2C+2021.&btnG=
External link:
https://ieeexplore.ieee.org/abstract/document/9369420
DOI: 10.1109/JPROC.2021.3060483

id:23  reference: W. Saeed and C. Omlin, “Explainable ai (xai): A systematic meta-
survey of current challenges and future opportunities,” Knowledge-
Based Systems, vol. 263, p. 110273, 2023.
Bibtex: 
@article saeed2023explainable,
  title= Explainable AI (XAI): A systematic meta-survey of current challenges and future opportunities ,
  author= Saeed, Waddah and Omlin, Christian ,
  journal= Knowledge-Based Systems ,
  volume= 263 ,
  pages= 110273 ,
  year= 2023 ,
  publisher= Elsevier 
 
Citation : 393
Abstract: The past decade has seen significant progress in artificial intelligence (AI), which has resulted in algorithms being adopted for resolving a variety of problems. However, this success has been met by increasing model complexity and employing black-box AI models that lack transparency. In response to this need, Explainable AI (XAI) has been proposed to make AI more transparent and thus advance the adoption of AI in critical domains. Although there are several reviews of XAI topics in the literature that have identified challenges and potential research directions of XAI, these challenges and research directions are scattered. This study, hence, presents a systematic meta-survey of challenges and future research directions in XAI organized in two themes: (1) general challenges and research directions of XAI and (2) challenges and research directions of XAI based on machine learning life cycle’s phases: design, development, and deployment. We believe that our meta-survey contributes to XAI literature by providing a guide for future exploration in the XAI area.
Keywords
Explainable AI (XAI)Interpretable AIBlack-boxMachine learningDeep learningMeta-surveyResponsible AI
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+W.+Saeed+and+C.+Omlin%2C+%E2%80%9CExplainable+ai+%28xai%29%3A+A+systematic+meta-+survey+of+current+challenges+and+future+opportunities%2C%E2%80%9D+Knowledge-+Based+Systems%2C+vol.+263%2C+p.+110273%2C+2023.&btnG=
External link:
https://www.sciencedirect.com/science/article/pii/S0950705123000230
DOI: https://doi.org/10.1016/j.knosys.2023.110273

id:24  reference: A. Ghorbani, J. Wexler, J. Y. Zou, and B. Kim, “Towards auto-
matic concept-based explanations,” Advances in neural information
processing systems, vol. 32, 2019.
Bibtex: 
@article ghorbani2019towards,
  title= Towards automatic concept-based explanations ,
  author= Ghorbani, Amirata and Wexler, James and Zou, James Y and Kim, Been ,
  journal= Advances in neural information processing systems ,
  volume= 32 ,
  year= 2019 
 
Citation : 698
Abstract: Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for\emph  concept  based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that\alg discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Ghorbani%2C+J.+Wexler%2C+J.+Y.+Zou%2C+and+B.+Kim%2C+%E2%80%9CTowards+auto-+matic+concept-based+explanations%2C%E2%80%9D+Advances+in+neural+information+processing+systems%2C+vol.+32%2C+2019.&btnG=
External link:
https://proceedings.neurips.cc/paper/2019/hash/77d2afcb31f6493e350fca61764efb9a-Abstract.html
DOI: https://proceedings.neurips.cc/paper/2019/hash/77d2afcb31f6493e350fca61764efb9a-Abstract.html

id:25  reference: J. Huang, A. Mishra, B. C. Kwon, and C. Bryan, “Conceptex-
plainer: Interactive explanation for deep neural networks from a
concept perspective,” IEEE Transactions on Visualization and Com-
puter Graphics, vol. 29, no. 1, pp. 831–841, 2022.
Bibtex: 
@article huang2022conceptexplainer,
  title= Conceptexplainer: Interactive explanation for deep neural networks from a concept perspective ,
  author= Huang, Jinbin and Mishra, Aditi and Kwon, Bum Chul and Bryan, Chris ,
  journal= IEEE Transactions on Visualization and Computer Graphics ,
  volume= 29 ,
  number= 1 ,
  pages= 831--841 ,
  year= 2022 ,
  publisher= IEEE 
 
Citation : 31
Abstract: Traditional deep learning interpretability methods which are suitable for model users cannot explain network behaviors at the global level and are inflexible at providing fine-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their flexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network's latent space and have commonly been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difficult to navigate and make sense of the concept space. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate C oncept E xplainer , a visual analytics system that enables people to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that model users face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how C oncept E xplainer supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classification, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.
Keywords: Conceptexplainer: Interactive explanation for deep neural networks from a concept perspective
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=J.+Huang%2C+A.+Mishra%2C+B.+C.+Kwon%2C+and+C.+Bryan%2C+%E2%80%9CConceptex-+plainer%3A+Interactive+explanation+for+deep+neural+networks+from+a+concept+perspective%2C%E2%80%9D+IEEE+Transactions+on+Visualization+and+Com-+puter+Graphics%2C+vol.+29%2C+no.+1%2C+pp.+831%E2%80%93841%2C+2022.&btnG=
External link:
https://ieeexplore.ieee.org/abstract/document/9903285?casa_token=CrVOZ_tJMIEAAAAA:u3JBNEBX4XEAxsijp_KUOOMKNunCMgkHSXadzh7EaW0FGd0069EBDwR0HA5OaNGZcDZDjc2w
DOI: 10.1109/TVCG.2022.3209384

id:26  reference: S. Keele et al., “Guidelines for performing systematic literature
reviews in software engineering,” 2007.
Bibtex: 
@techreport keele2007guidelines,
  title= Guidelines for performing systematic literature reviews in software engineering ,
  author= Keele, Staffs and others ,
  year= 2007 ,
  institution= Technical report, ver. 2.3 ebse technical report. ebse 
 
Abstract: This document presents general guidelines for undertaking systematic reviews. The goal of this document is to introduce the methodology for performing rigorous reviews of current empirical evidence to the software engineering community. It is aimed primarily at software engineering researchers including PhD students. It does not cover details of meta-analysis (a statistical procedure for synthesising quantitative results from different studies), nor does it discuss the implications that different types of systematic review questions have on research procedures.
The original impetus for employing systematic literature review practice was to support evidence-based medicine, and many guidelines reflect this viewpoint. This document attempts to construct guidelines for performing systematic literature reviews that are appropriate to the needs of software engineering researchers. It discusses a number of issues where software engineering research differs from medical research. In particular, software engineering research has relatively little empirical research compared with the medical domain; research methods used by software engineers are not as generally rigorous as those used by medical researchers; and much empirical data in software engineering is proprietary.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+S.+Keele+et+al.%2C+%E2%80%9CGuidelines+for+performing+systematic+literature+reviews+in+software+engineering%2C%E2%80%9D+2007.&btnG=
External link:
https://www.researchgate.net/profile/Barbara-Kitchenham/publication/302924724_Guidelines_for_performing_Systematic_Literature_Reviews_in_Software_Engineering/links/61712932766c4a211c03a6f7/Guidelines-for-performing-Systematic-Literature-Reviews-in-Software-Engineering.pdf

id:27  reference: R. J. Piper, “How to write a systematic literature review: a guide
for medical students,” National AMR, fostering medical research,
vol. 1, pp. 1–8, 2013.
Bibtex: 
@article piper2013write,
  title= How to write a systematic literature review: a guide for medical students ,
  author= Piper, Rory J ,
  journal= National AMR, fostering medical research ,
  volume= 1 ,
  pages= 1--8 ,
  year= 2013 ,
  publisher= University of Edinburgh Edinburgh, UK 
 
Abstract: Objectives
This guide aims to serve as a practical introduction to:
• the rationale for conducting a systematic review of the literature
• how to search the literature
• qualitative and quantitative interpretation
• how to structure a systematic review manuscript
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=R.+J.+Piper%2C+%E2%80%9CHow+to+write+a+systematic+literature+review%3A+a+guide+for+medical+students%2C%E2%80%9D+National+AMR%2C+fostering+medical+research%2C+vol.+1%2C+pp.+1%E2%80%938%2C+2013.&btnG=
External link:
https://www.southampton.ac.uk/assets/imported/transforms/content-block/UsefulDownloads_Download/A02316A7B39E4EE09F62F3210D16D1EC/NSAMR%20Systematic%20Review.pdf

id:28  reference: H. R. Kouchaksaraei and H. Karl, “Service function chaining
across openstack and kubernetes domains,” in Proceedings of the
13th ACM International Conference on Distributed and Event-based
Systems, 2019, pp. 240–243.
Bibtex: 
@inproceedings kouchaksaraei2019service,
  title= Service function chaining across openstack and kubernetes domains ,
  author= Kouchaksaraei, Hadi Razzaghi and Karl, Holger ,
  booktitle= Proceedings of the 13th ACM International Conference on Distributed and Event-based Systems ,
  pages= 240--243 ,
  year= 2019 
 
Abstract: Remarkable advantages of Containers (CNs) over Virtual Machines (VMs) such as lower overhead and faster startup has gained the attention of Communication Service Providers (CSPs) as using CNs for providing Virtual Network Functions (VNFs) can save costs while increasing the service agility. However, as it is not feasible to realise all types of VNFs in CNs, the coexistence of VMs and CNs is proposed. To put VMs and CNs together, an orchestration framework that can chain services across distributed and heterogeneous domains is required. To this end, we implemented a framework by extending and consolidating state-of-the-art tools and technologies originated from Network Function Virtualization (NFV), Software-defined Networking (SDN) and cloud computing environments. This framework chains services provisioned across Kubernetes and OpenStack domains. During the demo, we deploy a service consist of CN- and VM-based VNFs to demonstrate different features provided by our framework.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=H.+R.+Kouchaksaraei+and+H.+Karl%2C+%E2%80%9CService+function+chaining+across+openstack+and+kubernetes+domains%2C%E2%80%9D+in+Proceedings+of+the+13th+ACM+International+Conference+on+Distributed+and+Event-based+Systems%2C+2019%2C+pp.+240%E2%80%93243.&btnG=
External link:
https://dl.acm.org/doi/abs/10.1145/3328905.3332505?casa_token=LuBiUXRUkWIAAAAA:3fHre3tJVKbdDsQt_SfZahz-V9KMNviMCbHWTbVwJGj5g3Do-pZO9YaLcIO7tK5gvwPQ_oFgfN4
DOI: https://doi.org/10.1007/978-3-658-42798-6_17

id:29  reference: Y. Xiao and M. Watson, “Guidance on conducting a systematic
literature review,” Journal of planning education and research, vol. 39,
no. 1, pp. 93–112, 2019.
Bibtex: 
@article xiao2019guidance,
  title= Guidance on conducting a systematic literature review ,
  author= Xiao, Yu and Watson, Maria ,
  journal= Journal of planning education and research ,
  volume= 39 ,
  number= 1 ,
  pages= 93--112 ,
  year= 2019 ,
  publisher= SAGE Publications Sage CA: Los Angeles, CA 
 
Abstract: Literature reviews establish the foundation of academic inquires. However, in the planning field, we lack rigorous systematic reviews. In this article, through a systematic search on the methodology of literature review, we categorize a typology of literature reviews, discuss steps in conducting a systematic literature review, and provide suggestions on how to enhance rigor in literature reviews in planning education and research.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+Y.+Xiao+and+M.+Watson%2C+%E2%80%9CGuidance+on+conducting+a+systematic+literature+review%2C%E2%80%9D+Journal+of+planning+education+and+research%2C+vol.+39%2C+no.+1%2C+pp.+93%E2%80%93112%2C+2019.&btnG=
External link:
https://journals.sagepub.com/doi/abs/10.1177/0739456X17723971
DOI: https://doi.org/10.1177/0739456X17723971

id:39  reference: M. D. Zeiler and R. Fergus, “Visualizing and understanding
convolutional networks,” in Computer Vision–ECCV 2014: 13th
European Conference, Zurich, Switzerland, September 6-12, 2014, Pro-
ceedings, Part I 13. Springer, 2014, pp. 818–833.
Bibtex: 
@inproceedings zeiler2014visualizing,
  title= Visualizing and understanding convolutional networks ,
  author= Zeiler, Matthew D and Fergus, Rob ,
  booktitle= Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13 ,
  pages= 818--833 ,
  year= 2014 ,
  organization= Springer 
 
Citation : 23173
Abstract: Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]  reference:. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.
Keywords
* Input Image
* Training Image
* Convolutional Neural Network
* Stochastic Gradient Descent
* Pixel Space
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+M.+D.+Zeiler+and+R.+Fergus%2C+%E2%80%9CVisualizing+and+understanding+convolutional+networks%2C%E2%80%9D+in+Computer+Vision%E2%80%93ECCV+2014%3A+13th+European+Conference%2C+Zurich%2C+Switzerland%2C+September+6-12%2C+2014%2C+Pro-+ceedings%2C+Part+I+13.+Springer%2C+2014%2C+pp.+818%E2%80%93833.&btnG=
External link:
https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53

id:40  reference: J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller,
“Striving for simplicity: The all convolutional net,” arXiv preprint
arXiv:1412.6806, 2014.
Bibtex: 
@article springenberg2014striving,
  title= Striving for simplicity: The all convolutional net ,
  author= Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin ,
  journal= arXiv preprint arXiv:1412.6806 ,
  year= 2014 
 
Citation : 5969
Abstract: Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=J.+T.+Springenberg%2C+A.+Dosovitskiy%2C+T.+Brox%2C+and+M.+Riedmiller%2C+%E2%80%9CStriving+for+simplicity%3A+The+all+convolutional+net%2C%E2%80%9D+arXiv+preprint+arXiv%3A1412.6806%2C+2014.&btnG=
External link:
https://arxiv.org/abs/1412.6806
DOI:
https://doi.org/10.48550/arXiv.1412.6806
	
id:41  reference: M. Noroozi and P. Favaro, “Unsupervised learning of visual
representations by solving jigsaw puzzles,” in European conference
on computer vision. Springer, 2016, pp. 69–84.
Bibtex: 
@inproceedings noroozi2016unsupervised,
  title= Unsupervised learning of visual representations by solving jigsaw puzzles ,
  author= Noroozi, Mehdi and Favaro, Paolo ,
  booktitle= European conference on computer vision ,
  pages= 69--84 ,
  year= 2016 ,
  organization= Springer 
 
Citation : 3447
Abstract: We propose a novel unsupervised learning approach to build features suitable for object detection and classification. The features are pre-trained on a large dataset without human annotation and later transferred via fine-tuning on a different, smaller and labeled dataset. The pre-training consists of solving jigsaw puzzles of natural images. To facilitate the transfer of features to other tasks, we introduce the context-free network (CFN), a siamese-ennead convolutional neural network. The features correspond to the columns of the CFN and they process image tiles independently (i.e., free of context). The later layers of the CFN then use the features to identify their geometric arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. We pre-train the CFN on the training set of the ILSVRC2012 dataset and transfer the features on the combined training and validation set of Pascal VOC 2007 for object detection (via fast RCNN) and classification. These features outperform all current unsupervised features for detection and  for classification, and reduce the gap with supervised learning ( and  respectively).
Keywords
* Unsupervised learning
* Image representation learning
* Self-supervised learning
* Feature transfer
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=%5D+M.+Noroozi+and+P.+Favaro%2C+%E2%80%9CUnsupervised+learning+of+visual+representations+by+solving+jigsaw+puzzles%2C%E2%80%9D+in+European+conference+on+computer+vision.+Springer%2C+2016%2C+pp.+69%E2%80%9384.&btnG=
External link:
https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5

id:42  reference: X. Zheng, X. Wu, L. Huan, W. He, and H. Zhang, “A gather-to-
guide network for remote sensing semantic segmentation of rgb
and auxiliary image,” IEEE Transactions on Geoscience and Remote
Sensing, vol. 60, pp. 1–15, 2021.
Bibtex: 
@article zheng2021gather,
  title= A gather-to-guide network for remote sensing semantic segmentation of RGB and auxiliary image ,
  author= Zheng, Xianwei and Wu, Xiujie and Huan, Linxi and He, Wei and Zhang, Hongyan ,
  journal= IEEE Transactions on Geoscience and Remote Sensing ,
  volume= 60 ,
  pages= 1--15 ,
  year= 2021 ,
  publisher= IEEE 
 
Citation : 33
Abstract: Convolutional neural network (CNN)-based feature fusion of RGB and auxiliary remote sensing data is known to enable improved semantic segmentation. However, such fusion is challengeable because of the substantial variance in data characteristics and quality (e.g., data uncertainties and misalignment) between two modality data. In this article, we propose a unified gather-to-guide network (G2GNet) for remote sensing semantic segmentation of RGB and auxiliary data. The key aspect of the proposed architecture is a novel gather-to-guide module (G2GM) that consists of a feature gatherer and a feature guider. The feature gatherer generates a set of cross-modal descriptors by absorbing the complementary merits of RGB and auxiliary modality data. The feature guider calibrates the RGB feature response by using the channel-wise guide weights extracted from the cross-modal descriptors. In this way, the G2GM can perform RGB feature calibration with different modality data in a gather-to-guide fashion, thus preserving the informative features while suppressing redundant and noisy information. Extensive experiments conducted on two benchmark datasets show that the proposed G2GNet is robust to data uncertainties while also improving the semantic segmentation performance of RGB and auxiliary remote sensing data.
Keywords: — Deep learning, remote sensing, semantic segmentation.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=X.+Zheng%2C+X.+Wu%2C+L.+Huan%2C+W.+He%2C+and+H.+Zhang%2C+%E2%80%9CA+gather-to-+guide+network+for+remote+sensing+semantic+segmentation+of+rgb+and+auxiliary+image%2C%E2%80%9D+IEEE+Transactions+on+Geoscience+and+Remote+Sensing%2C+vol.+60%2C+pp.+1%E2%80%9315%2C+2021.&btnG=
External link:
https://ieeexplore.ieee.org/abstract/document/9519842?casa_token=ZihAl5LX75wAAAAA:aUJp2FCf46fODVtal6VylXgWcS2xiX5Cn91QsTVDm6zhY_7_AVdgqmP7AJAKcVD_nrX9GGTn
DOI: 10.1109/TGRS.2021.3103517

id:43  reference: T. Park, M.-Y. Liu, T.-C. Wang, and J.-Y. Zhu, “Semantic image
synthesis with spatially-adaptive normalization,” in Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition,
2019, pp. 2337–2346.
Bibtex: 
@article nguyen2020artificial,
  title= Artificial intelligence in the battle against coronavirus (COVID-19): a survey and future research directions ,
  author= Nguyen, Thanh Thi and Nguyen, Quoc Viet Hung and Nguyen, Dung Tien and Yang, Samuel and Eklund, Peter W and Huynh-The, Thien and Nguyen, Thanh Tam and Pham, Quoc-Viet and Razzak, Imran and Hsu, Edbert B ,
  journal= arXiv preprint arXiv:2008.07343 ,
  year= 2020 
 
Citation : 287
Abstract: We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the network, forcing the network to memorize the information throughout all the layers. Instead, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned affine transformation. Experiments on several challenging datasets demonstrate the superiority of our method compared to existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows users to easily control the style and content of image synthesis results as well as create multi-modal results. Code is available upon publication.
Google scholar link: https://scholar.googleusercontent.com/scholar.bib?q=info:4XfatXFpnRoJ:scholar.google.com/&output=citation&scisdr=ClH65O8MEMCp87a5TpQ:AFWwaeYAAAAAZwC_VpRv8UrmNDwIvBTbjZMb_AY&scisig=AFWwaeYAAAAAZwC_Voz67ldjoZWFvmsNv_udm3I&scisf=4&ct=citation&cd=-1&hl=en
External link:
https://openaccess.thecvf.com/content_CVPR_2019/html/Park_Semantic_Image_Synthesis_With_Spatially-Adaptive_Normalization_CVPR_2019_paper.html

id:44  reference: J. W. Soh, G. Y. Park, J. Jo, and N. I. Cho, “Natural and realistic
single image super-resolution with explicit natural manifold dis-
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 23
crimination,” in Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, 2019, pp. 8122–8131.
Bibtex: 
@inproceedings soh2019natural,
  title= Natural and realistic single image super-resolution with explicit natural manifold discrimination ,
  author= Soh, Jae Woong and Park, Gu Yong and Jo, Junho and Cho, Nam Ik ,
  booktitle= Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,
  pages= 8122--8131 ,
  year= 2019 
 
Citation : 146
Abstract:
Recently, many convolutional neural networks for single image super-resolution (SISR) have been proposed, which focus on reconstructing the high-resolution images in terms of objective distortion measures. However, the networks trained with objective loss functions generally fail to reconstruct the realistic fine textures and details that are essential for better perceptual quality. Recovering the realistic details remains a challenging problem, and only a few works have been proposed which aim at increasing the perceptual quality by generating enhanced textures. However, the generated fake details often make undesirable artifacts and the overall image looks somewhat unnatural. Therefore, in this paper, we present a new approach to reconstructing realistic super-resolved images with high perceptual quality, while maintaining the naturalness of the result. In particular, we focus on the domain prior properties of SISR problem. Specifically, we define the naturalness prior in the low-level domain and constrain the output image in the natural manifold, which eventually generates more natural and realistic images. Our results show better naturalness compared to the recent super-resolution algorithms including perception-oriented ones.
Google scholar link:
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=J.+W.+Soh%2C+G.+Y.+Park%2C+J.+Jo%2C+and+N.+I.+Cho%2C+%E2%80%9CNatural+and+realistic+single+image+super-resolution+with+explicit+natural+manifold+dis-+JOURNAL+OF+LATEX+CLASS+FILES%2C+VOL.+14%2C+NO.+8%2C+AUGUST+2015+23+crimination%2C%E2%80%9D+in+Proceedings+of+the+IEEE%2FCVF+conference+on+computer+vision+and+pattern+recognition%2C+2019%2C+pp.+8122%E2%80%938131.&btnG=
External Link:
https://openaccess.thecvf.com/content_CVPR_2019/html/Soh_Natural_and_Realistic_Single_Image_Super-Resolution_With_Explicit_Natural_Manifold_CVPR_2019_paper.html

id:45  reference: https://research.google/blog/inceptionism-going-deeper-into-neural-networks/.

id:46  reference: https://github.com/mftnakrsu/DeepDream.

id:47  reference: https://github.com/google/deepdream/blob/master/dream.Ipynb.

id:48  reference: https://www.memo.tv/works/journey-through-the-layers-of-the-mind-2015/.

id:49  reference: https://www.wired.com/2015/12/inside-deep-dreams-how-google-made-its-computers-go-crazy/.

id:50  reference: K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside con-
volutional networks: Visualising image classification models and
saliency maps,” arXiv preprint arXiv:1312.6034, 2013.
Bibtex:
@article simonyan2013deep,
  title= Deep inside convolutional networks: Visualising image classification models and saliency maps ,
  author= Simonyan, Karen ,
  journal= arXiv preprint arXiv:1312.6034 ,
  year= 2013 
 
Citation : 8702
Abstract:
This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [5]  reference:, thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13] reference:. 
Google scholar Link:
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=K.+Simonyan%2C+A.+Vedaldi%2C+and+A.+Zisserman%2C+%E2%80%9CDeep+inside+con-+volutional+networks%3A+Visualising+image+classification+models+and+saliency+maps%2C%E2%80%9D+arXiv+preprint+arXiv%3A1312.6034%2C+2013.&btnG=
External Link: 
https://arxiv.org/pdf/1312.6034

id:51  reference: D. Balduzzi, M. Frean, L. Leary, J. Lewis, K. W.-D. Ma, and
B. McWilliams, “The shattered gradients problem: If resnets are
the answer, then what is the question?” in International Conference
on Machine Learning. PMLR, 2017, pp. 342–350.
Bibtex:
@inproceedings balduzzi2017shattered,
  title= The shattered gradients problem: If resnets are the answer, then what is the question? ,
  author= Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian ,
  booktitle= International conference on machine learning ,
  pages= 342--350 ,
  year= 2017 ,
  organization= PMLR 
 
Citation : 457
Abstract:
A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new “looks linear”(LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.
Google Scholar Link:
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=D.+Balduzzi%2C+M.+Frean%2C+L.+Leary%2C+J.+Lewis%2C+K.+W.-D.+Ma%2C+and+B.+McWilliams%2C+%E2%80%9CThe+shattered+gradients+problem%3A+If+resnets+are+the+answer%2C+then+what+is+the+question%3F%E2%80%9D+in+International+Conference+on+Machine+Learning.+PMLR%2C+2017%2C+pp.+342%E2%80%93350.&btnG=
External Link:
https://proceedings.mlr.press/v70/balduzzi17b.html

id:52  reference: M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for
deep networks,” in International conference on machine learning.
PMLR, 2017, pp. 3319–3328.
Bibtex:
@inproceedings sundararajan2017axiomatic,
  title= Axiomatic attribution for deep networks ,
  author= Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi ,
  booktitle= International conference on machine learning ,
  pages= 3319--3328 ,
  year= 2017 ,
  organization= PMLR 
 
Citation : 6652
Abstract:
We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.
Google Scholar Link:
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=M.+Sundararajan%2C+A.+Taly%2C+and+Q.+Yan%2C+%E2%80%9CAxiomatic+attribution+for+deep+networks%2C%E2%80%9D+in+International+conference+on+machine+learning.+PMLR%2C+2017%2C+pp.+3319%E2%80%933328.&btnG=
External Link:
https://proceedings.mlr.press/v70/sundararajan17a.html

id:53  reference: M. T. Ribeiro, S. Singh, and C. Guestrin, “” why should i trust
you?” explaining the predictions of any classifier,” in Proceedings
of the 22nd ACM SIGKDD international conference on knowledge
discovery and data mining, 2016, pp. 1135–1144.
Bibtex:
@inproceedings ribeiro2016should,
  title= " Why should i trust you?" Explaining the predictions of any classifier ,
  author= Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos ,
  booktitle= Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining ,
  pages= 1135--1144 ,
  year= 2016 
 
Citation : 19603
Abstract:
Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.
In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=M.+T.+Ribeiro%2C+S.+Singh%2C+and+C.+Guestrin%2C+%E2%80%9C%E2%80%9D+why+should+i+trust+you%3F%E2%80%9D+explaining+the+predictions+of+any+classifier%2C%E2%80%9D+in+Proceedings+of+the+22nd+ACM+SIGKDD+international+conference+on+knowledge+discovery+and+data+mining%2C+2016%2C+pp.+1135%E2%80%931144.&btnG=
External Link: 
https://dl.acm.org/doi/abs/10.1145/2939672.2939778

id:54  reference: B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba,
“Learning deep features for discriminative localization,” in Pro-
ceedings of the IEEE conference on computer vision and pattern recogni-
tion, 2016, pp. 2921–2929.
Bibtex: 
@inproceedings zhou2016learning,
  title= Learning deep features for discriminative localization ,
  author= Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio ,
  booktitle= Proceedings of the IEEE conference on computer vision and pattern recognition ,
  pages= 2921--2929 ,
  year= 2016 
 
Citation : 11940
Abstract: 
In this work, we revisit the global average pooling layer proposed in [13]  reference:, and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them.
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=B.+Zhou%2C+A.+Khosla%2C+A.+Lapedriza%2C+A.+Oliva%2C+and+A.+Torralba%2C+%E2%80%9CLearning+deep+features+for+discriminative+localization%2C%E2%80%9D+in+Pro-+ceedings+of+the+IEEE+conference+on+computer+vision+and+pattern+recogni-+tion%2C+2016%2C+pp.+2921%E2%80%932929.&btnG=
External Link: 
https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html

id:55  reference: R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, “Grad-cam: Visual explanations from deep networks via
gradient-based localization,” in Proceedings of the IEEE international
conference on computer vision, 2017, pp. 618–626.
Bibtex: 
@inproceedings selvaraju2017grad,
  title= Grad-cam: Visual explanations from deep networks via gradient-based localization ,
  author= Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv ,
  booktitle= Proceedings of the IEEE international conference on computer vision ,
  pages= 618--626 ,
  year= 2017 
 
Citation : 20136
Abstract: 
We propose a technique for producing 'visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for 'dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. VQA) or reinforcement learning, and needs no architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a 'stronger' deep network from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/ along with a demo on CloudCV [2]  reference: 1 and video at youtu.be/COjUB9Izk6E.
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=R.+R.+Selvaraju%2C+M.+Cogswell%2C+A.+Das%2C+R.+Vedantam%2C+D.+Parikh%2C+and+D.+Batra%2C+%E2%80%9CGrad-cam%3A+Visual+explanations+from+deep+networks+via+gradient-based+localization%2C%E2%80%9D+in+Proceedings+of+the+IEEE+international+conference+on+computer+vision%2C+2017%2C+pp.+618%E2%80%93626.&btnG=
External Link: 
https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html

id:56  reference: A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubra-
manian, “Grad-cam++: Generalized gradient-based visual expla-
nations for deep convolutional networks,” in 2018 IEEE winter
conference on applications of computer vision (WACV). IEEE, 2018,
pp. 839–847.
Bibtex: 
@inproceedings chattopadhay2018grad,
  title= Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks ,
  author= Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N ,
  booktitle= 2018 IEEE winter conference on applications of computer vision (WACV) ,
  pages= 839--847 ,
  year= 2018 ,
  organization= IEEE 
 
Citation : 2944
Abstract: 
Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision based problems. However, deep models are perceived as "black box" methods considering the lack of understanding of their internal functioning. There has been a significant recent interest to develop explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose Grad-CAM++ to provide better visual explanations of CNN model predictions (when compared to Grad-CAM), in terms of better localization of objects as well as explaining occurrences of multiple objects of a class in a single image. We provide a mathematical explanation for the proposed method, Grad-CAM++, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the class label under consideration. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ indeed provides better visual explanations for a given CNN architecture when compared to Grad-CAM.
IEEE Keywords
* Visualization,
* Heating systems,
* Neurons,
* Machine learning,
* Predictive models,
* Mathematical model
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+A.+Chattopadhay%2C+A.+Sarkar%2C+P.+Howlader%2C+and+V.+N.+Balasubra-+manian%2C+%E2%80%9CGrad-cam%2B%2B%3A+Generalized+gradient-based+visual+expla-+nations+for+deep+convolutional+networks%2C%E2%80%9D+in+2018+IEEE+winter+conference+on+applications+of+computer+vision+%28WACV%29.+IEEE%2C+2018%2C+pp.+839%E2%80%93847.&btnG=
External Link: 
https://ieeexplore.ieee.org/document/8354201

id:57  reference: D. Smilkov, N. Thorat, B. Kim, F. Vi´egas, and M. Wattenberg,
“Smoothgrad: removing noise by adding noise,” arXiv preprint
arXiv:1706.03825, 2017.
Bibtex: 
@article smilkov2017smoothgrad,
  title= Smoothgrad: removing noise by adding noise ,
  author= Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi \'e gas, Fernanda and Wattenberg, Martin ,
  journal= arXiv preprint arXiv:1706.03825 ,
  year= 2017 
 
Citation : 2440
Abstract: 
Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=D.+Smilkov%2C+N.+Thorat%2C+B.+Kim%2C+F.+Vi%C2%B4egas%2C+and+M.+Wattenberg%2C+%E2%80%9CSmoothgrad%3A+removing+noise+by+adding+noise%2C%E2%80%9D+arXiv+preprint+arXiv%3A1706.03825%2C+2017.&btnG=
External Link: 
https://arxiv.org/abs/1706.03825
DOI Link
https://doi.org/10.48550/arXiv.1706.03825
	

id:58  reference: D. Omeiza, S. Speakman, C. Cintas, and K. Weldermariam,
“Smooth grad-cam++: An enhanced inference level visualization
technique for deep convolutional neural network models,” arXiv
preprint arXiv:1908.01224, 2019.
Bibtex: 
@article omeiza2019smooth,
  title= Smooth grad-cam++: An enhanced inference level visualization technique for deep convolutional neural network models ,
  author= Omeiza, Daniel and Speakman, Skyler and Cintas, Celia and Weldermariam, Komminist ,
  journal= arXiv preprint arXiv:1908.01224 ,
  year= 2019 
 
Citation : 267
Abstract: 
Gaining insight into how deep convolutional neural network models perform image classification and how to explain their outputs have been a concern to computer vision researchers and decision makers. These deep models are often referred to as black box due to low comprehension of their internal workings. As an effort to developing explainable deep learning models, several methods have been proposed such as finding gradients of class output with respect to input image (sensitivity maps), class activation map (CAM), and Gradient based Class Activation Maps (Grad-CAM). These methods under perform when localizing multiple occurrences of the same class and do not work for all CNNs. In addition, Grad-CAM does not capture the entire object in completeness when used on single object images, this affect performance on recognition tasks. With the intention to create an enhanced visual explanation in terms of visual sharpness, object localization and explaining multiple occurrences of objects in a single image, we present Smooth Grad-CAM++ \footnote Simple demo: http://35.238.22.135:5000/ , a technique that combines methods from two other recent techniques---SMOOTHGRAD and Grad-CAM++. Our Smooth Grad-CAM++ technique provides the capability of either visualizing a layer, subset of feature maps, or subset of neurons within a feature map at each instance at the inference level (model prediction process). After experimenting with few images, Smooth Grad-CAM++ produced more visually sharp maps with better localization of objects in the given input images when compared with other methods.
Keywords: Computer Vision, Convolutional Neural Network, Class Activation Maps
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=D.+Omeiza%2C+S.+Speakman%2C+C.+Cintas%2C+and+K.+Weldermariam%2C+%E2%80%9CSmooth+grad-cam%2B%2B%3A+An+enhanced+inference+level+visualization+technique+for+deep+convolutional+neural+network+models%2C%E2%80%9D+arXiv+preprint+arXiv%3A1908.01224%2C+2019.&btnG=
External Link:
https://arxiv.org/abs/1908.01224

id:59  reference: M. D. Zeiler and R. Fergus, “Stochastic pooling for regular-
ization of deep convolutional neural networks,” arXiv preprint
arXiv:1301.3557, 2013.
Bibtex: 
@article zeiler2013stochastic,
  title= Stochastic pooling for regularization of deep convolutional neural networks ,
  author= Zeiler, Matthew D and Fergus, Rob ,
  journal= arXiv preprint arXiv:1301.3557 ,
  year= 2013 
 
Citation : 1358
Abstract: 
We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=M.+D.+Zeiler+and+R.+Fergus%2C+%E2%80%9CStochastic+pooling+for+regular-+ization+of+deep+convolutional+neural+networks%2C%E2%80%9D+arXiv+preprint+arXiv%3A1301.3557%2C+2013.&btnG=
External Link: 
https://arxiv.org/abs/1301.3557
DOI Link:
https://doi.org/10.48550/arXiv.1301.3557
	

id:60  reference: G. Zhao, B. Zhou, K. Wang, R. Jiang, and M. Xu, “Respond-cam:
Analyzing deep models for 3d imaging data by visualizations,”
in Medical Image Computing and Computer Assisted Intervention–
MICCAI 2018: 21st International Conference, Granada, Spain, Septem-
ber 16-20, 2018, Proceedings, Part I. Springer, 2018, pp. 485–492.
Bibtex:
@inproceedings zhao2018respond,
  title= Respond-cam: Analyzing deep models for 3d imaging data by visualizations ,
  author= Zhao, Guannan and Zhou, Bo and Wang, Kaiwen and Jiang, Rui and Xu, Min ,
  booktitle= Medical Image Computing and Computer Assisted Intervention--MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part I ,
  pages= 485--492 ,
  year= 2018 ,
  organization= Springer 
 
Citation : 58
Abstract: 
The convolutional neural network (CNN) has become a powerful tool for various biomedical image analysis tasks, but there is a lack of visual explanation for the machinery of CNNs. In this paper, we present a novel algorithm, Respond-weighted Class Activation Mapping (Respond-CAM), for making CNN-based models interpretable by visualizing input regions that are important for predictions, especially for biomedical 3D imaging data inputs. Our method uses the gradients of any target concept (e.g. the score of target class) that flow into a convolutional layer. The weighted feature maps are combined to produce a heatmap that highlights the important regions in the image for predicting the target concept. We prove a preferable sum-to-score property of the Respond-CAM and verify its significant improvement on 3D images from the current state-of-the-art approach. Our tests on Cellular Electron Cryo-Tomography 3D images show that Respond-CAM achieves superior performance on visualizing the CNNs with 3D biomedical image inputs, and is able to get reasonably good results on visualizing the CNNs with natural image inputs. The Respond-CAM is an efficient and reliable approach for visualizing the CNN machinery, and is applicable to a wide variety of CNN model families and image analysis tasks. Our code is available at: https://github.com/xulabs/projects/tree/master/respond_cam.
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=G.+Zhao%2C+B.+Zhou%2C+K.+Wang%2C+R.+Jiang%2C+and+M.+Xu%2C+%E2%80%9CRespond-cam%3A+Analyzing+deep+models+for+3d+imaging+data+by+visualizations%2C%E2%80%9D+in+Medical+Image+Computing+and+Computer+Assisted+Intervention%E2%80%93+MICCAI+2018%3A+21st+International+Conference%2C+Granada%2C+Spain%2C+Septem-+ber+16-20%2C+2018%2C+Proceedings%2C+Part+I.+Springer%2C+2018%2C+pp.+485%E2%80%93492.&btnG=
External Link: 
https://link.springer.com/chapter/10.1007/978-3-030-00928-1_55

id:61  reference: A. Mordvintsev, C. Olah, and M. Tyka, “Inceptionism: Going
deeper into neural networks,” Google research blog, vol. 20, no. 14,
p. 5, 2015.
Bibtex: 
@article mordvintsev2015inceptionism,
  title= Inceptionism: Going deeper into neural networks ,
  author= Mordvintsev, Alexander and Olah, Christopher and Tyka, Mike ,
  journal= Google research blog ,
  volume= 20 ,
  number= 14 ,
  pages= 5 ,
  year= 2015 
 
Citation : 784
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Mordvintsev%2C+C.+Olah%2C+and+M.+Tyka%2C+%E2%80%9CInceptionism%3A+Going+deeper+into+neural+networks%2C%E2%80%9D+Google+research+blog%2C+vol.+20%2C+no.+14%2C+p.+5%2C+2015.&btnG=
https://research.google/pubs/inceptionism-going-deeper-into-neural-networks/

id:62  reference: S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. M ¨uller, and
W. Samek, “On pixel-wise explanations for non-linear classifier
decisions by layer-wise relevance propagation,” PloS one, vol. 10,
no. 7, p. e0130140, 2015.
Bibtex: 
@article bach2015pixel,
  title= On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation ,
  author= Bach, Sebastian and Binder, Alexander and Montavon, Gr \'e goire and Klauschen, Frederick and M \"u ller, Klaus-Robert and Samek, Wojciech ,
  journal= PloS one ,
  volume= 10 ,
  number= 7 ,
  pages= e0130140 ,
  year= 2015 ,
  publisher= Public Library of Science San Francisco, CA USA 
 
Citation : 5036
Abstract: 
Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.
Google Scholar: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=S.+Bach%2C+A.+Binder%2C+G.+Montavon%2C+F.+Klauschen%2C+K.-R.+M+%C2%A8uller%2C+and+W.+Samek%2C+%E2%80%9COn+pixel-wise+explanations+for+non-linear+classifier+decisions+by+layer-wise+relevance+propagation%2C%E2%80%9D+PloS+one%2C+vol.+10%2C+no.+7%2C+p.+e0130140%2C+2015.&btnG=
External Link: 
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140

id:63  reference: M. Ancona, E. Ceolini, C. ¨Oztireli, and M. Gross, “Towards better
understanding of gradient-based attribution methods for deep
neural networks,” arXiv preprint arXiv:1711.06104, 2017.
Bibtex: 
@article ancona2017towards,
  title= Towards better understanding of gradient-based attribution methods for deep neural networks ,
  author= Ancona, Marco and Ceolini, Enea and  \"O ztireli, Cengiz and Gross, Markus ,
  journal= arXiv preprint arXiv:1711.06104 ,
  year= 2017 
 
Citation : 1185
Abstract: 
Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.
Google Scholar: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=M.+Ancona%2C+E.+Ceolini%2C+C.+%C2%A8Oztireli%2C+and+M.+Gross%2C+%E2%80%9CTowards+better+understanding+of+gradient-based+attribution+methods+for+deep+neural+networks%2C%E2%80%9D+arXiv+preprint+arXiv%3A1711.06104%2C+2017.&btnG=
External Link: 
https://arxiv.org/abs/1711.06104
DOI LINK: 
https://doi.org/10.48550/arXiv.1711.06104
	

id:64  reference: J. Gu, Y. Yang, and V. Tresp, “Understanding individual de-
cisions of cnns via contrastive backpropagation,” in Computer
Vision–ACCV 2018: 14th Asian Conference on Computer Vision, Perth,
Australia, December 2–6, 2018, Revised Selected Papers, Part III 14.
Springer, 2019, pp. 119–134.
Bibtex: 
@inproceedings gu2019understanding,
  title= Understanding individual decisions of cnns via contrastive backpropagation ,
  author= Gu, Jindong and Yang, Yinchong and Tresp, Volker ,
  booktitle= Computer Vision--ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2--6, 2018, Revised Selected Papers, Part III 14 ,
  pages= 119--134 ,
  year= 2019 ,
  organization= Springer 
 
Citation : 112
Abstract: 
A number of backpropagation-based approaches such as DeConvNets, vanilla Gradient Visualization and Guided Backpropagation have been proposed to better understand individual decisions of deep convolutional neural networks. The saliency maps produced by them are proven to be non-discriminative. Recently, the Layer-wise Relevance Propagation (LRP) approach was proposed to explain the classification decisions of rectifier neural networks. In this work, we evaluate the discriminativeness of the generated explanations and analyze the theoretical foundation of LRP, i.e. Deep Taylor Decomposition. The experiments and analysis conclude that the explanations generated by LRP are not class-discriminative. Based on LRP, we propose Contrastive Layer-wise Relevance Propagation (CLRP), which is capable of producing instance-specific, class-discriminative, pixel-wise explanations. In the experiments, we use the CLRP to explain the decisions and understand the difference between neurons in individual classification decisions. We also evaluate the explanations quantitatively with a Pointing Game and an ablation study. Both qualitative and quantitative evaluations show that the CLRP generates better explanations than the LRP.
Keywords
* Explainable deep learning
* LRP
* Discriminative saliency maps
Google Scholar: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=J.+Gu%2C+Y.+Yang%2C+and+V.+Tresp%2C+%E2%80%9CUnderstanding+individual+de-+cisions+of+cnns+via+contrastive+backpropagation%2C%E2%80%9D+in+Computer+Vision%E2%80%93ACCV+2018%3A+14th+Asian+Conference+on+Computer+Vision%2C+Perth%2C+Australia%2C+December+2%E2%80%936%2C+2018%2C+Revised+Selected+Papers%2C+Part+III+14.+Springer%2C+2019%2C+pp.+119%E2%80%93134.&btnG=
External Link: 
https://link.springer.com/chapter/10.1007/978-3-030-20893-6_8

id:65  reference: B. K. Iwana, R. Kuroki, and S. Uchida, “Explaining convolu-
tional neural networks using softmax gradient layer-wise rele-
vance propagation,” in 2019 IEEE/CVF International Conference on
Computer Vision Workshop (ICCVW). IEEE, 2019, pp. 4176–4185.
Bibtex: 
@inproceedings iwana2019explaining,
  title= Explaining convolutional neural networks using softmax gradient layer-wise relevance propagation ,
  author= Iwana, Brian Kenji and Kuroki, Ryohei and Uchida, Seiichi ,
  booktitle= 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW) ,
  pages= 4176--4185 ,
  year= 2019 ,
  organization= IEEE 
 
Citation : 101
Abstract: 
Convolutional Neural Networks (CNN) have become state-of-the-art in the field of image classification. However, not everything is understood about their inner representations. This paper tackles the interpretability and explainability of the predictions of CNNs for multi-class classification problems. Specifically, we propose a novel visualization method of pixel-wise input attribution called Softmax-Gradient Layer-wise Relevance Propagation (SGLRP). The proposed model is a class discriminate extension to Deep Taylor Decomposition (DTD) using the gradient of softmax to back propagate the relevance of the output probability to the input image. Through qualitative and quantitative analysis, we demonstrate that SGLRP can successfully localize and attribute the regions on input images which contribute to a target object's classification. We show that the proposed method excels at discriminating the target objects class from the other possible objects in the images. We confirm that SGLRP performs better than existing Layer-wise Relevance Propagation (LRP) based methods and can help in the understanding of the decision process of CNNs.
IEEE Keywords
* Visualization,
* Heating systems,
* Convolutional neural networks,
* Machine learning,
* Robustness,
* Computer vision
Google Scholar : 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=B.+K.+Iwana%2C+R.+Kuroki%2C+and+S.+Uchida%2C+%E2%80%9CExplaining+convolu-+tional+neural+networks+using+softmax+gradient+layer-wise+rele-+vance+propagation%2C%E2%80%9D+in+2019+IEEE%2FCVF+International+Conference+on+Computer+Vision+Workshop+%28ICCVW%29.+IEEE%2C+2019%2C+pp.+4176%E2%80%934185.&btnG=
External Link: 
https://ieeexplore.ieee.org/document/9022542

id:66  reference: R. Achtibat, M. Dreyer, I. Eisenbraun, S. Bosse, T. Wiegand,
W. Samek, and S. Lapuschkin, “From attribution maps to human-
understandable explanations through concept relevance propaga-
tion,” Nature Machine Intelligence, vol. 5, no. 9, pp. 1006–1019, 2023.
Bibtex: 
@article achtibat2023attribution,
  title= From attribution maps to human-understandable explanations through concept relevance propagation ,
  author= Achtibat, Reduan and Dreyer, Maximilian and Eisenbraun, Ilona and Bosse, Sebastian and Wiegand, Thomas and Samek, Wojciech and Lapuschkin, Sebastian ,
  journal= Nature Machine Intelligence ,
  volume= 5 ,
  number= 9 ,
  pages= 1006--1019 ,
  year= 2023 ,
  publisher= Nature Publishing Group UK London 
 
Citation : 71
Abstract: 
The field of explainable artificial intelligence (XAI) aims to bring transparency to today’s powerful but opaque deep learning models. While local XAI methods explain individual predictions in the form of attribution maps, thereby identifying ‘where’ important features occur (but not providing information about ‘what’ they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of method thus provide only partial insights and leave the burden of interpreting the model’s reasoning to the user. Here we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives and thus allows answering both the ‘where’ and ‘what’ questions for individual predictions. We demonstrate the capability of our method in various settings, showcasing that CRP leads to more human interpretable explanations and provides deep insights into the model’s representation and reasoning through concept atlases, concept-composition analyses, and quantitative investigations of concept subspaces and their role in fine-grained decision-making.
Google Scholar: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=R.+Achtibat%2C+M.+Dreyer%2C+I.+Eisenbraun%2C+S.+Bosse%2C+T.+Wiegand%2C+W.+Samek%2C+and+S.+Lapuschkin%2C+%E2%80%9CFrom+attribution+maps+to+human-+understandable+explanations+through+concept+relevance+propaga-+tion%2C%E2%80%9D+Nature+Machine+Intelligence%2C+vol.+5%2C+no.+9%2C+pp.+1006%E2%80%931019%2C+2023.&btnG=
External Link: 
https://www.nature.com/articles/s42256-023-00711-8

id:67  reference: A. Shrikumar, P. Greenside, and A. Kundaje, “Learning important
features through propagating activation differences,” in Interna-
tional conference on machine learning. PMLR, 2017, pp. 3145–3153.
Bibtex: 
@inproceedings shrikumar2017learning,
  title= Learning important features through propagating activation differences ,
  author= Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul ,
  booktitle= International conference on machine learning ,
  pages= 3145--3153 ,
  year= 2017 ,
  organization= PMlR 
 
Citation : 4683
Abstract: 
The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to itsreference activation’and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo. gl/qKb7pL code: http://goo. gl/RM8jvH
Google Scholar: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Shrikumar%2C+P.+Greenside%2C+and+A.+Kundaje%2C+%E2%80%9CLearning+important+features+through+propagating+activation+differences%2C%E2%80%9D+in+Interna-+tional+conference+on+machine+learning.+PMLR%2C+2017%2C+pp.+3145%E2%80%933153.&btnG=
External Link: 
https://proceedings.mlr.press/v70/shrikumar17a

id:68  reference: S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting model predictions,” Advances in neural information processing
systems, vol. 30, 2017.
Bibtex: 
@article lundberg2017unified,
  title= A unified approach to interpreting model predictions ,
  author= Lundberg, Scott ,
  journal= arXiv preprint arXiv:1705.07874 ,
  year= 2017 
 
Citation : 26651
Abstract: 
Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=S.+M.+Lundberg+and+S.-I.+Lee%2C+%E2%80%9CA+unified+approach+to+interpreting+model+predictions%2C%E2%80%9D+Advances+in+neural+information+processing+systems%2C+vol.+30%2C+2017.&btnG=
External Link: 
https://arxiv.org/abs/1705.07874
DOI LINK: 
https://doi.org/10.48550/arXiv.1705.07874
	

id:69  reference: S. M. Lundberg, G. Erion, H. Chen, A. DeGrave, J. M. Prutkin,
B. Nair, R. Katz, J. Himmelfarb, N. Bansal, and S.-I. Lee, “Explain-
able ai for trees: From local explanations to global understanding,”
arXiv preprint arXiv:1905.04610, 2019.
Bibtex: 
@article lundberg2019explainable,
  title= Explainable AI for trees: From local explanations to global understanding ,
  author= Lundberg, Scott M and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In ,
  journal= arXiv preprint arXiv:1905.04610 ,
  year= 2019 
 
Citation : 383
Abstract: 
Tree-based machine learning models such as random forests, decision trees, and gradient boosted trees are the most popular non-linear predictive models used in practice today, yet comparatively little attention has been paid to explaining their predictions. Here we significantly improve the interpretability of tree-based models through three main contributions: 1) The first polynomial time algorithm to compute optimal explanations based on game theory. 2) A new type of explanation that directly measures local feature interaction effects. 3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the general US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=S.+M.+Lundberg%2C+G.+Erion%2C+H.+Chen%2C+A.+DeGrave%2C+J.+M.+Prutkin%2C+B.+Nair%2C+R.+Katz%2C+J.+Himmelfarb%2C+N.+Bansal%2C+and+S.-I.+Lee%2C+%E2%80%9CExplain-+able+ai+for+trees%3A+From+local+explanations+to+global+understanding%2C%E2%80%9D+arXiv+preprint+arXiv%3A1905.04610%2C+2019.&btnG=
Exetrnal Link: 
https://arxiv.org/abs/1905.04610
DOI LINK: 
https://doi.org/10.48550/arXiv.1905.04610
	

id:70  reference: X. Huang, S. Jamonnak, Y. Zhao, T. H. Wu, and W. Xu, “A
visual designer of layer-wise relevance propagation models,” in
Computer Graphics Forum, vol. 40, no. 3. Wiley Online Library,
2021, pp. 227–238.
Bibtex: 
@inproceedings huang2021visual,
  title= A Visual Designer of Layer-wise Relevance Propagation Models ,
  author= Huang, Xinyi and Jamonnak, Suphanut and Zhao, Ye and Wu, Tsung Heng and Xu, Wei ,
  booktitle= Computer Graphics Forum ,
  volume= 40 ,
  number= 3 ,
  pages= 227--238 ,
  year= 2021 ,
  organization= Wiley Online Library 
 
Citation : 16
Abstract: 
Layer‐wise Relevance Propagation (LRP) is an emerging and widely‐used method for interpreting the prediction results of convolutional neural networks (CNN). LRP developers often select and employ different relevance backpropagation rules and parameters, to compute relevance scores on input images. However, there exists no obvious solution to define a “best” LRP model. A satisfied model is highly reliant on pertinent images and designers' goals. We develop a visual model designer, named as VisLRPDesigner, to overcome the challenges in the design and use of LRP models. Various LRP rules are unified into an integrated framework with an intuitive workflow of parameter setup. VisLRPDesigner thus allows users to interactively configure and compare LRP models. It also facilitates relevance‐based visual analysis with two important functions: relevance‐based pixel flipping and neuron ablation. Several use cases illustrate the benefits of VisLRPDesigner. The usability and limitation of the visual designer is evaluated by LRP users.
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=X.+Huang%2C+S.+Jamonnak%2C+Y.+Zhao%2C+T.+H.+Wu%2C+and+W.+Xu%2C+%E2%80%9CA+visual+designer+of+layer-wise+relevance+propagation+models%2C%E2%80%9D+in+Computer+Graphics+Forum%2C+vol.+40%2C+no.+3.+Wiley+Online+Library%2C+2021%2C+pp.+227%E2%80%93238.&btnG=
External Link: 
https://doi.org/10.1111/cgf.14302

id:71  reference: L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Ka-
gal, “Explaining explanations: An overview of interpretability of
machine learning,” in 2018 IEEE 5th International Conference on data
science and advanced analytics (DSAA). IEEE, 2018, pp. 80–89.
Bibtex: 
@inproceedings gilpin2018explaining,
  title= Explaining explanations: An overview of interpretability of machine learning ,
  author= Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana ,
  booktitle= 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA) ,
  pages= 80--89 ,
  year= 2018 ,
  organization= IEEE 
 
Citation : 2818
Abstract: 
There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.
IEEE Keywords
* Artificial intelligence,
* Computational modeling,
* Decision trees,
* Biological neural networks,
* Taxonomy,
* Complexity theory
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=L.+H.+Gilpin%2C+D.+Bau%2C+B.+Z.+Yuan%2C+A.+Bajwa%2C+M.+Specter%2C+and+L.+Ka-+gal%2C+%E2%80%9CExplaining+explanations%3A+An+overview+of+interpretability+of+machine+learning%2C%E2%80%9D+in+2018+IEEE+5th+International+Conference+on+data+science+and+advanced+analytics+%28DSAA%29.+IEEE%2C+2018%2C+pp.+80%E2%80%9389.&btnG=
External Link: 
https://ieeexplore.ieee.org/document/8631448

id:72  reference: A. Binder, G. Montavon, S. Lapuschkin, K.-R. M ¨uller, and
W. Samek, “Layer-wise relevance propagation for neural networks
with local renormalization layers,” in Artificial Neural Networks
and Machine Learning–ICANN 2016: 25th International Conference
on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016,
Proceedings, Part II 25. Springer, 2016, pp. 63–71.
Bibtex: 
@inproceedings binder2016layer,
  title= Layer-wise relevance propagation for neural networks with local renormalization layers ,
  author= Binder, Alexander and Montavon, Gr \'e goire and Lapuschkin, Sebastian and M \"u ller, Klaus-Robert and Samek, Wojciech ,
  booktitle= Artificial Neural Networks and Machine Learning--ICANN 2016: 25th International Conference on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016, Proceedings, Part II 25 ,
  pages= 63--71 ,
  year= 2016 ,
  organization= Springer 
 
Citation : 547
Abstract: 
Layer-wise relevance propagation is a framework which allows to decompose the prediction of a deep neural network computed over a sample, e.g. an image, down to relevance scores for the single input dimensions of the sample such as subpixels of an image. While this approach can be applied directly to generalized linear mappings, product type non-linearities are not covered. This paper proposes an approach to extend layer-wise relevance propagation to neural networks with local renormalization layers, which is a very common product-type non-linearity in convolutional neural networks. We evaluate the proposed method for local renormalization layers on the CIFAR-10, Imagenet and MIT Places datasets
Keywords
* Neural networks
* Image classification
* Interpretability
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Binder%2C+G.+Montavon%2C+S.+Lapuschkin%2C+K.-R.+M+%C2%A8uller%2C+and+W.+Samek%2C+%E2%80%9CLayer-wise+relevance+propagation+for+neural+networks+with+local+renormalization+layers%2C%E2%80%9D+in+Artificial+Neural+Networks+and+Machine+Learning%E2%80%93ICANN+2016%3A+25th+International+Conference+on+Artificial+Neural+Networks%2C+Barcelona%2C+Spain%2C+September+6-9%2C+2016%2C+Proceedings%2C+Part+II+25.+Springer%2C+2016%2C+pp.+63%E2%80%9371.&btnG=
External Link: 
https://link.springer.com/chapter/10.1007/978-3-319-44781-0_8

id:73  reference: B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas et al.,
“Interpretability beyond feature attribution: Quantitative testing
with concept activation vectors (tcav),” in International conference
on machine learning. PMLR, 2018, pp. 2668–2677.
Bibtex: 
@inproceedings kim2018interpretability,
  title= Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav) ,
  author= Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and others ,
  booktitle= International conference on machine learning ,
  pages= 2668--2677 ,
  year= 2018 ,
  organization= PMLR 
 
Citation : 2030
Abstract : 
The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result–for example, how sensitive a prediction of “zebra” is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+B.+Kim%2C+M.+Wattenberg%2C+J.+Gilmer%2C+C.+Cai%2C+J.+Wexler%2C+F.+Viegas+et+al.%2C+%E2%80%9CInterpretability+beyond+feature+attribution%3A+Quantitative+testing+with+concept+activation+vectors+%28tcav%29%2C%E2%80%9D+in+International+conference+on+machine+learning.+PMLR%2C+2018%2C+pp.+2668%E2%80%932677.&btnG=
External Link: 
https://proceedings.mlr.press/v80/kim18d.html

id:74  reference: Y. Goyal, A. Feder, U. Shalit, and B. Kim, “Explaining classifiers
with causal concept effect (cace),” arXiv preprint arXiv:1907.07165,
2019.
Bibtex: 
@article goyal2019explaining,
  title= Explaining classifiers with causal concept effect (cace) ,
  author= Goyal, Yash and Feder, Amir and Shalit, Uri and Kim, Been ,
  journal= arXiv preprint arXiv:1907.07165 ,
  year= 2019 
 
Citation : 178
Abstract: 
How can we understand classification decisions made by deep neural networks? Many existing explainability methods rely solely on correlations and fail to account for confounding, which may result in potentially misleading explanations. To overcome this problem, we define the Causal Concept Effect (CaCE) as the causal effect of (the presence or absence of) a human-interpretable concept on a deep neural net's predictions. We show that the CaCE measure can avoid errors stemming from confounding. Estimating CaCE is difficult in situations where we cannot easily simulate the do-operator. To mitigate this problem, we use a generative model, specifically a Variational AutoEncoder (VAE), to measure VAE-CaCE. In an extensive experimental analysis, we show that the VAE-CaCE is able to estimate the true concept causal effect, compared to baselines for a number of datasets including high dimensional images.
Google Scholar: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=Y.+Goyal%2C+A.+Feder%2C+U.+Shalit%2C+and+B.+Kim%2C+%E2%80%9CExplaining+classifiers+with+causal+concept+effect+%28cace%29%2C%E2%80%9D+arXiv+preprint+arXiv%3A1907.07165%2C+2019.&btnG=
External Link: 
https://arxiv.org/abs/1907.07165
DOI LINK: 
https://doi.org/10.48550/arXiv.1907.07165
	

id:75  reference: C.-K. Yeh, B. Kim, S. Arik, C.-L. Li, T. Pfister, and P. Ravikumar,
“On completeness-aware concept-based explanations in deep neu-
ral networks,” Advances in neural information processing systems,
vol. 33, pp. 20 554–20 565, 2020.
Bibtex: 
@article yeh2020completeness,
  title= On completeness-aware concept-based explanations in deep neural networks ,
  author= Yeh, Chih-Kuan and Kim, Been and Arik, Sercan and Li, Chun-Liang and Pfister, Tomas and Ravikumar, Pradeep ,
  journal= Advances in neural information processing systems ,
  volume= 33 ,
  pages= 20554--20565 ,
  year= 2020 
 
Citation : 311
Abstract: 
Human explanations of high-level decisions are often expressed in terms of key concepts the decisions are based on. In this paper, we study such concept-based explainability for Deep Neural Networks (DNNs). First, we define the notion of\emph  completeness , which quantifies how sufficient a particular set of concepts is in explaining a model's prediction behavior based on the assumption that complete concept scores are sufficient statistics of the model prediction. Next, we propose a concept discovery method that aims to infer a complete set of concepts that are additionally encouraged to be interpretable, which addresses the limitations of existing methods on concept explanations. To define an importance score for each discovered concept, we adapt game-theoretic notions to aggregate over sets and propose\emph  ConceptSHAP . Via proposed metrics and user studies, on a synthetic dataset with apriori-known concept explanations, as well as on real-world image and language datasets, we validate the effectiveness of our method in finding concepts that are both complete in explaining the decisions and interpretable.
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=C.-K.+Yeh%2C+B.+Kim%2C+S.+Arik%2C+C.-L.+Li%2C+T.+Pfister%2C+and+P.+Ravikumar%2C+%E2%80%9COn+completeness-aware+concept-based+explanations+in+deep+neu-+ral+networks%2C%E2%80%9D+Advances+in+neural+information+processing+systems%2C+vol.+33%2C+pp.+20+554%E2%80%9320+565%2C+2020.&btnG=
External Link: 
https://proceedings.neurips.cc/paper/2020/hash/ecb287ff763c169694f682af52c1f309-Abstract.html

id:76  reference: N. Kokhlikyan, V. Miglani, M. Martin, E. Wang, B. Alsallakh,
J. Reynolds, A. Melnikov, N. Kliushkina, C. Araya, S. Yan et al.,
“Captum: A unified and generic model interpretability library for
pytorch,” arXiv preprint arXiv:2009.07896, 2020.
Bibtex: 
@article kokhlikyan2020captum,
  title= Captum: A unified and generic model interpretability library for pytorch ,
  author= Kokhlikyan, Narine and Miglani, Vivek and Martin, Miguel and Wang, Edward and Alsallakh, Bilal and Reynolds, Jonathan and Melnikov, Alexander and Kliushkina, Natalia and Araya, Carlos and Yan, Siqi and others ,
  journal= arXiv preprint arXiv:2009.07896 ,
  year= 2020 
 
Citation : 798
Abstract: 
In this paper we introduce a novel, unified, open-source model interpretability library for PyTorch [12]  reference:. The library contains generic implementations of a number of gradient and perturbation-based attribution algorithms, also known as feature, neuron and layer importance algorithms, as well as a set of evaluation metrics for these algorithms. It can be used for both classification and non-classification models including graph-structured models built on Neural Networks (NN). In this paper we give a high-level overview of supported attribution algorithms and show how to perform memory-efficient and scalable computations. We emphasize that the three main characteristics of the library are multimodality, extensibility and ease of use. Multimodality supports different modality of inputs such as image, text, audio or video. Extensibility allows adding new algorithms and features. The library is also designed for easy understanding and use. Besides, we also introduce an interactive visualization tool called Captum Insights that is built on top of Captum library and allows sample-based model debugging and visualization using feature importance metrics.
Keywords: Interpretability, Attribution, Multi-Modal, Model Understanding
Google Scholar Link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+N.+Kokhlikyan%2C+V.+Miglani%2C+M.+Martin%2C+E.+Wang%2C+B.+Alsallakh%2C+J.+Reynolds%2C+A.+Melnikov%2C+N.+Kliushkina%2C+C.+Araya%2C+S.+Yan+et+al.%2C+%E2%80%9CCaptum%3A+A+unified+and+generic+model+interpretability+library+for+pytorch%2C%E2%80%9D+arXiv+preprint+arXiv%3A2009.07896%2C+2020.&btnG=
External Link: 
https://arxiv.org/abs/2009.07896
DOI Link:
https://doi.org/10.48550/arXiv.2009.07896
	

id:77  reference: A. Hedstr ¨om, L. Weber, D. Krakowczyk, D. Bareeva, F. Motzkus,
W. Samek, S. Lapuschkin, and M. M.-C. H ¨ohne, “Quantus: An
explainable ai toolkit for responsible evaluation of neural network
explanations and beyond,” Journal of Machine Learning Research,
vol. 24, no. 34, pp. 1–11, 2023.
Bibtex:
@article hedstrom2023quantus,
  title= Quantus: An explainable ai toolkit for responsible evaluation of neural network explanations and beyond ,
  author= Hedstr \"o m, Anna and Weber, Leander and Krakowczyk, Daniel and Bareeva, Dilyara and Motzkus, Franz and Samek, Wojciech and Lapuschkin, Sebastian and H \"o hne, Marina M-C ,
  journal= Journal of Machine Learning Research ,
  volume= 24 ,
  number= 34 ,
  pages= 1--11 ,
  year= 2023 
 
Abstract:
The evaluation of explanation methods is a research topic that has not yet been explored deeply, however, since explainability is supposed to strengthen trust in artificial intelligence, it is necessary to systematically review and compare explanation methods in order to confirm their correctness. Until now, no tool with focus on XAI evaluation exists that exhaustively and speedily allows researchers to evaluate the performance of explanations of neural network predictions. To increase transparency and reproducibility in the field, we therefore built Quantus--a comprehensive, evaluation toolkit in Python that includes a growing, well-organised collection of evaluation metrics and tutorials for evaluating explainable methods. The toolkit has been thoroughly tested and is available under an open-source license on PyPi (or on https://github.com/understandable-machine-intelligence-lab/Quantus/).
Google Scholar Link:
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Hedstr+%C2%A8om%2C+L.+Weber%2C+D.+Krakowczyk%2C+D.+Bareeva%2C+F.+Motzkus%2C+W.+Samek%2C+S.+Lapuschkin%2C+and+M.+M.-C.+H+%C2%A8ohne%2C+%E2%80%9CQuantus%3A+An+explainable+ai+toolkit+for+responsible+evaluation+of+neural+network+explanations+and+beyond%2C%E2%80%9D+Journal+of+Machine+Learning+Research%2C+vol.+24%2C+no.+34%2C+pp.+1%E2%80%9311%2C+2023.&btnG=
External Link:
https://www.jmlr.org/papers/v24/22-0142.html

id:78  reference: C.-K. Yeh, C.-Y. Hsieh, A. Suggala, D. I. Inouye, and P. K. Raviku-
mar, “On the (in) fidelity and sensitivity of explanations,” Advances
in neural information processing systems, vol. 32, 2019.
Bibtex:
@article yeh2019fidelity,
  title= On the (in) fidelity and sensitivity of explanations ,
  author= Yeh, Chih-Kuan and Hsieh, Cheng-Yu and Suggala, Arun and Inouye, David I and Ravikumar, Pradeep K ,
  journal= Advances in neural information processing systems ,
  volume= 32 ,
  year= 2019 
 
Abstract:
We consider objective evaluation measures of saliency explanations for complex black-box machine learning models. We propose simple robust variants of two notions that have been considered in recent literature:(in) fidelity, and sensitivity. We analyze optimal explanations with respect to both these measures, and while the optimal explanation for sensitivity is a vacuous constant explanation, the optimal explanation for infidelity is a novel combination of two popular explanation methods. By varying the perturbation distribution that defines infidelity, we obtain novel explanations by optimizing infidelity, which we show to out-perform existing explanations in both quantitative and qualitative measurements. Another salient question given these measures is how to modify any given explanation to have better values with respect to these measures. We propose a simple modification based on lowering sensitivity, and moreover show that when done appropriately, we could simultaneously improve both sensitivity as well as fidelity.
Google Scholar Link:
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=C.-K.+Yeh%2C+C.-Y.+Hsieh%2C+A.+Suggala%2C+D.+I.+Inouye%2C+and+P.+K.+Raviku-+mar%2C+%E2%80%9COn+the+%28in%29+fidelity+and+sensitivity+of+explanations%2C%E2%80%9D+Advances+in+neural+information+processing+systems%2C+vol.+32%2C+2019.&btnG=
External link:
https://proceedings.neurips.cc/paper/2019/hash/a7471fdc77b3435276507cc8f2dc2569-Abstract.html

id:79  reference: P. Chalasani, J. Chen, A. R. Chowdhury, X. Wu, and S. Jha, “Con-
cise explanations of neural networks using adversarial training,”
in International Conference on Machine Learning. PMLR, 2020, pp.
1383–1391.
Bibtex:
@inproceedings chalasani2020concise,
  title= Concise explanations of neural networks using adversarial training ,
  author= Chalasani, Prasad and Chen, Jiefeng and Chowdhury, Amrita Roy and Wu, Xi and Jha, Somesh ,
  booktitle= International Conference on Machine Learning ,
  pages= 1383--1391 ,
  year= 2020 ,
  organization= PMLR 
 
Abstract:
We show new connections between adversarial learning and explainability for deep neural networks (DNNs). One form of explanation of the output of a neural network model in terms of its input features, is a vector of feature-attributions, which can be generated by various techniques such as Integrated Gradients (IG), DeepSHAP, LIME, and CXPlain. Two desirable characteristics of an attribution-based explanation are:(1)\emph  sparseness : the attributions of irrelevant or weakly relevant features should be negligible, thus resulting in\emph  concise  explanations in terms of the significant features, and (2)\emph  stability : it should not vary significantly within a small local neighborhood of the input. Our first contribution is a theoretical exploration of how these two properties (when using IG-based attributions) are related to adversarial training, for a class of 1-layer networks (which includes logistic regression models for binary and multi-class classification); for these networks we show that (a) adversarial training using an -bounded adversary produces models with sparse attribution vectors, and (b) natural model-training while encouraging stable explanations (via an extra term in the loss function), is equivalent to adversarial training. Our second contribution is an empirical verification of phenomenon (a), which we show, somewhat surprisingly, occurs\emph  not only in 1-layer networks, but also DNNs trained on standard image datasets , and extends beyond IG-based attributions, to those based on DeepSHAP: adversarial training with $\linf $-bounded perturbations yields significantly sparser attribution vectors, with little degradation in performance on natural test data, compared to natural training. Moreover, the sparseness of the attribution vectors is significantly better than that achievable via -regularized natural training.
Google scholar Link:
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=P.+Chalasani%2C+J.+Chen%2C+A.+R.+Chowdhury%2C+X.+Wu%2C+and+S.+Jha%2C+%E2%80%9CCon-+cise+explanations+of+neural+networks+using+adversarial+training%2C%E2%80%9D+in+International+Conference+on+Machine+Learning.+PMLR%2C+2020%2C+pp.+1383%E2%80%931391.&btnG=
External Link:
https://proceedings.mlr.press/v119/chalasani20a.html

id:80  reference: J. Zhang, S. A. Bargal, Z. Lin, J. Brandt, X. Shen, and S. Sclaroff,
“Top-down neural attention by excitation backprop,” International
Journal of Computer Vision, vol. 126, no. 10, pp. 1084–1102, 2018.
Bibtex:
@article zhang2018top,
  title= Top-down neural attention by excitation backprop ,
  author= Zhang, Jianming and Bargal, Sarah Adel and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan ,
  journal= International Journal of Computer Vision ,
  volume= 126 ,
  number= 10 ,
  pages= 1084--1102 ,
  year= 2018 ,
  publisher= Springer 
 
Abstract:
We aim to model the top-down attention of a convolutional neural network (CNN) classifier for generating task-specific attention maps. Inspired by a top-down human visual attention model, we propose a new backpropagation scheme, called Excitation Backprop, to pass along top-down signals downwards in the network hierarchy via a probabilistic Winner-Take-All process. Furthermore, we introduce the concept of contrastive attention to make the top-down attention maps more discriminative. We show a theoretic connection between the proposed contrastive attention formulation and the Class Activation Map computation. Efficient implementation of Excitation Backprop for common neural network layers is also presented. In experiments, we visualize the evidence of a model’s classification decision by computing the proposed top-down attention maps. For quantitative evaluation, we report the accuracy of our method in weakly supervised localization tasks on the MS COCO, PASCAL VOC07 and ImageNet datasets. The usefulness of our method is further validated in the text-to-region association task. On the Flickr30k Entities dataset, we achieve promising performance in phrase localization by leveraging the top-down attention of a CNN model that has been trained on weakly labeled web images. Finally, we demonstrate applications of our method in model interpretation and data annotation assistance for facial expression analysis and medical imaging tasks.
Google Scholar link:
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=J.+Zhang%2C+S.+A.+Bargal%2C+Z.+Lin%2C+J.+Brandt%2C+X.+Shen%2C+and+S.+Sclaroff%2C+%E2%80%9CTop-down+neural+attention+by+excitation+backprop%2C%E2%80%9D+International+Journal+of+Computer+Vision%2C+vol.+126%2C+no.+10%2C+pp.+1084%E2%80%931102%2C+2018.&btnG=
External Link:
https://link.springer.com/article/10.1007/s11263-017-1059-x

id:81  reference: L. Sixt, M. Granz, and T. Landgraf, “When explanations lie: Why
many modified bp attributions fail,” in International conference on
machine learning. PMLR, 2020, pp. 9046–9057.
Bibtex:
@inproceedings sixt2020explanations,
  title= When explanations lie: Why many modified bp attributions fail ,
  author= Sixt, Leon and Granz, Maximilian and Landgraf, Tim ,
  booktitle= International conference on machine learning ,
  pages= 9046--9057 ,
  year= 2020 ,
  organization= PMLR 
 
Abstract:
Attribution methods aim to explain a neural network’s prediction by highlighting the most relevant image areas. A popular approach is to backpropagate (BP) a custom relevance score using modified rules, rather than the gradient. We analyze an extensive set of modified BP methods: Deep Taylor Decomposition, Layer-wise Relevance Propagation (LRP), Excitation BP, PatternAttribution, DeepLIFT, Deconv, RectGrad, and Guided BP. We find empirically that the explanations of all mentioned methods, except for DeepLIFT, are independent of the parameters of later layers. We provide theoretical insights for this surprising behavior and also analyze why DeepLIFT does not suffer from this limitation. Empirically, we measure how information of later layers is ignored by using our new metric, cosine similarity convergence (CSC). The paper provides a framework to assess the faithfulness of new and existing modified BP methods theoretically and empirically.
Google scholar link:
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+L.+Sixt%2C+M.+Granz%2C+and+T.+Landgraf%2C+%E2%80%9CWhen+explanations+lie%3A+Why+many+modified+bp+attributions+fail%2C%E2%80%9D+in+International+conference+on+machine+learning.+PMLR%2C+2020%2C+pp.+9046%E2%80%939057.&btnG=
External link:
https://proceedings.mlr.press/v119/sixt20a.html

id:82  reference: V. Dhore, A. Bhat, V. Nerlekar, K. Chavhan, and A. Umare, “En-
hancing explainable ai: A hybrid approach combining gradcam
and lrp for cnn interpretability,” arXiv preprint arXiv:2405.12175,
2024.
Bibtex:
@article dhore2024enhancing,
  title= Enhancing Explainable AI: A Hybrid Approach Combining GradCAM and LRP for CNN Interpretability ,
  author= Dhore, Vaibhav and Bhat, Achintya and Nerlekar, Viraj and Chavhan, Kashyap and Umare, Aniket ,
  journal= arXiv preprint arXiv:2405.12175 ,
  year= 2024 
 
Abstract:
We present a new technique that explains the output of a CNN-based model using a combination of GradCAM and LRP methods. Both of these methods produce visual explanations by highlighting input regions that are important for predictions. In the new method, the explanation produced by GradCAM is first processed to remove noises. The processed output is then multiplied elementwise with the output of LRP. Finally, a Gaussian blur is applied on the product. We compared the proposed method with GradCAM and LRP on the metrics of Faithfulness, Robustness, Complexity, Localisation and Randomisation. It was observed that this method performs better on Complexity than both GradCAM and LRP and is better than atleast one of them in the other metrics.
Google Scholar link:
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=V.+Dhore%2C+A.+Bhat%2C+V.+Nerlekar%2C+K.+Chavhan%2C+and+A.+Umare%2C+%E2%80%9CEn-+hancing+explainable+ai%3A+A+hybrid+approach+combining+gradcam+and+lrp+for+cnn+interpretability%2C%E2%80%9D+arXiv+preprint+arXiv%3A2405.12175%2C+2024.&btnG=
External link:
https://arxiv.org/abs/2405.12175
DOI link:
https://doi.org/10.48550/arXiv.2405.12175
	

id:83  reference: J. Wang, S. Liu, and W. Zhang, “Visual analytics for machine learn-
ing: A data perspective survey,” IEEE transactions on visualization
and computer graphics, 2024.
Bibtex:
@article wang2024visual,
  title= Visual analytics for machine learning: A data perspective survey ,
  author= Wang, Junpeng and Liu, Shixia and Zhang, Wei ,
  journal= IEEE Transactions on Visualization and Computer Graphics ,
  year= 2024 ,
  publisher= IEEE 
 
Abstract:
The past decade has witnessed a plethora of works that leverage the power of visualization (VIS) to interpret machine learning (ML) models. The corresponding research topic, VIS4ML, keeps growing at a fast pace. To better organize the enormous works and shed light on the developing trend of VIS4ML, we provide a systematic review of these works through this survey. Since data quality greatly impacts the performance of ML models, our survey focuses specifically on summarizing VIS4ML works from the data perspective . First, we categorize the common data handled by ML models into five types, explain the unique features of each type, and highlight the corresponding ML models that are good at learning from them. Second, from the large number of VIS4ML works, we tease out six tasks that operate on these types of data (i.e., data-centric tasks) at different stages of the ML pipeline to understand, diagnose, and refine ML models. Lastly, by studying the distribution of 143 surveyed papers across the five data types, six data-centric tasks, and their intersections, we analyze the prospective research directions and envision future research trends.
Google Scholar Link:
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=J.+Wang%2C+S.+Liu%2C+and+W.+Zhang%2C+%E2%80%9CVisual+analytics+for+machine+learn-+ing%3A+A+data+perspective+survey%2C%E2%80%9D+IEEE+transactions+on+visualization+and+computer+graphics%2C+2024.&btnG=
External Link:
https://ieeexplore.ieee.org/document/10412199

id:84  reference: Y. Li, J. Wang, T. Fujiwara, and K.-L. Ma, “Visual analytics of
neuron vulnerability to adversarial attacks on convolutional neu-
ral networks,” ACM Transactions on Interactive Intelligent Systems,
vol. 13, no. 4, pp. 1–26, 2023.
Bibtex:
@article li2023visual,
  title= Visual analytics of neuron vulnerability to adversarial attacks on convolutional neural networks ,
  author= Li, Yiran and Wang, Junpeng and Fujiwara, Takanori and Ma, Kwan-Liu ,
  journal= ACM Transactions on Interactive Intelligent Systems ,
  volume= 13 ,
  number= 4 ,
  pages= 1--26 ,
  year= 2023 ,
  publisher= ACM New York, NY 
 
Citation : 7
Abstract:
Adversarial attacks on a convolutional neural network (CNN)—injecting human-imperceptible perturbations into an input image—could fool a high-performance CNN into making incorrect predictions. The success of adversarial attacks raises serious concerns about the robustness of CNNs, and prevents them from being used in safety-critical applications, such as medical diagnosis and autonomous driving. Our work introduces a visual analytics approach to understanding adversarial attacks by answering two questions: (1) Which neurons are more vulnerable to attacks? and (2) Which image features do these vulnerable neurons capture during the prediction? For the first question, we introduce multiple perturbation-based measures to break down the attacking magnitude into individual CNN neurons and rank the neurons by their vulnerability levels. For the second, we identify image features (e.g., cat ears) that highly stimulate a user-selected neuron to augment and validate the neuron’s responsibility. Furthermore, we support an interactive exploration of a large number of neurons by aiding with hierarchical clustering based on the neurons’ roles in the prediction. To this end, a visual analytics system is designed to incorporate visual reasoning for interpreting adversarial attacks. We validate the effectiveness of our system through multiple case studies as well as feedback from domain experts.
Keywords: Convolutional neural networks, adversarial attack, explainable machine learning
Google scholar link:
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=Y.+Li%2C+J.+Wang%2C+T.+Fujiwara%2C+and+K.-L.+Ma%2C+%E2%80%9CVisual+analytics+of+neuron+vulnerability+to+adversarial+attacks+on+convolutional+neu-+ral+networks%2C%E2%80%9D+ACM+Transactions+on+Interactive+Intelligent+Systems%2C+vol.+13%2C+no.+4%2C+pp.+1%E2%80%9326%2C+2023.&btnG=
External link:
https://dl.acm.org/doi/full/10.1145/3587470
https://doi.org/10.1145/3587470

id:85  reference: D. Collaris and J. J. van Wijk, “Strategyatlas: Strategy analysis for
machine learning interpretability,” IEEE Transactions on Visualiza-
tion and Computer Graphics, vol. 29, no. 6, pp. 2996–3008, 2022.
Bibtex: 
@article collaris2022strategyatlas,
  title= Strategyatlas: Strategy analysis for machine learning interpretability ,
  author= Collaris, Dennis and van Wijk, Jarke J ,
  journal= IEEE Transactions on Visualization and Computer Graphics ,
  volume= 29 ,
  number= 6 ,
  pages= 2996--3008 ,
  year= 2022 ,
  publisher= IEEE 
 
Citation : 15
Abstract:
Businesses in high-risk environments have been reluctant to adopt modern machine learning approaches due to their complex and uninterpretable nature. Most current solutions provide local, instance-level explanations, but this is insufficient for understanding the model as a whole. In this work, we show that strategy clusters (i.e., groups of data instances that are treated distinctly by the model) can be used to understand the global behavior of a complex ML model. To support effective exploration and understanding of these clusters, we introduce StrategyAtlas , a system designed to analyze and explain model strategies. Furthermore, it supports multiple ways to utilize these strategies for simplifying and improving the reference model. In collaboration with a large insurance company, we present a use case in automatic insurance acceptance, and show how professional data scientists were enabled to understand a complex model and improve the production model based on these insights.
IEEE Keywords
* Data models,
* Analytical models,
* Machine learning,
* Predictive models,
* Computational modeling,
* Insurance,
* Data visualization
Google Scholar link:
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=D.+Collaris+and+J.+J.+van+Wijk%2C+%E2%80%9CStrategyatlas%3A+Strategy+analysis+for+machine+learning+interpretability%2C%E2%80%9D+IEEE+Transactions+on+Visualiza-+tion+and+Computer+Graphics%2C+vol.+29%2C+no.+6%2C+pp.+2996%E2%80%933008%2C+2022.&btnG=
External link:
https://ieeexplore.ieee.org/document/9695246

id:86  reference: F. Hohman, H. Park, C. Robinson, and D. H. P. Chau, “S ummit:
Scaling deep learning interpretability by visualizing activation and
attribution summarizations,” IEEE transactions on visualization and
computer graphics, vol. 26, no. 1, pp. 1096–1106, 2019.
Bibtex: 
@article hohman2019s,
  title= S ummit: Scaling deep learning interpretability by visualizing activation and attribution summarizations ,
  author= Hohman, Fred and Park, Haekyu and Robinson, Caleb and Chau, Duen Horng Polo ,
  journal= IEEE transactions on visualization and computer graphics ,
  volume= 26 ,
  number= 1 ,
  pages= 1096--1106 ,
  year= 2019 ,
  publisher= IEEE 
 
Citation : 256
Abstract:
Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier's learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.
IEEE Keywords
* Neurons,
* Biological neural networks,
* Feature extraction,
* Data visualization,
* Computational modeling,
* Predictive models,
* Visualization
Google scholar link:
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=F.+Hohman%2C+H.+Park%2C+C.+Robinson%2C+and+D.+H.+P.+Chau%2C+%E2%80%9CS+ummit%3A+Scaling+deep+learning+interpretability+by+visualizing+activation+and+attribution+summarizations%2C%E2%80%9D+IEEE+transactions+on+visualization+and+computer+graphics%2C+vol.+26%2C+no.+1%2C+pp.+1096%E2%80%931106%2C+2019.&btnG=
External Link:
https://ieeexplore.ieee.org/document/8807294

id:87  reference: Y. Ouyang, Y. Wu, H. Wang, C. Zhang, F. Cheng, C. Jiang, L. Jin,
Y. Cao, and Q. Li, “Leveraging historical medical records as
a proxy via multimodal modeling and visualization to enrich
medical diagnostic learning,” IEEE Transactions on Visualization and
Computer Graphics, 2023.
Bibtex: 
@article ouyang2023leveraging,
  title= Leveraging historical medical records as a proxy via multimodal modeling and visualization to enrich medical diagnostic learning ,
  author= Ouyang, Yang and Wu, Yuchen and Wang, He and Zhang, Chenyang and Cheng, Furui and Jiang, Chang and Jin, Lixia and Cao, Yuanwu and Li, Quan ,
  journal= IEEE Transactions on Visualization and Computer Graphics ,
  year= 2023 ,
  publisher= IEEE 
 
Citation : 8
Abstract: 
Simulation-based Medical Education (SBME) has been developed as a cost-effective means of enhancing the diagnostic skills of novice physicians and interns, thereby mitigating the need for resource-intensive mentor-apprentice training. However, feedback provided in most SBME is often directed towards improving the operational proficiency of learners, rather than providing summative medical diagnoses that result from experience and time. Additionally, the multimodal nature of medical data during diagnosis poses significant challenges for interns and novice physicians, including the tendency to overlook or over-rely on data from certain modalities, and difficulties in comprehending potential associations between modalities. To address these challenges, we present DiagnosisAssistant , a visual analytics system that leverages historical medical records as a proxy for multimodal modeling and visualization to enhance the learning experience of interns and novice physicians. The system employs elaborately designed visualizations to explore different modality data, offer diagnostic interpretive hints based on the constructed model, and enable comparative analyses of specific patients. Our approach is validated through two case studies and expert interviews, demonstrating its effectiveness in enhancing medical training.
IEEE Keywords
* Medical diagnostic imaging,
* Solid modeling,
* Computational modeling,
* Medical services,
* Data visualization,
* Data models,
* Training
Google scholar link: 
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=Y.+Ouyang%2C+Y.+Wu%2C+H.+Wang%2C+C.+Zhang%2C+F.+Cheng%2C+C.+Jiang%2C+L.+Jin%2C+Y.+Cao%2C+and+Q.+Li%2C+%E2%80%9CLeveraging+historical+medical+records+as+a+proxy+via+multimodal+modeling+and+visualization+to+enrich+medical+diagnostic+learning%2C%E2%80%9D+IEEE+Transactions+on+Visualization+and+Computer+Graphics%2C+2023.&btnG=
External LINK: https://ieeexplore.ieee.org/document/10295394

id:88  reference: R. Fong, M. Patrick, and A. Vedaldi, “Understanding deep net-
works via extremal perturbations and smooth masks,” in Proceed-
ings of the IEEE/CVF international conference on computer vision, 2019,
pp. 2950–2958.
Bibtex: 
@inproceedings fong2019understanding,
  title= Understanding deep networks via extremal perturbations and smooth masks ,
  author= Fong, Ruth and Patrick, Mandela and Vedaldi, Andrea ,
  booktitle= Proceedings of the IEEE/CVF international conference on computer vision ,
  pages= 2950--2958 ,
  year= 2019 
 
Citation : 469
Abstract:  Attribution is the problem of finding which parts of an image are the most responsible for the output of a deep neural network. An important family of attribution methods is based on measuring the effect of perturbations applied to the input image, either via exhaustive search or by finding representative perturbations via optimization. In this paper, we discuss some of the shortcomings of existing approaches to perturbation analysis and address them by introducing the concept of extremal perturbations, which are theoretically grounded and interpretable. We also introduce a number of technical innovations to compute these extremal perturbations, including a new area constraint and a parametric family of smooth perturbations, which allow us to remove all tunable weighing factors from the optimization problem. We analyze the effect of perturbations as a function of their area, demonstrating excellent sensitivity to the spatial properties of the network under stimulation. We also extend perturbation analysis to the intermediate layers of a deep neural network. This application allows us to show how compactly an image can be represented (in terms of the number of channels it requires). We also demonstrate that the consistency with which images of a given class rely on the same intermediate channel correlates well with class accuracy.
APA:  Bianchi, M., De Santis, A., Tocchetti, A., & Brambilla, M. (2024). Interpretable Network Visualizations: A Human-in-the-Loop Approach for Post-hoc Explainability of CNN-based Image Classification. arXiv preprint arXiv:2405.03301.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=R.+Fong%2C+M.+Patrick%2C+and+A.+Vedaldi%2C+%E2%80%9CUnderstanding+deep+net-+works+via+extremal+perturbations+and+smooth+masks%2C%E2%80%9D+in+Proceed-+ings+of+the+IEEE%2FCVF+international+conference+on+computer+vision%2C+2019%2C+pp.+2950%E2%80%932958.&btnG=
External link:https://openaccess.thecvf.com/content_ICCV_2019/html/Fong_Understanding_Deep_Networks_via_Extremal_Perturbations_and_Smooth_Masks_ICCV_2019_paper.html
DOI:
	

id:89  reference: M. Bianchi, A. De Santis, A. Tocchetti, and M. Brambilla, “Inter-
pretable network visualizations: A human-in-the-loop approach
for post-hoc explainability of cnn-based image classification,”
arXiv preprint arXiv:2405.03301, 2024.
Bibtex: 
@article bianchi2024interpretable,
  title= Interpretable Network Visualizations: A Human-in-the-Loop Approach for Post-hoc Explainability of CNN-based Image Classification ,
  author= Bianchi, Matteo and De Santis, Antonio and Tocchetti, Andrea and Brambilla, Marco ,
  journal= arXiv preprint arXiv:2405.03301 ,
  year= 2024 
 
Citation : 0
Abstract:  Transparency and explainability in image classification are essential for establishing trust in machine learning models and detecting biases and errors. State-of-the-art explainability methods generate saliency maps to show where a specific class is identified, without providing a detailed explanation of the model's decision process. Striving to address such a need, we introduce a post-hoc method that explains the entire feature extraction process of a Convolutional Neural Network. These explanations include a layer-wise representation of the features the model extracts from the input. Such features are represented as saliency maps generated by clustering and merging similar feature maps, to which we associate a weight derived by generalizing Grad-CAM for the proposed methodology. To further enhance these explanations, we include a set of textual labels collected through a gamified crowdsourcing activity and processed using NLP techniques and Sentence-BERT. Finally, we show an approach to generate global explanations by aggregating labels across multiple images.
APA:  Bianchi, M., De Santis, A., Tocchetti, A., & Brambilla, M. (2024). Interpretable Network Visualizations: A Human-in-the-Loop Approach for Post-hoc Explainability of CNN-based Image Classification. arXiv preprint arXiv:2405.03301.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=M.+Bianchi%2C+A.+De+Santis%2C+A.+Tocchetti%2C+and+M.+Brambilla%2C+%E2%80%9CInter-+pretable+network+visualizations%3A+A+human-in-the-loop+approach+for+post-hoc+explainability+of+cnn-based+image+classification%2C%E2%80%9D+arXiv+preprint+arXiv%3A2405.03301%2C+2024.&btnG=
External link:https://arxiv.org/abs/2405.03301
DOI:
https://doi.org/10.48550/arXiv.2405.03301
	

id:90  reference: C. J. Anders, D. Neumann, W. Samek, K.-R. M ¨uller, and S. La-
puschkin, “Software for dataset-wide xai: from local explanations
to global insights with zennit, corelay, and virelay,” arXiv preprint
arXiv:2106.13200, 2021.
Bibtex: 
@article anders2021software,
  title= Software for dataset-wide XAI: from local explanations to global insights with Zennit, CoRelAy, and ViRelAy ,
  author= Anders, Christopher J and Neumann, David and Samek, Wojciech and M \"u ller, Klaus-Robert and Lapuschkin, Sebastian ,
  journal= arXiv preprint arXiv:2106.13200 ,
  year= 2021 
 
Citation : 71
Abstract:  Deep Neural Networks (DNNs) are known to be strong predictors, but their prediction strategies can rarely be understood. With recent advances in Explainable Artificial Intelligence (XAI), approaches are available to explore the reasoning behind those complex models' predictions. Among post-hoc attribution methods, Layer-wise Relevance Propagation (LRP) shows high performance. For deeper quantitative analysis, manual approaches exist, but without the right tools they are unnecessarily labor intensive. In this software paper, we introduce three software packages targeted at scientists to explore model reasoning using attribution approaches and beyond: (1) Zennit - a highly customizable and intuitive attribution framework implementing LRP and related approaches in PyTorch, (2) CoRelAy - a framework to easily and quickly construct quantitative analysis pipelines for dataset-wide analyses of explanations, and (3) ViRelAy - a web-application to interactively explore data, attributions, and analysis results. With this, we provide a standardized implementation solution for XAI, to contribute towards more reproducibility in our field.
APA:Anders, C. J., Neumann, D., Samek, W., Müller, K. R., & Lapuschkin, S. (2021). Software for dataset-wide XAI: from local explanations to global insights with Zennit, CoRelAy, and ViRelAy. arXiv preprint arXiv:2106.13200.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+C.+J.+Anders%2C+D.+Neumann%2C+W.+Samek%2C+K.-R.+M+%C2%A8uller%2C+and+S.+La-+puschkin%2C+%E2%80%9CSoftware+for+dataset-wide+xai%3A+from+local+explanations+to+global+insights+with+zennit%2C+corelay%2C+and+virelay%2C%E2%80%9D+arXiv+preprint+arXiv%3A2106.13200%2C+2021.&btnG=
External link:https://arxiv.org/abs/2106.13200
DOI:
https://doi.org/10.48550/arXiv.2106.13200
	

id:91  reference: H. Park, N. Das, R. Duggal, A. P. Wright, O. Shaikh, F. Hohman,
and D. H. P. Chau, “Neurocartography: Scalable automatic visual
summarization of concepts in deep neural networks,” IEEE Trans-
actions on Visualization and Computer Graphics, vol. 28, no. 1, pp.
813–823, 2021.
Bibtex: 
@article park2021neurocartography,
  title= Neurocartography: Scalable automatic visual summarization of concepts in deep neural networks ,
  author= Park, Haekyu and Das, Nilaksh and Duggal, Rahul and Wright, Austin P and Shaikh, Omar and Hohman, Fred and Chau, Duen Horng Polo ,
  journal= IEEE Transactions on Visualization and Computer Graphics ,
  volume= 28 ,
  number= 1 ,
  pages= 813--823 ,
  year= 2021 ,
  publisher= IEEE 
 
Citation : 21
Abstract:  Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present Neurocartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. Neurocartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting “dog faces” of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting “dog face” and “dog tail” are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs' relationships, in time linear to the number of neurons instead of quadratic time. Neurocartography scales to large data, such as the ImageNet dataset with 1.2M images. The system's tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The Neurocartography visualization runs in modern browsers and is open-sourced.
Keywords: Deep learning interpretability, visual analytics, scalable summarization, neuron clustering, neuron embedding
APA:Park, H., Das, N., Duggal, R., Wright, A. P., Shaikh, O., Hohman, F., & Chau, D. H. P. (2021). Neurocartography: Scalable automatic visual summarization of concepts in deep neural networks. IEEE Transactions on Visualization and Computer Graphics, 28(1), 813-823.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=H.+Park%2C+N.+Das%2C+R.+Duggal%2C+A.+P.+Wright%2C+O.+Shaikh%2C+F.+Hohman%2C+and+D.+H.+P.+Chau%2C+%E2%80%9CNeurocartography%3A+Scalable+automatic+visual+summarization+of+concepts+in+deep+neural+networks%2C%E2%80%9D+IEEE+Trans-+actions+on+Visualization+and+Computer+Graphics%2C+vol.+28%2C+no.+1%2C+pp.+813%E2%80%93823%2C+2021.&btnG=
External link:https://ieeexplore.ieee.org/document/9552879
DOI:https://ieeexplore.ieee.org/document/9552879

id:92  reference: Y. Li, J. Wang, P. Aboagye, C.-C. M. Yeh, Y. Zheng, L. Wang,
W. Zhang, and K.-L. Ma, “Visual analytics for efficient image
exploration and user-guided image captioning,” IEEE Transactions
on Visualization and Computer Graphics, 2024.
Bibtex: 
@article li2024visual,
  title= Visual Analytics for Efficient Image Exploration and User-Guided Image Captioning ,
  author= Li, Yiran and Wang, Junpeng and Aboagye, Prince and Yeh, Chin-Chia Michael and Zheng, Yan and Wang, Liang and Zhang, Wei and Ma, Kwan-Liu ,
  journal= IEEE Transactions on Visualization and Computer Graphics ,
  year= 2024 ,
  publisher= IEEE 
 
Citation : 0
Abstract:  Recent advancements in pre-trained language-image models have ushered in a new era of visual comprehension. Leveraging the power of these models, this paper tackles two issues within the realm of visual analytics: (1) the efficient exploration of large-scale image datasets and identification of data biases within them; (2) the evaluation of image captions and steering of their generation process. On the one hand, by visually examining the captions generated from language-image models for an image dataset, we gain deeper insights into the visual contents, unearthing data biases that may be entrenched within the dataset. On the other hand, by depicting the association between visual features and textual captions, we expose the weaknesses of pre-trained language-image models in their captioning capability and propose an interactive interface to steer caption generation. The two parts have been coalesced into a coordinated visual analytics system, fostering the mutual enrichment of visual and textual contents. We validate the effectiveness of the system with domain practitioners through concrete case studies with large-scale image datasets
IEEE Keywords
* Visual analytics,
* Analytical models,
* Training,
* Image segmentation,
* Transformers,
* Snow,
* Heating systems
APA: Li, Y., Wang, J., Aboagye, P., Yeh, C. C. M., Zheng, Y., Wang, L., ... & Ma, K. L. (2024). Visual Analytics for Efficient Image Exploration and User-Guided Image Captioning. IEEE Transactions on Visualization and Computer Graphics.
Google scholar link: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=Y.+Li%2C+J.+Wang%2C+P.+Aboagye%2C+C.-C.+M.+Yeh%2C+Y.+Zheng%2C+L.+Wang%2C+W.+Zhang%2C+and+K.-L.+Ma%2C+%E2%80%9CVisual+analytics+for+efficient+image+exploration+and+user-guided+image+captioning%2C%E2%80%9D+IEEE+Transactions+on+Visualization+and+Computer+Graphics%2C+2024.&btnG=
External link:https://ieeexplore.ieee.org/document/10502235
DOI: https://ieeexplore.ieee.org/document/10502235`