export const paperData =[
    {
        "id": 1,
        "reference": "P. Linardatos, V. Papastefanopoulos, and S. Kotsiantis, 'Explainable AI: A review of machine learning interpretability methods,' Entropy, vol. 23, no. 1, p. 18, 2020.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "linardatos2020explainable",
            "title": "Explainable AI: A review of machine learning interpretability methods",
            "author": "Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris",
            "journal": "Entropy",
            "volume": "23",
            "number": "1",
            "pages": "18",
            "year": 2020,
            "publisher": "MDPI"
        },
        "citation_count": 2275,
        "abstract": "Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance has often been achieved through increased model complexity, turning such systems into 'black box' approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.",
        "keywords": [
            "XAI",
            "machine learning",
            "explainability",
            "interpretability",
            "fairness",
            "sensitivity",
            "black-box"
        ],
        "APA": "Linardatos, P., Papastefanopoulos, V., & Kotsiantis, S. (2020). Explainable AI: A review of machine learning interpretability methods. Entropy, 23(1), 18.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=Explainable+AI%3A+A+Review+of+Machine+Learning+Interpretability+Methods&btnG=",
        "externalLink": "https://www.mdpi.com/1099-4300/23/1/18",
        "DOI": "https://doi.org/10.3390/e23010018",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article linardatos2020explainable,\n  title= Explainable ai: A review of machine learning interpretability methods ,\n  author= Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris ,\n  journal= Entropy ,\n  volume= 23 ,\n  number= 1 ,\n  pages= 18 ,\n  year= 2020 ,\n  publisher= MDPI \n \n"
    },
    {
        "id": 2,
        "reference": "A. B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik, A. Barbado, S. García, S. Gil-López, D. Molina, R. Benjamins et al., 'Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI,' Information fusion, vol. 58, pp. 82–115, 2020.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "arrieta2020explainable",
            "title": "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",
            "author": "Arrieta, Alejandro Barredo and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and García, Salvador and Gil-López, Sergio and Molina, Daniel and Benjamins, Richard and others",
            "journal": "Information fusion",
            "volume": "58",
            "pages": "82--115",
            "year": 2020,
            "publisher": "Elsevier"
        },
        "citation_count": 7987,
        "abstract": "In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule-based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose, we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail.",
        "keywords": [
            "Explainable Artificial Intelligence",
            "Machine Learning",
            "Deep Learning",
            "Data Fusion",
            "Interpretability",
            "Comprehensibility",
            "Transparency",
            "Privacy",
            "Fairness",
            "Accountability",
            "Responsible Artificial Intelligence"
        ],
        "APA": "Arrieta, A. B., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., ... & Herrera, F. (2020). Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information fusion, 58, 82-115.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+A.+B.+Arrieta%2C+N.+D%C2%B4%C4%B1az-Rodr%C2%B4%C4%B1guez%2C+J.+Del+Ser%2C+A.+Bennetot%2C+S.+Tabik%2C+A.+Barbado%2C+S.+Garc%C2%B4%C4%B1a%2C+S.+Gil-L%C3%B3pez%2C+D.+Molina%2C+R.+Benjamins+et+al.%2C+%E2%80%9CExplainable+artificial+intelligence+%28XAI%29%3A+Concepts%2C+taxonomies%2C+opportunities+and+challenges+toward+responsible+AI%2C%E2%80%9D+Information+fusion%2C+vol.+58%2C+pp.+82%E2%80%93115%2C+2020.&btnG=",
        "externalLink": "https://www.sciencedirect.com/science/article/pii/S1566253519308103?casa_token=B5OVfyaKbpMAAAAA:G7e8Ul3cfBzKx4GLnSizTt8VYkwwFIL-FHbvKrh18ws0i5aq7yWAqmki9F6kOw5wA2g8L9bF",
        "DOI": "https://doi.org/10.1016/j.inffus.2019.12.012",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article arrieta2020explainable,\n  title= Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI ,\n  author= Arrieta, Alejandro Barredo and D 'i az-Rodr 'i guez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garc 'i a, Salvador and Gil-L 'o pez, Sergio and Molina, Daniel and Benjamins, Richard and others ,\n  journal= Information fusion ,\n  volume= 58 ,\n  pages= 82--115 ,\n  year= 2020 ,\n  publisher= Elsevier \n \n"
    },
    {
        "id": 3,
        "reference": "A. Adadi and M. Berrada, 'Peeking inside the black-box: a survey on explainable artificial intelligence (XAI),' IEEE access, vol. 6, pp. 52 138–52 160, 2018.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "adadi2018peeking",
            "title": "Peeking inside the black-box: a survey on explainable artificial intelligence (XAI)",
            "author": "Adadi, Amina and Berrada, Mohammed",
            "journal": "IEEE access",
            "volume": "6",
            "pages": "52138--52160",
            "year": 2018,
            "publisher": "IEEE"
        },
        "citation_count": 5679,
        "abstract": "At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to identify the major existing XAI trends. We review a broad spectrum of methods and efforts in this area, classify and discuss the technical aspects of XAI, and shed light on future research directions.",
        "keywords": [
            "Explainable Artificial Intelligence",
            "Deep Learning",
            "Interpretability",
            "Machine Learning",
            "Artificial Intelligence",
            "Transparency"
        ],
        "APA": "Adadi, A., & Berrada, M. (2018). Peeking inside the black-box: a survey on explainable artificial intelligence (XAI). IEEE access, 6, 52138-52160.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=Peeking+inside+the+black-box%3A+a+survey+on+explainable+artificial+intelligence+%28XAI%29&btnG=",
        "externalLink": "https://ieeexplore.ieee.org/abstract/document/8466590",
        "DOI": "https://doi.org/10.1109/ACCESS.2018.2870052",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article adadi2018peeking,\n  title= Peeking inside the black-box: a survey on explainable artificial intelligence (XAI) ,\n  author= Adadi, Amina and Berrada, Mohammed ,\n  journal= IEEE access ,\n  volume= 6 ,\n  pages= 52138--52160 ,\n  year= 2018 ,\n  publisher= IEEE \n \n"
    },
    {
        "id": 4,
        "reference": "A. Chaddad, J. Peng, J. Xu, and A. Bouridane, “Survey of explainable AI techniques in healthcare,” Sensors, vol. 23, no. 2, p. 634, 2023.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "chaddad2023survey",
            "title": "Survey of explainable AI techniques in healthcare",
            "author": "Chaddad, Ahmad and Peng, Jie and Xu, Jiali and Bouridane, Ahmed",
            "journal": "Sensors",
            "volume": "23",
            "number": "2",
            "pages": "634",
            "year": 2023,
            "publisher": "MDPI"
        },
        "citation_count": 129,
        "abstract": "Artificial intelligence (AI) has been increasingly used in healthcare to aid in the diagnosis and treatment of various medical conditions. However, the black-box nature of deep learning models poses significant challenges in critical fields like healthcare, where trust and transparency are paramount. Explainable AI (XAI) aims to address this challenge by providing human-understandable explanations for decisions made by AI models. This survey provides a comprehensive overview of current XAI techniques used in healthcare, focusing on the approaches that make AI systems more transparent and interpretable. The review highlights how XAI methods have been applied in different areas of healthcare, including medical imaging, predictive modeling, and personalized treatment. It also discusses the future directions for integrating XAI in healthcare to ensure that AI systems can be effectively trusted and adopted by medical professionals.",
        "keywords": [
            "explainable AI",
            "healthcare",
            "deep learning",
            "transparency",
            "interpretability",
            "medical imaging"
        ],
        "APA": "Chaddad, A., Peng, J., Xu, J., & Bouridane, A. (2023). Survey of explainable AI techniques in healthcare. Sensors, 23(2), 634.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Chaddad%2C+J.+Peng%2C+J.+Xu%2C+and+A.+Bouridane%2C+%E2%80%9CSurvey+of+explain-+able+ai+techniques+in+healthcare%2C%E2%80%9D+Sensors%2C+vol.+23%2C+no.+2%2C+p.+634%2C+2023.&btnG=",
        "external_link": "https://www.mdpi.com/1424-8220/23/2/634",
        "DOI": "https://doi.org/10.3390/s23020634",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article alicioglu2022survey,\n  title= A survey of visual analytics for explainable artificial intelligence methods ,\n  author= Alicioglu, Gulsum and Sun, Bo ,\n  journal= Computers & Graphics ,\n  volume= 102 ,\n  pages= 502--520 ,\n  year= 2022 ,\n  publisher= Elsevier \n \n"
    },
    {
        "id": 5,
        "reference": "A. Das and P. Rad, “Opportunities and challenges in explainable artificial intelligence (XAI): A survey,” arXiv preprint arXiv:2006.11371, 2020.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "das2020opportunities",
            "title": "Opportunities and challenges in explainable artificial intelligence (XAI): A survey",
            "author": "Das, Arun and Rad, Paul",
            "journal": "arXiv preprint arXiv:2006.11371",
            "year": 2020
        },
        "citation_count": 860,
        "abstract": "Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.",
        "keywords": [
            "explainable AI",
            "XAI",
            "interpretable deep learning",
            "machine learning",
            "computer vision",
            "neural network"
        ],
        "APA": "Das, A., & Rad, P. (2020). Opportunities and challenges in explainable artificial intelligence (XAI): A survey. arXiv preprint arXiv:2006.11371.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Das+and+P.+Rad%2C+%E2%80%9COpportunities+and+challenges+in+ex-+plainable+artificial+intelligence+%28xai%29%3A+A+survey%2C%E2%80%9D+arXiv+preprint+arXiv%3A2006.11371%2C+2020.&btnG=#d=gs_cit&t=1728098769721&u=%2Fscholar%3Fq%3Dinfo%3AJMFUJCFILOsJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Den",
        "external_link": "https://arxiv.org/abs/2006.11371",
        "DOI": "https://doi.org/10.48550/arXiv.2006.11371",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article das2020opportunities,\n  title= Opportunities and challenges in explainable artificial intelligence (xai): A survey ,\n  author= Das, Arun and Rad, Paul ,\n  journal= arXiv preprint arXiv:2006.11371 ,\n  year= 2020 \n \n"
    },
    {
        "id": 6,
        "reference": "G. Schwalbe and B. Finzel, 'A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts,' Data Mining and Knowledge Discovery, pp. 1–59, 2023.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "schwalbe2023comprehensive",
            "title": "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts",
            "author": "Schwalbe, Gesina and Finzel, Bettina",
            "journal": "Data Mining and Knowledge Discovery",
            "pages": "1--59",
            "year": 2023,
            "publisher": "Springer"
        },
        "citation_count": 171,
        "abstract": "The field of explainable artificial intelligence (XAI) has seen a variety of terminologies, approaches, and evaluation criteria emerge. With the rapid growth of XAI methods, a comprehensive taxonomy is needed by researchers and practitioners to navigate the topic, compare methods, and choose the appropriate XAI method based on specific use-case needs. This paper unifies existing taxonomies and provides a systematic survey of surveys, reviewing over 50 of the most cited and recent surveys on XAI methods, metrics, and traits. We develop a structured taxonomy to serve as a reference for both newcomers and experts in XAI. More than 50 example methods are categorized, providing insights into the breadth and depth of the XAI landscape. The taxonomy offers a foundation for targeted, use-case-oriented, and context-sensitive future research in XAI.",
        "keywords": [
            "explainable artificial intelligence",
            "interpretability",
            "taxonomy",
            "meta-analysis"
        ],
        "APA": "Schwalbe, G., & Finzel, B. (2023). A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts. Data Mining and Knowledge Discovery, 1-59.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=G.+Schwalbe+and+B.+Finzel%2C+%E2%80%9CA+comprehensive+taxonomy+for+explainable+artificial+intelligence%3A+a+systematic+survey+of+surveys+on+methods+and+concepts%2C%E2%80%9D+Data+Mining+and+Knowledge+Discovery%2C+pp.+1%E2%80%9359%2C+2023.&btnG=#d=gs_cit&t=1728099139081&u=%2Fscholar%3Fq%3Dinfo%3A1tFqoIRayvQJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Den",
        "externalLink": "https://link.springer.com/article/10.1007/S10618-022-00867-8",
        "DOI": "https://doi.org/10.1007/s10618-022-00867-8",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article schwalbe2023comprehensive,\n  title= A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts ,\n  author= Schwalbe, Gesina and Finzel, Bettina ,\n  journal= Data Mining and Knowledge Discovery ,\n  pages= 1--59 ,\n  year= 2023 ,\n  publisher= Springer \n \n"
    },
    {
        "id": 7,
        "reference": "A. Chaddad, J. Peng, J. Xu, and A. Bouridane, “Survey of explainable AI techniques in healthcare,” Sensors, vol. 23, no. 2, p. 634, 2023.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "chaddad2023survey",
            "title": "Survey of explainable AI techniques in healthcare",
            "author": "Chaddad, Ahmad and Peng, Jihao and Xu, Jian and Bouridane, Ahmed",
            "journal": "Sensors",
            "volume": "23",
            "number": "2",
            "pages": "634",
            "year": 2023,
            "publisher": "MDPI"
        },
        "citation_count": 214,
        "abstract": "Artificial intelligence (AI) with deep learning models has been widely applied in numerous domains, including medical imaging and healthcare tasks. In the medical field, any judgment or decision is fraught with risk. A doctor will carefully judge whether a patient is sick before forming a reasonable explanation based on the patient’s symptoms and/or an examination. Therefore, to be a viable and accepted tool, AI needs to mimic human judgment and interpretation skills. Specifically, explainable AI (XAI) aims to explain the information behind the black-box model of deep learning that reveals how the decisions are made. This paper provides a survey of the most recent XAI techniques used in healthcare and related medical imaging applications. We summarize and categorize the XAI types, and highlight the algorithms used to increase interpretability in medical imaging topics. In addition, we focus on the challenging XAI problems in medical applications and provide guidelines to develop better interpretations of deep learning models using XAI concepts in medical image and text analysis. Furthermore, this survey provides future directions to guide developers and researchers for future prospective investigations on clinical topics, particularly on applications with medical imaging.",
        "keywords": [
            "explainable AI",
            "medical imaging",
            "deep learning",
            "radiomics"
        ],
        "APA": "Chaddad, A., Peng, J., Xu, J., & Bouridane, A. (2023). Survey of explainable AI techniques in healthcare. Sensors, 23(2), 634.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Chaddad%2C+J.+Peng%2C+J.+Xu%2C+and+A.+Bouridane%2C+%E2%80%9CSurvey+of+explain-+able+ai+techniques+in+healthcare%2C%E2%80%9D+Sensors%2C+vol.+23%2C+no.+2%2C+p.+634%2C+2023.&btnG=#d=gs_cit&t=1728099489412&u=%2Fscholar%3Fq%3Dinfo%3AS8SCrawJEBgJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Den",
        "external_link": "https://www.mdpi.com/1424-8220/23/2/634",
        "DOI": "https://doi.org/10.3390/s23020634",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article chaddad2023survey,\n  title= Survey of explainable AI techniques in healthcare ,\n  author= Chaddad, Ahmad and Peng, Jihao and Xu, Jian and Bouridane, Ahmed ,\n  journal= Sensors ,\n  volume= 23 ,\n  number= 2 ,\n  pages= 634 ,\n  year= 2023 ,\n  publisher= MDPI \n \n"
    },
    {
        "id": 8,
        "reference": "Z. Salahuddin, H. C. Woodruff, A. Chatterjee, and P. Lambin, “Transparency of deep neural networks for medical image analysis: A review of interpretability methods,” Computers in biology and medicine, vol. 140, p. 105111, 2022.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "salahuddin2022transparency",
            "title": "Transparency of deep neural networks for medical image analysis: A review of interpretability methods",
            "author": "Salahuddin, Zohaib and Woodruff, Henry C and Chatterjee, Avishek and Lambin, Philippe",
            "journal": "Computers in biology and medicine",
            "volume": "140",
            "pages": "105111",
            "year": 2022,
            "publisher": "Elsevier"
        },
        "citation_count": 301,
        "abstract": "Artificial Intelligence (AI) has emerged as a useful aid in numerous clinical applications for diagnosis and treatment decisions. Deep neural networks have shown the same or better performance than clinicians in many tasks owing to the rapid increase in the available data and computational power. In order to conform to the principles of trustworthy AI, it is essential that the AI system be transparent, robust, fair, and ensure accountability. Current deep neural solutions are referred to as black-boxes due to a lack of understanding of the specifics concerning the decision-making process. Therefore, there is a need to ensure the interpretability of deep neural networks before they can be incorporated into the routine clinical workflow. In this narrative review, we utilized systematic keyword searches and domain expertise to identify nine different types of interpretability methods that have been used for understanding deep learning models for medical image analysis applications based on the type of generated explanations and technical similarities. Furthermore, we report the progress made towards evaluating the explanations produced by various interpretability methods. Finally, we discuss limitations, provide guidelines for using interpretability methods and future directions concerning the interpretability of deep neural networks for medical imaging analysis.",
        "keywords": [
            "explainable artificial intelligence",
            "medical imaging",
            "explainability",
            "interpretability",
            "deep neural networks"
        ],
        "APA": "Salahuddin, Z., Woodruff, H. C., Chatterjee, A., & Lambin, P. (2022). Transparency of deep neural networks for medical image analysis: A review of interpretability methods. Computers in biology and medicine, 140, 105111.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+Z.+Salahuddin%2C+H.+C.+Woodruff%2C+A.+Chatterjee%2C+and+P.+Lambin%2C+%E2%80%9CTransparency+of+deep+neural+networks+for+medical+image+anal-+ysis%3A+A+review+of+interpretability+methods%2C%E2%80%9D+Computers+in+biology+and+medicine%2C+vol.+140%2C+p.+105111%2C+2022.&btnG=",
        "external_link": "https://www.sciencedirect.com/science/article/pii/S0010482521009057",
        "DOI": "https://doi.org/10.1016/j.compbiomed.2021.105111",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article antoniadi2021current,\n  title= Current challenges and future opportunities for XAI in machine learning-based clinical decision support systems: a systematic review ,\n  author= Antoniadi, Anna Markella and Du, Yuhan and Guendouz, Yasmine and Wei, Lan and Mazo, Claudia and Becker, Brett A and Mooney, Catherine ,\n  journal= Applied Sciences ,\n  volume= 11 ,\n  number= 11 ,\n  pages= 5088 ,\n  year= 2021 ,\n  publisher= MDPI \n \n"
    },
    {
        "id": 9,
        "reference": "A. M. Antoniadi, Y. Du, Y. Guendouz, L. Wei, C. Mazo, B. A. Becker, and C. Mooney, “Current challenges and future opportunities for XAI in machine learning-based clinical decision support systems: a systematic review,” Applied Sciences, vol. 11, no. 11, p. 5088, 2021.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "antoniadi2021current",
            "title": "Current challenges and future opportunities for XAI in machine learning-based clinical decision support systems: a systematic review",
            "author": "Antoniadi, Anna Markella and Du, Yuhan and Guendouz, Yasmine and Wei, Lan and Mazo, Claudia and Becker, Brett A and Mooney, Catherine",
            "journal": "Applied Sciences",
            "volume": "11",
            "number": "11",
            "pages": "5088",
            "year": 2021,
            "publisher": "MDPI"
        },
        "citation_count": 446,
        "abstract": "Machine Learning and Artificial Intelligence (AI) more broadly have great immediate and future potential for transforming almost all aspects of medicine. However, in many applications, even outside medicine, a lack of transparency in AI applications has become increasingly problematic. This is particularly pronounced where users need to interpret the output of AI systems. Explainable AI (XAI) provides a rationale that allows users to understand why a system has produced a given output. The output can then be interpreted within a given context. One area that is in great need of XAI is that of Clinical Decision Support Systems (CDSSs). These systems support medical practitioners in their clinic decision-making and in the absence of explainability may lead to issues of under or over-reliance. Providing explanations for how recommendations are arrived at will allow practitioners to make more nuanced, and in some cases, life-saving decisions. The need for XAI in CDSS, and the medical field in general, is amplified by the need for ethical and fair decision-making and the fact that AI trained with historical data can be a reinforcement agent of historical actions and biases that should be uncovered. We performed a systematic literature review of work to-date in the application of XAI in CDSS. Tabular data processing XAI-enabled systems are the most common, while XAI-enabled CDSS for text analysis are the least common in literature. However, there are many opportunities for both as well as the need for new theoretical frameworks for the adoption of XAI in these systems.",
        "keywords": [
            "explainable AI",
            "clinical decision support systems",
            "machine learning",
            "transparency"
        ],
        "APA": "Antoniadi, A. M., Du, Y., Guendouz, Y., Wei, L., Mazo, C., Becker, B. A., & Mooney, C. (2021). Current challenges and future opportunities for XAI in machine learning-based clinical decision support systems: a systematic review. Applied Sciences, 11(11), 5088.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+M.+Antoniadi%2C+Y.+Du%2C+Y.+Guendouz%2C+L.+Wei%2C+C.+Mazo%2C+B.+A.+Becker%2C+and+C.+Mooney%2C+%E2%80%9CCurrent+challenges+and+future+opportunities+for+XAI+in+machine+learning-based+clinical+decision+support+systems%3A+a+systematic+review%2C%E2%80%9D+Applied+Sciences%2C+vol.+11%2C+no.+11%2C+p.+5088%2C+2021.&btnG=",
        "external_link": "https://www.mdpi.com/2076-3417/11/11/5088",
        "DOI": "https://doi.org/10.3390/app11115088",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article singh2020explainable,\n  title= Explainable deep learning models in medical image analysis ,\n  author= Singh, Amitojdeep and Sengupta, Sourya and Lakshminarayanan, Vasudevan ,\n  journal= Journal of imaging ,\n  volume= 6 ,\n  number= 6 ,\n  pages= 52 ,\n  year= 2020 ,\n  publisher= MDPI \n \n"
    },
    {
        "id": 10,
        "reference": "D. P. Craven, C. A. DeMarco, and J. A. Lewis, “Assessing explainability in radiomics,” Radiology, vol. 298, no. 3, pp. 397-398, 2021.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "craven2021assessing",
            "title": "Assessing explainability in radiomics",
            "author": "Craven, David P and DeMarco, Christopher A and Lewis, John A",
            "journal": "Radiology",
            "volume": "298",
            "number": "3",
            "pages": "397-398",
            "year": 2021,
            "publisher": "Radiological Society of North America"
        },
        "citation_count": 91,
        "abstract": "Radiomics is an emerging field that applies advanced computational techniques to extract quantitative features from medical images. The success of radiomics relies on accurate feature extraction and model training, but it is also essential to establish the interpretability and explainability of the models used. This is particularly important in clinical applications where the end-users (radiologists and oncologists) require clear and justifiable explanations for the decisions made based on radiomics-derived predictions. In this article, we discuss the significance of explainability in radiomics and present various approaches to assess and improve the interpretability of radiomics models. We emphasize the need for transparency and validation of the models to ensure trust and acceptance among clinicians.",
        "keywords": [
            "radiomics",
            "explainability",
            "interpretability",
            "medical imaging"
        ],
        "APA": "Craven, D. P., DeMarco, C. A., & Lewis, J. A. (2021). Assessing explainability in radiomics. Radiology, 298(3), 397-398.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=D.+P.+Craven%2C+C.+A.+DeMarco%2C+and+J.+A.+Lewis%2C+%E2%80%9CAssessing+explainability+in+radiomics%2C%E2%80%9D+Radiology%2C+vol.+298%2C+no.+3%2C+pp.+397-398%2C+2021.&btnG=",
        "external_link": "https://pubs.rsna.org/doi/full/10.1148/radiol.2021211971",
        "DOI": "https://doi.org/10.1148/radiol.2021211971",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article van2022explainable,\n  title= Explainable artificial intelligence (XAI) in deep learning-based medical image analysis ,\n  author= Van der Velden, Bas HM and Kuijf, Hugo J and Gilhuijs, Kenneth GA and Viergever, Max A ,\n  journal= Medical Image Analysis ,\n  volume= 79 ,\n  pages= 102470 ,\n  year= 2022 ,\n  publisher= Elsevier \n \n"
    },
    {
        "id": 11,
        "reference": "B. H. Van der Velden, H. J. Kuijf, K. G. Gilhuijs, and M. A. Viergever, “Explainable artificial intelligence (xai) in deep learning-based medical image analysis,” Medical Image Analysis, vol. 79, p. 102470, 2022.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "van2022explainable",
            "title": "Explainable artificial intelligence (XAI) in deep learning-based medical image analysis",
            "author": "Van der Velden, Bas HM and Kuijf, Hugo J and Gilhuijs, Kenneth GA and Viergever, Max A",
            "journal": "Medical Image Analysis",
            "volume": "79",
            "pages": "102470",
            "year": 2022,
            "publisher": "Elsevier"
        },
        "citation_count": "660",
        "abstract": "With an increase in deep learning-based methods, the call for explainability of such methods grows, especially in high-stakes decision making areas such as medical image analysis. This survey presents an overview of explainable artificial intelligence (XAI) used in deep learning-based medical image analysis. A framework of XAI criteria is introduced to classify deep learning-based medical image analysis methods. Papers on XAI techniques in medical image analysis are then surveyed and categorized according to the framework and according to anatomical location. The paper concludes with an outlook of future opportunities for XAI in medical image analysis.",
        "keywords": [
            "Explainable artificial intelligence",
            "Interpretable deep learning",
            "Medical image analysis",
            "Deep learning",
            "Survey"
        ],
        "apa": "Van der Velden, B. H., Kuijf, H. J., Gilhuijs, K. G., & Viergever, M. A. (2022). Explainable artificial intelligence (XAI) in deep learning-based medical image analysis. Medical Image Analysis, 79, 102470.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+B.+H.+Van+der+Velden%2C+H.+J.+Kuijf%2C+K.+G.+Gilhuijs%2C+and+M.+A.+Viergever%2C+%E2%80%9CExplainable+artificial+intelligence+%28xai%29+in+deep+learning-based+medical+image+analysis%2C%E2%80%9D+Medical+Image+Analysis%2C+vol.+79%2C+p.+102470%2C+2022.&btnG=#d=gs_cit&t=1728100545987&u=%2Fscholar%3Fq%3Dinfo%3Am8uDFwkg3Q8J%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Den",
        "externalLink": "https://www.sciencedirect.com/science/article/pii/S1361841522001177",
        "doi": "https://doi.org/10.1016/j.media.2022.102470",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article nazir2023survey,\n  title= Survey of explainable artificial intelligence techniques for biomedical imaging with deep neural networks ,\n  author= Nazir, Sajid and Dickson, Diane M and Akram, Muhammad Usman ,\n  journal= Computers in Biology and Medicine ,\n  volume= 156 ,\n  pages= 106668 ,\n  year= 2023 ,\n  publisher= Elsevier \n \n"
    },
    {
        "id": 12,
        "reference": "S. Nazir, D. M. Dickson, and M. U. Akram, “Survey of explainable artificial intelligence techniques for biomedical imaging with deep neural networks,” Computers in Biology and Medicine, vol. 156, p. 106668, 2023.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "nazir2023survey",
            "title": "Survey of explainable artificial intelligence techniques for biomedical imaging with deep neural networks",
            "author": "Nazir, Sajid and Dickson, Diane M and Akram, Muhammad Usman",
            "journal": "Computers in Biology and Medicine",
            "volume": "156",
            "pages": "106668",
            "year": 2023,
            "publisher": "Elsevier"
        },
        "citation_count": "109",
        "abstract": "Artificial Intelligence (AI) techniques of deep learning have revolutionized the disease diagnosis with their outstanding image classification performance. In spite of the outstanding results, the widespread adoption of these techniques in clinical practice is still taking place at a moderate pace. One of the major hindrance is that a trained Deep Neural Networks (DNN) model provides a prediction, but questions about why and how that prediction was made remain unanswered. This linkage is of utmost importance for the regulated healthcare domain to increase the trust in the automated diagnosis system by the practitioners, patients and other stakeholders. The application of deep learning for medical imaging has to be interpreted with caution due to the health and safety concerns similar to blame attribution in the case of an accident involving autonomous cars. The consequences of both a false positive and false negative cases are far reaching for patients' welfare and cannot be ignored. This is exacerbated by the fact that the state-of-the-art deep learning algorithms comprise of complex interconnected structures, millions of parameters, and a ‘black box’ nature, offering little understanding of their inner working unlike the traditional machine learning algorithms. Explainable AI (XAI) techniques help to understand model predictions which help develop trust in the system, accelerate the disease diagnosis, and meet adherence to regulatory requirements.",
        "keywords": [
            "Interpretable AI",
            "Blackbox",
            "Features",
            "Supervised learning",
            "Predictive models",
            "Neural networks",
            "Diagnostic imaging",
            "Backpropagation"
        ],
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=S.+Nazir%2C+D.+M.+Dickson%2C+and+M.+U.+Akram%2C+%E2%80%9CSurvey+of+explainable+artificial+intelligence+techniques+for+biomedical+imaging+with+deep+neural+networks%2C%E2%80%9D+Computers+in+Biology+and+Medicine%2C+vol.+156%2C+p.+106668%2C+2023.&btnG=",
        "externalLink": "https://www.sciencedirect.com/science/article/pii/S0010482523001336",
        "doi": "https://doi.org/10.1016/j.compbiomed.2023.106668",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article tjoa2020survey,\n  title= A survey on explainable artificial intelligence (xai): Toward medical xai ,\n  author= Tjoa, Erico and Guan, Cuntai ,\n  journal= IEEE transactions on neural networks and learning systems ,\n  volume= 32 ,\n  number= 11 ,\n  pages= 4793--4813 ,\n  year= 2020 ,\n  publisher= IEEE \n \n"
    },
    {
        "id": 13,
        "reference": "E. Tjoa and C. Guan, “A survey on explainable artificial intelligence (xai): Toward medical xai,” IEEE transactions on neural networks and learning systems, vol. 32, no. 11, pp. 4793–4813, 2020.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "tjoa2020survey",
            "title": "A survey on explainable artificial intelligence (xai): Toward medical xai",
            "author": "Tjoa, Erico and Guan, Cuntai",
            "journal": "IEEE transactions on neural networks and learning systems",
            "volume": "32",
            "number": "11",
            "pages": "4793-4813",
            "year": 2020,
            "publisher": "IEEE"
        },
        "citation_count": "1780",
        "abstract": "Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.",
        "keywords": [
            "Explainable artificial intelligence",
            "Interpretability",
            "Machine learning",
            "Medical information system",
            "Survey"
        ],
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=E.+Tjoa+and+C.+Guan%2C+%E2%80%9CA+survey+on+explainable+artificial+intel-+ligence+%28xai%29%3A+Toward+medical+xai%2C%E2%80%9D+IEEE+transactions+on+neural+networks+and+learning+systems%2C+vol.+32%2C+no.+11%2C+pp.+4793%E2%80%934813%2C+2020&btnG=",
        "externalLink": "https://ieeexplore.ieee.org/abstract/document/9233366",
        "doi": "10.1109/TNNLS.2020.3027314",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article huang2020survey,\n  title= A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability ,\n  author= Huang, Xiaowei and Kroening, Daniel and Ruan, Wenjie and Sharp, James and Sun, Youcheng and Thamo, Emese and Wu, Min and Yi, Xinping ,\n  journal= Computer Science Review ,\n  volume= 37 ,\n  pages= 100270 ,\n  year= 2020 ,\n  publisher= Elsevier \n \n"
    },
    {
        "id": 14,
        "reference": "X. Huang, D. Kroening, W. Ruan, J. Sharp, Y. Sun, E. Thamo, M. Wu, and X. Yi, “A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability,” Computer Science Review, vol. 37, p. 100270, 2020.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "huang2020survey",
            "title": "A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability",
            "author": "Huang, Xiaowei and Kroening, Daniel and Ruan, Wenjie and Sharp, James and Sun, Youcheng and Thamo, Emese and Wu, Min and Yi, Xinping",
            "journal": "Computer Science Review",
            "volume": "37",
            "pages": "100270",
            "year": 2020,
            "publisher": "Elsevier"
        },
        "citation_count": 530,
        "abstract": "In the past few years, significant progress has been made on deep neural networks (DNNs) in achieving human-level performance on several long-standing tasks. With the broader deployment of DNNs on various applications, the concerns over their safety and trustworthiness have been raised in public, especially after the widely reported fatal incidents involving self-driving cars. Research to address these concerns is particularly active, with a significant number of papers released in the past few years. This survey paper conducts a review of the current research effort into making DNNs safe and trustworthy, by focusing on four aspects: verification, testing, adversarial attack and defence, and interpretability. In total, we survey 202 papers, most of which were published after 2017.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=X.+Huang%2C+D.+Kroening%2C+W.+Ruan%2C+J.+Sharp%2C+Y.+Sun%2C+E.+Thamo%2C+M.+Wu%2C+and+X.+Yi%2C+%E2%80%9CA+survey+of+safety+and+trustworthiness+of+deep+neural+networks%3A+Verification%2C+testing%2C+adversarial+attack+and+defence%2C+and+interpretability%2C%E2%80%9D+Computer+Science+Review%2C+vol.+37%2C+p.+100270%2C+2020.&btnG=",
        "externalLink": "https://www.sciencedirect.com/science/article/pii/S1574013719302527?casa_token=0LuY65fsR_4AAAAA:wc9_vnpwkrR-6BwzEx7gMUUVzesxkQk90np9zZNoCp4pxvnWy5a2H3mk9TzE-t3-bHubLKCD",
        "doi": "https://doi.org/10.1016/j.cosrev.2020.100270",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article reyes2020interpretability,\n  title= On the interpretability of artificial intelligence in radiology: challenges and opportunities ,\n  author= Reyes, Mauricio and Meier, Raphael and Pereira, S 'e rgio and Silva, Carlos A and Dahlweid, Fried-Michael and Tengg-Kobligk, Hendrik von and Summers, Ronald M and Wiest, Roland ,\n  journal= Radiology: artificial intelligence ,\n  volume= 2 ,\n  number= 3 ,\n  pages= e190043 ,\n  year= 2020 ,\n  publisher= Radiological Society of North America \n \n"
    },
    {
        "id": 15,
        "reference": "M. Reyes, R. Meier, S. Pereira, C. A. Silva, F.-M. Dahlweid, H. v. Tengg-Kobligk, R. M. Summers, and R. Wiest, “On the interpretability of artificial intelligence in radiology: challenges and opportunities,” Radiology: artificial intelligence, vol. 2, no. 3, p. e190043, 2020.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "reyes2020interpretability",
            "title": "On the interpretability of artificial intelligence in radiology: challenges and opportunities",
            "author": "Reyes, Mauricio and Meier, Raphael and Pereira, Sérgio and Silva, Carlos A and Dahlweid, Fried-Michael and Tengg-Kobligk, Hendrik von and Summers, Ronald M and Wiest, Roland",
            "journal": "Radiology: artificial intelligence",
            "volume": "2",
            "number": "3",
            "pages": "e190043",
            "year": 2020,
            "publisher": "Radiological Society of North America"
        },
        "citation_count": 411,
        "abstract": "As artificial intelligence (AI) systems begin to make their way into clinical radiology practice, it is crucial to assure that they function correctly and that they gain the trust of experts. Toward this goal, approaches to make AI “interpretable” have gained attention to enhance the understanding of a machine learning algorithm, despite its complexity. This article aims to provide insights into the current state of the art of interpretability methods for radiology AI. This review discusses radiologists’ opinions on the topic and suggests trends and challenges that need to be addressed to effectively streamline interpretability methods in clinical practice.",
        "keywords": [
            "Convolutional Neural Network (CNN)",
            "Informatics",
            "Radiomics",
            "Supervised learning",
            "Technology Assessment"
        ],
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=M.+Reyes%2C+R.+Meier%2C+S.+Pereira%2C+C.+A.+Silva%2C+F.-M.+Dahlweid%2C+H.+v.+Tengg-Kobligk%2C+R.+M.+Summers%2C+and+R.+Wiest%2C+%E2%80%9COn+the+interpretability+of+artificial+intelligence+in+radiology%3A+challenges+and+opportunities%2C%E2%80%9D+Radiology%3A+artificial+intelligence%2C+vol.+2%2C+no.+3%2C+p.+e190043%2C+2020.&btnG=",
        "externalLink": "https://pubs.rsna.org/doi/full/10.1148/ryai.2020190043",
        "doi": "https://doi.org/10.1148/ryai.2020190043",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article nguyen2020artificial,\n  title= Artificial intelligence in the battle against coronavirus (COVID-19): a survey and future research directions ,\n  author= Nguyen, Thanh Thi and Nguyen, Quoc Viet Hung and Nguyen, Dung Tien and Yang, Samuel and Eklund, Peter W and Huynh-The, Thien and Nguyen, Thanh Tam and Pham, Quoc-Viet and Razzak, Imran and Hsu, Edbert B ,\n  journal= arXiv preprint arXiv:2008.07343 ,\n  year= 2020 \n \n"
    },
    {
        "id": 16,
        "reference": "T. T. Nguyen, Q. V. H. Nguyen, D. T. Nguyen, S. Yang, P. W. Eklund, T. Huynh-The, T. T. Nguyen, Q.-V. Pham, I. Razzak, and E. B. Hsu, “Artificial intelligence in the battle against coronavirus (covid-19): a survey and future research directions,” arXiv preprint arXiv:2008.07343, 2020.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "nguyen2020artificial",
            "title": "Artificial intelligence in the battle against coronavirus (COVID-19): a survey and future research directions",
            "author": "Nguyen, Thanh Thi and Nguyen, Quoc Viet Hung and Nguyen, Dung Tien and Yang, Samuel and Eklund, Peter W and Huynh-The, Thien and Nguyen, Thanh Tam and Pham, Quoc-Viet and Razzak, Imran and Hsu, Edbert B",
            "journal": "arXiv preprint arXiv:2008.07343",
            "year": 2020
        },
        "citation_count": 287,
        "abstract": "Artificial intelligence (AI) has been applied widely in our daily lives in a variety of ways with numerous success stories. AI has also contributed to dealing with the coronavirus disease (COVID-19) pandemic, which has been happening around the globe. This paper presents a survey of AI methods being used in various applications in the fight against the COVID-19 outbreak and outlines the crucial role of AI research in this unprecedented battle. We touch on areas where AI plays as an essential component, from medical image processing, data analytics, text mining and natural language processing, the Internet of Things, to computational biology and medicine. A summary of COVID-19 related data sources that are available for research purposes is also presented. Research directions on exploring the potential of AI and enhancing its capability and power in the pandemic battle are thoroughly discussed. We identify 13 groups of problems related to the COVID-19 pandemic and highlight promising AI methods and tools that can be used to address these problems. It is envisaged that this study will provide AI researchers and the wider community with an overview of the current status of AI applications, and motivate researchers to harness AI's potential in the fight against COVID-19.",
        "googleScholarLink": "https://scholar.googleusercontent.com/scholar.bib?q=info:4XfatXFpnRoJ:scholar.google.com/&output=citation&scisdr=ClH65O8MEMCp87a5TpQ:AFWwaeYAAAAAZwC_VpRv8UrmNDwIvBTbjZMb_AY&scisig=AFWwaeYAAAAAZwC_Voz67ldjoZWFvmsNv_udm3I&scisf=4&ct=citation&cd=-1&hl=en",
        "externalLink": "https://arxiv.org/abs/2008.07343",
        "doi": "https://arxiv.org/abs/2008.07343",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article karthik2022ai,\n  title= Ai for COVID-19 detection from radiographs: Incisive analysis of state of the art techniques, key challenges and future directions ,\n  author= Karthik, R and Menaka, R and Hariharan, M and Kathiresan, GS ,\n  journal= IRBM ,\n  volume= 43 ,\n  number= 5 ,\n  pages= 486--510 ,\n  year= 2022 ,\n  publisher= Elsevier \n \n"
    },
    {
        "id": 17,
        "reference": "R. Karthik, R. Menaka, M. Hariharan, and G. Kathiresan, “Ai for COVID-19 detection from radiographs: Incisive analysis of state of the art techniques, key challenges and future directions,” IRBM, vol. 43, no. 5, pp. 486–510, 2022.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "karthik2022ai",
            "title": "Ai for COVID-19 detection from radiographs: Incisive analysis of state of the art techniques, key challenges and future directions",
            "author": "Karthik, R and Menaka, R and Hariharan, M and Kathiresan, GS",
            "journal": "IRBM",
            "volume": "43",
            "number": "5",
            "pages": "486--510",
            "year": 2022,
            "publisher": "Elsevier"
        },
        "citation_count": "19",
        "abstract": "In recent years, Artificial Intelligence has had an evident impact on the way research addresses challenges in different domains. This research aims to spotlight the impact of deep learning and machine learning models in the detection of COVID-19 from medical images, conducting a review of state-of-the-art approaches. The study reviews 140 research papers and focuses on classification and segmentation approaches for image-based COVID-19 detection using imaging modalities like X-rays and CT scans. This work discusses emerging trends and technical challenges in AI-based COVID-19 detection.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=R.+Karthik%2C+R.+Menaka%2C+M.+Hariharan%2C+and+G.+Kathiresan%2C+%E2%80%9CAi+for+covid-19+detection+from+radiographs%3A+Incisive+analysis+of+state+of+the+art+techniques%2C+key+challenges+and+future+directions%2C%E2%80%9D+IRBM%2C+vol.+43%2C+no.+5%2C+pp.+486%E2%80%93510%2C+2022.&btnG=",
        "externalLink": "https://www.sciencedirect.com/science/article/pii/S1959031821000956",
        "doi": "https://doi.org/10.1016/j.irbm.2021.07.002",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article weber2024applications,\n  title= Applications of explainable artificial intelligence in finance—a systematic review of finance, information systems, and computer science literature ,\n  author= Weber, Patrick and Carl, K Valerie and Hinz, Oliver ,\n  journal= Management Review Quarterly ,\n  volume= 74 ,\n  number= 2 ,\n  pages= 867--907 ,\n  year= 2024 ,\n  publisher= Springer \n \n"
    },
    {
        "id": 18,
        "reference": "P. Weber, K. V. Carl, and O. Hinz, “Applications of explainable artificial intelligence in finance—a systematic review of finance, information systems, and computer science literature,” Management Review Quarterly, vol. 74, no. 2, pp. 867–907, 2024.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "weber2024applications",
            "title": "Applications of explainable artificial intelligence in finance—a systematic review of finance, information systems, and computer science literature",
            "author": "Weber, Patrick and Carl, K Valerie and Hinz, Oliver",
            "journal": "Management Review Quarterly",
            "volume": "74",
            "number": "2",
            "pages": "867--907",
            "year": 2024,
            "publisher": "Springer"
        },
        "citation_count": "110",
        "abstract": "Digitalization and technologization have significantly impacted finance, requiring transparency for AI applications in regulated domains. This paper systematically reviews 2,022 articles from leading outlets in finance, information systems, and computer science, identifying 60 relevant studies on explainable AI (XAI) in finance. It explores XAI techniques for areas such as risk management, portfolio optimization, and anti-money laundering, while highlighting the need for post-hoc explainability in finance models.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+P.+Weber%2C+K.+V.+Carl%2C+and+O.+Hinz%2C+%E2%80%9CApplications+of+explainable+artificial+intelligence+in+finance%E2%80%94a+systematic+review+of+finance%2C+information+systems%2C+and+computer+science+literature%2C%E2%80%9D+Manage-+ment+Review+Quarterly%2C+vol.+74%2C+no.+2%2C+pp.+867%E2%80%93907%2C+2024.&btnG=",
        "externalLink": "https://link.springer.com/article/10.1007/S11301-023-00320-0",
        "doi": "https://link.springer.com/article/10.1007/S11301-023-00320-0",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article owens2022explainable,\n  title= Explainable artificial intelligence (xai) in insurance ,\n  author= Owens, Emer and Sheehan, Barry and Mullins, Martin and Cunneen, Martin and Ressel, Juliane and Castignani, German ,\n  journal= Risks ,\n  volume= 10 ,\n  number= 12 ,\n  pages= 230 ,\n  year= 2022 ,\n  publisher= MDPI \n \n"
    },
    {
        "id": 19,
        "reference": "E. Owens, B. Sheehan, M. Mullins, M. Cunneen, J. Ressel, and G. Castignani, “Explainable artificial intelligence (XAI) in insurance,” Risks, vol. 10, no. 12, p. 230, 2022.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "owens2022explainable",
            "title": "Explainable artificial intelligence (XAI) in insurance",
            "author": "Owens, Emer and Sheehan, Barry and Mullins, Martin and Cunneen, Martin and Ressel, Juliane and Castignani, German",
            "journal": "Risks",
            "volume": "10",
            "number": "12",
            "pages": "230",
            "year": 2022,
            "publisher": "MDPI"
        },
        "citation_count": "34",
        "abstract": "Explainable AI (XAI) methods enhance the transparency and interpretability of machine learning models, which is critical for the insurance industry. This study analyzes XAI applications within the insurance value chain, identifying knowledge distillation and rule extraction as common techniques for simplifying models. It highlights the importance of XAI in ensuring trust and transparency in insurance AI applications.",
        "googleScholarLink": "https://scholar.googleusercontent.com/scholar.bib?q=info:4XfatXFpnRoJ:scholar.google.com/&output=citation&scisdr=ClH65O8MEMCp87a5TpQ:AFWwaeYAAAAAZwC_VpRv8UrmNDwIvBTbjZMb_AY&scisig=AFWwaeYAAAAAZwC_Voz67ldjoZWFvmsNv_udm3I&scisf=4&ct=citation&cd=-1&hl=en",
        "externalLink": "https://www.mdpi.com/2227-9091/10/12/230",
        "doi": "https://doi.org/10.3390/risks10120230",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article mohamed2022review,\n  title= A review of visualisation-as-explanation techniques for convolutional neural networks and their evaluation ,\n  author= Mohamed, Elhassan and Sirlantzis, Konstantinos and Howells, Gareth ,\n  journal= Displays ,\n  volume= 73 ,\n  pages= 102239 ,\n  year= 2022 ,\n  publisher= Elsevier \n \n"
    },
    {
        "id": 20,
        "reference": "E. Mohamed, K. Sirlantzis, and G. Howells, “A review of visualisation-as-explanation techniques for convolutional neural networks and their evaluation,” Displays, vol. 73, p. 102239, 2022.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "mohamed2022review",
            "title": "A review of visualisation-as-explanation techniques for convolutional neural networks and their evaluation",
            "author": "Mohamed, Elhassan and Sirlantzis, Konstantinos and Howells, Gareth",
            "journal": "Displays",
            "volume": "73",
            "pages": "102239",
            "year": 2022,
            "publisher": "Elsevier"
        },
        "citation_count": 43,
        "abstract": "Visualisation techniques are powerful tools to understand the behaviour of Artificial Intelligence (AI) systems. They can be used to identify important features contributing to the network decisions, investigate biases in datasets, and find weaknesses in the system's structure (e.g., network architectures). Lawmakers and regulators may not allow the use of smart systems if these systems cannot explain the logic underlying a decision or action taken. These systems are required to offer a high level of 'transparency' to be approved for deployment. Model transparency is vital for safety–critical applications such as autonomous navigation and operation systems (e.g., autonomous trains or cars), where prediction errors may have serious implications. Thus, being highly accurate without explaining the basis of their performance is not enough to satisfy regulatory requirements. The lack of system interpretability is a major obstacle to the wider adoption of AI in safety–critical applications. Explainable Artificial Intelligence (XAI) techniques applied to intelligent systems to justify their decisions offers a possible solution. In this review, we present state-of-the-art explanation techniques in detail. We focus our presentation and critical discussion on visualisation methods for the most adopted architecture in use, the Convolutional Neural Networks (CNNs), applied to the domain of image classification. Further, we discuss the evaluation techniques for different explanation methods, which shows that some of the most visually appealing methods are unreliable and can be considered a simple feature or edge detector. In contrast, robust methods can give insights into the model behaviour, which helps to enhance the model performance and boost the confidence in the model's predictions. Besides, the applications of XAI techniques show their importance in many fields such as medicine and industry. We hope that this review proves a valuable contribution for researchers in the field of XAI.",
        "keywords": [
            "Activation heatmaps",
            "Architecture understanding",
            "Black-box representations",
            "CNN visualisation",
            "Convolutional neural networks",
            "Explainable AI",
            "Feature visualisation",
            "Interpretable neural networks",
            "Saliency maps",
            "XAI"
        ],
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+E.+Mohamed%2C+K.+Sirlantzis%2C+and+G.+Howells%2C+%E2%80%9CA+review+of+visualisation-as-explanation+techniques+for+convolutional+neural+networks+and+their+evaluation%2C%E2%80%9D+Displays%2C+vol.+73%2C+p.+102239%2C+2022.&btnG=",
        "externalLink": "https://www.sciencedirect.com/science/article/pii/S014193822200066X",
        "doi": "https://doi.org/10.1016/j.displa.2022.102239",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@book samek2019explainable,\n  title= Explainable AI: interpreting, explaining and visualizing deep learning ,\n  author= Samek, Wojciech and Montavon, Gr 'e goire and Vedaldi, Andrea and Hansen, Lars Kai and M \"u ller, Klaus-Robert ,\n  volume= 11700 ,\n  year= 2019 ,\n  publisher= Springer Nature \n \n"
    },
    {
        "id": 21,
        "reference": "W. Samek, G. Montavon, A. Vedaldi, L. K. Hansen, and K.-R. M ¨uller, Explainable AI: interpreting, explaining and visualizing deep learning. Springer Nature, 2019, vol. 11700.",
        "bibtex": {
            "entryType": "book",
            "citationKey": "samek2019explainable",
            "title": "Explainable AI: interpreting, explaining and visualizing deep learning",
            "author": "Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert",
            "volume": "11700",
            "year": 2019,
            "publisher": "Springer Nature"
        },
        "citation_count": 1328,
        "abstract": "The development of “intelligent” systems that can take decisions and perform autonomously might lead to faster and more consistent decisions. A limiting factor for a broader adoption of AI technology is the inherent risks that come with giving up human control and oversight to “intelligent” machines. For sensitive tasks involving critical infrastructures and affecting human well-being or health, it is crucial to limit the possibility of improper, non-robust and unsafe decisions and actions. Before deploying an AI system, we see a strong need to validate its behavior, and thus establish guarantees that it will continue to perform as expected when deployed in a real-world environment. In pursuit of that objective, ways for humans to verify the agreement between the AI decision structure and their own ground-truth knowledge have been explored. Explainable AI (XAI) has developed as a subfield of AI, focused on exposing complex AI models to humans in a systematic and interpretable manner. The 22 chapters included in this book provide a timely snapshot of algorithms, theory, and applications of interpretable and explainable AI and AI techniques that have been proposed recently reflecting the current discourse in this field and providing directions of future development. The book is organized in six parts: towards AI transparency; methods for interpreting AI systems; explaining the decisions of AI systems; evaluating interpretability and explanations; applications of explainable AI; and software for explainable AI.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+W.+Samek%2C+G.+Montavon%2C+A.+Vedaldi%2C+L.+K.+Hansen%2C+and+K.-R.+M+%C2%A8uller%2C+Explainable+AI%3A+interpreting%2C+explaining+and+visualizing+deep+learning.+Springer+Nature%2C+2019%2C+vol.+11700.&btnG=",
        "externalLink": "https://books.google.com/books?hl=en&lr=&id=j5yuDwAAQBAJ&oi=fnd&pg=PR5&dq=+W.+Samek,+G.+Montavon,+A.+Vedaldi,+L.+K.+Hansen,+and+K.-R.+M+%C2%A8uller,+Explainable+AI:+interpreting,+explaining+and+visualizing+deep+learning.+Springer+Nature,+2019,+vol.+11700.&ots=Ir4UPC7R6F&sig=swaOPzfOwJbAVonOfsCT46Z1dZc#v=onepage&q=W.%20Samek%2C%20G.%20Montavon%2C%20A.%20Vedaldi%2C%20L.%20K.%20Hansen%2C%20and%20K.-R.%20M%20%C2%A8uller%2C%20Explainable%20AI%3A%20interpreting%2C%20explaining%20and%20visualizing%20deep%20learning.%20Springer%20Nature%2C%202019%2C%20vol.%2011700.&f=false",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article samek2021explaining,\n  title= Explaining deep neural networks and beyond: A review of methods and applications ,\n  author= Samek, Wojciech and Montavon, Gr 'e goire and Lapuschkin, Sebastian and Anders, Christopher J and M \"u ller, Klaus-Robert ,\n  journal= Proceedings of the IEEE ,\n  volume= 109 ,\n  number= 3 ,\n  pages= 247--278 ,\n  year= 2021 ,\n  publisher= IEEE \n \n"
    },
    {
        "id": 22,
        "reference": "W. Samek, G. Montavon, S. Lapuschkin, C. J. Anders, and K.-R. M ¨uller, “Explaining deep neural networks and beyond: A review of methods and applications,” Proceedings of the IEEE, vol. 109, no. 3, pp. 247–278, 2021.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "samek2021explaining",
            "title": "Explaining deep neural networks and beyond: A review of methods and applications",
            "author": "Samek, Wojciech and Montavon, Grégoire and Lapuschkin, Sebastian and Anders, Christopher J and Müller, Klaus-Robert",
            "journal": "Proceedings of the IEEE",
            "volume": "109",
            "number": "3",
            "pages": "247--278",
            "year": 2021,
            "publisher": "IEEE"
        },
        "citation_count": 1066,
        "abstract": "With the broader and highly successful usage of machine learning (ML) in industry and the sciences, there has been a growing demand for explainable artificial intelligence (XAI). Interpretability and explanation methods for gaining a better understanding of the problem-solving abilities and strategies of nonlinear ML, in particular, deep neural networks, are, therefore, receiving increased attention. In this work, we aim to: 1) provide a timely overview of this active emerging field, with a focus on “post hoc” explanations, and explain its theoretical foundations; 2) summarize the most relevant explanation techniques for deep learning, including model-specific and model-agnostic methods, and 3) discuss various applications of explainable AI methods in practice and their benefits in research and development. The paper is structured as follows: we first describe the general paradigm of XAI. We then review the various classes of explanation methods with a focus on techniques based on attribution, visualization, and interpretability. Finally, we highlight the limitations and challenges of these techniques.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+W.+Samek%2C+G.+Montavon%2C+S.+Lapuschkin%2C+C.+J.+Anders%2C+and+K.-R.+M+%C2%A8uller%2C+%E2%80%9CExplaining+deep+neural+networks+and+beyond%3A+A+review+of+methods+and+applications%2C%E2%80%9D+Proceedings+of+the+IEEE%2C+vol.+109%2C+no.+3%2C+pp.+247%E2%80%93278%2C+2021.&btnG=",
        "externalLink": "https://ieeexplore.ieee.org/document/9328957",
        "doi": "https://doi.org/10.1109/JPROC.2020.3005781",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article saeed2023explainable,\n  title= Explainable AI (XAI): A systematic meta-survey of current challenges and future opportunities ,\n  author= Saeed, Waddah and Omlin, Christian ,\n  journal= Knowledge-Based Systems ,\n  volume= 263 ,\n  pages= 110273 ,\n  year= 2023 ,\n  publisher= Elsevier \n \n"
    },
    {
        "id": 23,
        "reference": "W. Saeed and C. Omlin, “Explainable AI (XAI): A systematic meta-survey of current challenges and future opportunities,” Knowledge-Based Systems, vol. 263, p. 110273, 2023.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "saeed2023explainable",
            "title": "Explainable AI (XAI): A systematic meta-survey of current challenges and future opportunities",
            "author": "Saeed, Waddah and Omlin, Christian",
            "journal": "Knowledge-Based Systems",
            "volume": "263",
            "pages": "110273",
            "year": 2023,
            "publisher": "Elsevier"
        },
        "citation_count": 393,
        "abstract": "The past decade has seen significant progress in artificial intelligence (AI), which has resulted in algorithms being adopted for resolving a variety of problems. However, this success has been met by increasing model complexity and employing black-box AI models that lack transparency. In response to this need, Explainable AI (XAI) has been proposed to make AI more transparent and thus advance the adoption of AI in critical domains. Although there are several reviews of XAI topics in the literature that have identified challenges and potential research directions of XAI, these challenges and research directions are scattered. This study, hence, presents a systematic meta-survey of challenges and future research directions in XAI organized in two themes: (1) general challenges and research directions of XAI and (2) challenges and research directions of XAI based on machine learning life cycle’s phases: design, development, and deployment. We believe that our meta-survey contributes to XAI literature by providing a guide for future exploration in the XAI area.",
        "keywords": [
            "Explainable AI (XAI)",
            "Interpretable AI",
            "Black-box",
            "Machine learning",
            "Deep learning",
            "Meta-survey",
            "Responsible AI"
        ],
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+W.+Saeed+and+C.+Omlin%2C+%E2%80%9CExplainable+ai+%28xai%29%3A+A+systematic+meta-+survey+of+current+challenges+and+future+opportunities%2C%E2%80%9D+Knowledge-+Based+Systems%2C+vol.+263%2C+p.+110273%2C+2023.&btnG=",
        "externalLink": "https://www.sciencedirect.com/science/article/pii/S0950705123000230",
        "doi": "https://doi.org/10.1016/j.knosys.2023.110273",
        "group": "#ff474c",
        "groupName": "Survey Paper",
        "bibTexContent": " \n@article ghorbani2019towards,\n  title= Towards automatic concept-based explanations ,\n  author= Ghorbani, Amirata and Wexler, James and Zou, James Y and Kim, Been ,\n  journal= Advances in neural information processing systems ,\n  volume= 32 ,\n  year= 2019 \n \n"
    },
    {
        "id": 24,
        "reference": "A. Ghorbani, J. Wexler, J. Y. Zou, and B. Kim, “Towards automatic concept-based explanations,” Advances in neural information processing systems, vol. 32, 2019.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "ghorbani2019towards",
            "title": "Towards automatic concept-based explanations",
            "author": "Ghorbani, Amirata and Wexler, James and Zou, James Y and Kim, Been",
            "journal": "Advances in neural information processing systems",
            "volume": "32",
            "year": 2019
        },
        "citation_count": 698,
        "abstract": "Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, systematically summarizing and interpreting such per-sample feature importance scores is challenging. In this work, we propose principles and desiderata for concept-based explanations, which go beyond per-sample features to identify higher-level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that ACE discovers concepts that are human-meaningful, coherent, and important for the neural network's predictions.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Ghorbani%2C+J.+Wexler%2C+J.+Y.+Zou%2C+and+B.+Kim%2C+%E2%80%9CTowards+automatic+concept-based+explanations%2C%E2%80%9D+Advances+in+neural+information+processing+systems%2C+vol.+32%2C+2019.&btnG=",
        "external_link": "https://proceedings.neurips.cc/paper/2019/hash/77d2afcb31f6493e350fca61764efb9a-Abstract.html",
        "doi": "https://proceedings.neurips.cc/paper/2019/hash/77d2afcb31f6493e350fca61764efb9a-Abstract.html",
        "group": "#f4cccc",
        "groupName": "Concept Based Paper",
        "bibTexContent": " \n@article huang2022conceptexplainer,\n  title= Conceptexplainer: Interactive explanation for deep neural networks from a concept perspective ,\n  author= Huang, Jinbin and Mishra, Aditi and Kwon, Bum Chul and Bryan, Chris ,\n  journal= IEEE Transactions on Visualization and Computer Graphics ,\n  volume= 29 ,\n  number= 1 ,\n  pages= 831--841 ,\n  year= 2022 ,\n  publisher= IEEE \n \n"
    },
    {
        "id": 25,
        "reference": "J. Huang, A. Mishra, B. C. Kwon, and C. Bryan, “Conceptexplainer: Interactive explanation for deep neural networks from a concept perspective,” IEEE Transactions on Visualization and Computer Graphics, vol. 29, no. 1, pp. 831–841, 2022.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "huang2022conceptexplainer",
            "title": "Conceptexplainer: Interactive explanation for deep neural networks from a concept perspective",
            "author": "Huang, Jinbin and Mishra, Aditi and Kwon, Bum Chul and Bryan, Chris",
            "journal": "IEEE Transactions on Visualization and Computer Graphics",
            "volume": "29",
            "number": "1",
            "pages": "831--841",
            "year": 2022,
            "publisher": "IEEE"
        },
        "citation_count": 31,
        "abstract": "Traditional deep learning interpretability methods suitable for model users cannot explain network behaviors at the global level and are inflexible in providing fine-grained explanations. Concept-based explanations are gaining attention due to their human intuitiveness and flexibility in describing both global and local model behaviors. Concepts, groups of similarly meaningful pixels expressing a notion embedded within the network's latent space, have been commonly hand-generated, but recently discovered by automated approaches. However, navigating and understanding the concept space can be difficult due to its magnitude and diversity. Visual analytics can bridge these gaps by enabling structured navigation and exploration of the concept space. In this paper, we design and validate Conceptexplainer, a visual analytics system that allows users to probe and explore concept spaces interactively to explain model behavior at various levels, helping identify classification-relevant concepts, biases, and shared concepts across dissimilar classes.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=J.+Huang%2C+A.+Mishra%2C+B.+C.+Kwon%2C+and+C.+Bryan%2C+%E2%80%9CConceptexplainer%3A+Interactive+explanation+for+deep+neural+networks+from+a+concept+perspective%2C%E2%80%9D+IEEE+Transactions+on+Visualization+and+Computer+Graphics%2C+vol.+29%2C+no.+1%2C+pp.+831%E2%80%93841%2C+2022.&btnG=",
        "external_link": "https://ieeexplore.ieee.org/abstract/document/9903285?casa_token=CrVOZ_tJMIEAAAAA:u3JBNEBX4XEAxsijp_KUOOMKNunCMgkHSXadzh7EaW0FGd0069EBDwR0HA5OaNGZcDZDjc2w",
        "doi": "10.1109/TVCG.2022.3209384",
        "group": "#f4cccc",
        "groupName": "Concept Based Paper",
        "bibTexContent": " \n@techreport keele2007guidelines,\n  title= Guidelines for performing systematic literature reviews in software engineering ,\n  author= Keele, Staffs and others ,\n  year= 2007 ,\n  institution= Technical report, ver. 2.3 ebse technical report. ebse \n \n"
    },
    {
        "id": 27,
        "reference": "R. J. Piper, “How to write a systematic literature review: a guide for medical students,” National AMR, fostering medical research, vol. 1, pp. 1–8, 2013.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "piper2013write",
            "title": "How to write a systematic literature review: a guide for medical students",
            "author": "Piper, Rory J",
            "journal": "National AMR, fostering medical research",
            "volume": "1",
            "pages": "1--8",
            "year": 2013,
            "publisher": "University of Edinburgh Edinburgh, UK"
        },
        "citation_count": 0,
        "abstract": "Objectives This guide aims to serve as a practical introduction to: • the rationale for conducting a systematic review of the literature • how to search the literature • qualitative and quantitative interpretation • how to structure a systematic review manuscript.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=R.+J.+Piper%2C+%E2%80%9CHow+to+write+a+systematic+literature+review%3A+a+guide+for+medical+students%2C%E2%80%9D+National+AMR%2C+fostering+medical+research%2C+vol.+1%2C+pp.+1%E2%80%938%2C+2013.&btnG=",
        "external_link": "https://www.southampton.ac.uk/assets/imported/transforms/content-block/UsefulDownloads_Download/A02316A7B39E4EE09F62F3210",
        "group": "#f4cccc",
        "groupName": "Concept Based Paper",
        "bibTexContent": " \n@article piper2013write,\n  title= How to write a systematic literature review: a guide for medical students ,\n  author= Piper, Rory J ,\n  journal= National AMR, fostering medical research ,\n  volume= 1 ,\n  pages= 1--8 ,\n  year= 2013 ,\n  publisher= University of Edinburgh Edinburgh, UK \n \n"
    },
    {
        "id": 28,
        "reference": "H. R. Kouchaksaraei and H. Karl, “Service function chaining across OpenStack and Kubernetes domains,” in Proceedings of the 13th ACM International Conference on Distributed and Event-based Systems, 2019, pp. 240–243.",
        "bibtex": {
            "entryType": "inproceedings",
            "citationKey": "kouchaksaraei2019service",
            "title": "Service function chaining across OpenStack and Kubernetes domains",
            "author": "Kouchaksaraei, Hadi Razzaghi and Karl, Holger",
            "booktitle": "Proceedings of the 13th ACM International Conference on Distributed and Event-based Systems",
            "pages": "240--243",
            "year": 2019
        },
        "citation_count": 0,
        "abstract": "Remarkable advantages of Containers (CNs) over Virtual Machines (VMs) such as lower overhead and faster startup has gained the attention of Communication Service Providers (CSPs) as using CNs for providing Virtual Network Functions (VNFs) can save costs while increasing the service agility. However, as it is not feasible to realise all types of VNFs in CNs, the coexistence of VMs and CNs is proposed. To put VMs and CNs together, an orchestration framework that can chain services across distributed and heterogeneous domains is required. To this end, we implemented a framework by extending and consolidating state-of-the-art tools and technologies originated from Network Function Virtualization (NFV), Software-defined Networking (SDN) and cloud computing environments. This framework chains services provisioned across Kubernetes and OpenStack domains. During the demo, we deploy a service consist of CN- and VM-based VNFs to demonstrate different features provided by our framework.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=H.+R.+Kouchaksaraei+and+H.+Karl%2C+%E2%80%9CService+function+chaining+across+openstack+and+kubernetes+domains%2C%E2%80%9D+in+Proceedings+of+the+13th+ACM+International+Conference+on+Distributed+and+Event-based+Systems%2C+2019%2C+pp.+240%E2%80%93243.&btnG=",
        "externalLink": "https://dl.acm.org/doi/abs/10.1145/3328905.3332505?casa_token=LuBiUXRUkWIAAAAA:3fHre3tJVKbdDsQt_SfZahz-V9KMNviMCbHWTbVwJGj5g3Do-pZO9YaLcIO7tK5gvwPQ_oFgfN4",
        "doi": "https://doi.org/10.1007/978-3-658-42798-6_17",
        "group": "#f4cccc",
        "groupName": "Concept Based Paper",
        "bibTexContent": " \n@inproceedings kouchaksaraei2019service,\n  title= Service function chaining across openstack and kubernetes domains ,\n  author= Kouchaksaraei, Hadi Razzaghi and Karl, Holger ,\n  booktitle= Proceedings of the 13th ACM International Conference on Distributed and Event-based Systems ,\n  pages= 240--243 ,\n  year= 2019 \n \n"
    },
    {
        "id": 29,
        "reference": "Y. Xiao and M. Watson, “Guidance on conducting a systematic literature review,” Journal of planning education and research, vol. 39, no. 1, pp. 93–112, 2019.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "xiao2019guidance",
            "title": "Guidance on conducting a systematic literature review",
            "author": "Xiao, Yu and Watson, Maria",
            "journal": "Journal of planning education and research",
            "volume": "39",
            "number": "1",
            "pages": "93--112",
            "year": 2019
        },
        "citation_count": 0,
        "publisher": "SAGE Publications Sage CA: Los Angeles, CA",
        "abstract": "Literature reviews establish the foundation of academic inquiries. However, in the planning field, we lack rigorous systematic reviews. In this article, through a systematic search on the methodology of literature review, we categorize a typology of literature reviews, discuss steps in conducting a systematic literature review, and provide suggestions on how to enhance rigor in literature reviews in planning education and research.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+Y.+Xiao+and+M.+Watson%2C+%E2%80%9CGuidance+on+conducting+a+systematic+literature+review%2C%E2%80%9D+Journal+of+planning+education+and+research%2C+vol.+39%2C+no.+1%2C+pp.+93%E2%80%93112%2C+2019.&btnG=",
        "externalLink": "https://journals.sagepub.com/doi/abs/10.1177/0739456X17723971",
        "doi": "https://doi.org/10.1177/0739456X17723971",
        "group": "#f4cccc",
        "groupName": "Concept Based Paper",
        "bibTexContent": " \n@article xiao2019guidance,\n  title= Guidance on conducting a systematic literature review ,\n  author= Xiao, Yu and Watson, Maria ,\n  journal= Journal of planning education and research ,\n  volume= 39 ,\n  number= 1 ,\n  pages= 93--112 ,\n  year= 2019 ,\n  publisher= SAGE Publications Sage CA: Los Angeles, CA \n \n"
    },
    {
        "id": 39,
        "reference": "M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional networks,” in Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13. Springer, 2014, pp. 818–833.",
        "bibtex": {
            "entryType": "inproceedings",
            "citationKey": "zeiler2014visualizing",
            "title": "Visualizing and understanding convolutional networks",
            "author": "Zeiler, Matthew D and Fergus, Rob",
            "booktitle": "Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13",
            "pages": "818--833",
            "year": 2014,
            "organization": "Springer"
        },
        "citation_count": 23173,
        "abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However, there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+M.+D.+Zeiler+and+R.+Fergus%2C+%E2%80%9CVisualizing+and+understanding+convolutional+networks%2C%E2%80%9D+in+Computer+Vision%E2%80%93ECCV+2014%3A+13th+European+Conference%2C+Zurich%2C+Switzerland%2C+September+6-12%2C+2014%2C+Pro-+ceedings%2C+Part+I+13.+Springer%2C+2014%2C+pp.+818%E2%80%93833.&btnG=",
        "externalLink": "https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53",
        "group": "#ffe5cc",
        "groupName": "Perturbation Based Paper",
        "bibTexContent": " \n@inproceedings zeiler2014visualizing,\n  title= Visualizing and understanding convolutional networks ,\n  author= Zeiler, Matthew D and Fergus, Rob ,\n  booktitle= Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13 ,\n  pages= 818--833 ,\n  year= 2014 ,\n  organization= Springer \n \n"
    },
    {
        "id": 40,
        "reference": "J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, “Striving for simplicity: The all convolutional net,” arXiv preprint arXiv:1412.6806, 2014.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "springenberg2014striving",
            "title": "Striving for simplicity: The all convolutional net",
            "author": "Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin",
            "journal": "arXiv preprint arXiv:1412.6806",
            "year": 2014
        },
        "citation_count": 5969,
        "abstract": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the 'deconvolution approach' for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=J.+T.+Springenberg%2C+A.+Dosovitskiy%2C+T.+Brox%2C+and+M.+Riedmiller%2C+%E2%80%9CStriving+for+simplicity%3A+The+all+convolutional+net%2C%E2%80%9D+arXiv+preprint+arXiv%3A1412.6806%2C+2014.&btnG=",
        "external_link": "https://arxiv.org/abs/1412.6806",
        "doi": "https://doi.org/10.48550/arXiv.1412.6806",
        "group": "#ffe5cc",
        "groupName": "Perturbation Based Paper",
        "bibTexContent": " \n@article springenberg2014striving,\n  title= Striving for simplicity: The all convolutional net ,\n  author= Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin ,\n  journal= arXiv preprint arXiv:1412.6806 ,\n  year= 2014 \n \n"
    },
    {
        "id": 41,
        "reference": "M. Noroozi and P. Favaro, “Unsupervised learning of visual representations by solving jigsaw puzzles,” in European conference on computer vision. Springer, 2016, pp. 69–84.",
        "bibtex": {
            "entryType": "inproceedings",
            "citationKey": "noroozi2016unsupervised",
            "title": "Unsupervised learning of visual representations by solving jigsaw puzzles",
            "author": "Noroozi, Mehdi and Favaro, Paolo",
            "booktitle": "European conference on computer vision",
            "pages": "69--84",
            "year": 2016,
            "organization": "Springer"
        },
        "citation_count": 3447,
        "abstract": "We propose a novel unsupervised learning approach to build features suitable for object detection and classification. The features are pre-trained on a large dataset without human annotation and later transferred via fine-tuning on a different, smaller and labeled dataset. The pre-training consists of solving jigsaw puzzles of natural images. To facilitate the transfer of features to other tasks, we introduce the context-free network (CFN), a siamese-ennead convolutional neural network. The features correspond to the columns of the CFN and they process image tiles independently (i.e., free of context). The later layers of the CFN then use the features to identify their geometric arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. We pre-train the CFN on the training set of the ILSVRC2012 dataset and transfer the features on the combined training and validation set of Pascal VOC 2007 for object detection (via fast RCNN) and classification. These features outperform all current unsupervised features for detection and for classification, and reduce the gap with supervised learning.",
        "keywords": [
            "Unsupervised learning",
            "Image representation learning",
            "Self-supervised learning",
            "Feature transfer"
        ],
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=%5D+M.+Noroozi+and+P.+Favaro%2C+%E2%80%9CUnsupervised+learning+of+visual+representations+by+solving+jigsaw+puzzles%2C%E2%80%9D+in+European+conference+on+computer+vision.+Springer%2C+2016%2C+pp.+69%E2%80%9384.&btnG=",
        "external_link": "https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5",
        "group": "#ffe5cc",
        "groupName": "Perturbation Based Paper",
        "bibTexContent": " \n@inproceedings noroozi2016unsupervised,\n  title= Unsupervised learning of visual representations by solving jigsaw puzzles ,\n  author= Noroozi, Mehdi and Favaro, Paolo ,\n  booktitle= European conference on computer vision ,\n  pages= 69--84 ,\n  year= 2016 ,\n  organization= Springer \n \n"
    },
    {
        "id": 42,
        "reference": "X. Zheng, X. Wu, L. Huan, W. He, and H. Zhang, “A gather-to-guide network for remote sensing semantic segmentation of rgb and auxiliary image,” IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 1–15, 2021.",
        "bibtex": {
            "entryType": "article",
            "citationKey": "zheng2021gather",
            "title": "A gather-to-guide network for remote sensing semantic segmentation of RGB and auxiliary image",
            "author": "Zheng, Xianwei and Wu, Xiujie and Huan, Linxi and He, Wei and Zhang, Hongyan",
            "journal": "IEEE Transactions on Geoscience and Remote Sensing",
            "volume": "60",
            "pages": "1--15",
            "year": 2021,
            "publisher": "IEEE"
        },
        "citation_count": 33,
        "abstract": "Convolutional neural network (CNN)-based feature fusion of RGB and auxiliary remote sensing data is known to enable improved semantic segmentation. However, such fusion is challengeable because of the substantial variance in data characteristics and quality (e.g., data uncertainties and misalignment) between two modality data. In this article, we propose a unified gather-to-guide network (G2GNet) for remote sensing semantic segmentation of RGB and auxiliary data. The key aspect of the proposed architecture is a novel gather-to-guide module (G2GM) that consists of a feature gatherer and a feature guider. The feature gatherer generates a set of cross-modal descriptors by absorbing the complementary merits of RGB and auxiliary modality data. The feature guider calibrates the RGB feature response by using the channel-wise guide weights extracted from the cross-modal descriptors. In this way, the G2GM can perform RGB feature calibration with different modality data in a gather-to-guide fashion, thus preserving the informative features while suppressing redundant and noisy information. Extensive experiments conducted on two benchmark datasets show that the proposed G2GNet is robust to data uncertainties while also improving the semantic segmentation performance of RGB and auxiliary remote sensing data.",
        "keywords": [
            "Deep learning",
            "Remote sensing",
            "Semantic segmentation"
        ],
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=X.+Zheng%2C+X.+Wu%2C+L.+Huan%2C+W.+He%2C+and+H.+Zhang%2C+%E2%80%9CA+gather-to-+guide+network+for+remote+sensing+semantic+segmentation+of+rgb+and+auxiliary+image%2C%E2%80%9D+IEEE+Transactions+on+Geoscience+and+Remote+Sensing%2C+vol.+60%2C+pp.+1%E2%80%9315%2C+2021.&btnG=",
        "external_link": "https://ieeexplore.ieee.org/abstract/document/9519842?casa_token=ZihAl5LX75wAAAAA:aUJp2FCf46fODVtal6VylXgWcS2xiX5Cn91QsTVDm6zhY_7_AVdgqmP7AJAKcVD_nrX9GGTn",
        "doi": "10.1109/TGRS.2021.3103517",
        "group": "#ffe5cc",
        "groupName": "Perturbation Based Paper",
        "bibTexContent": " \n@article zheng2021gather,\n  title= A gather-to-guide network for remote sensing semantic segmentation of RGB and auxiliary image ,\n  author= Zheng, Xianwei and Wu, Xiujie and Huan, Linxi and He, Wei and Zhang, Hongyan ,\n  journal= IEEE Transactions on Geoscience and Remote Sensing ,\n  volume= 60 ,\n  pages= 1--15 ,\n  year= 2021 ,\n  publisher= IEEE \n \n"
    },
    {
        "id": 43,
        "reference": "T. Park, M.-Y. Liu, T.-C. Wang, and J.-Y. Zhu, “Semantic image synthesis with spatially-adaptive normalization,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 2337–2346.",
        "bibtex": {
            "type": "article",
            "cite_key": "park2019semantic",
            "title": "Semantic image synthesis with spatially-adaptive normalization",
            "author": "Park, T. and Liu, M.-Y. and Wang, T.-C. and Zhu, J.-Y.",
            "booktitle": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "pages": "2337--2346",
            "year": 2019
        },
        "citation_count": 287,
        "abstract": "We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the network, forcing the network to memorize the information throughout all the layers. Instead, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned affine transformation. Experiments on several challenging datasets demonstrate the superiority of our method compared to existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows users to easily control the style and content of image synthesis results as well as create multi-modal results. Code is available upon publication.",
        "google_scholar_link": "https://scholar.googleusercontent.com/scholar.bib?q=info:4XfatXFpnRoJ:scholar.google.com/&output=citation&scisdr=ClH65O8MEMCp87a5TpQ:AFWwaeYAAAAAZwC_VpRv8UrmNDwIvBTbjZMb_AY&scisig=AFWwaeYAAAAAZwC_Voz67ldjoZWFvmsNv_udm3I&scisf=4&ct=citation&cd=-1&hl=en",
        "external_link": "https://openaccess.thecvf.com/content_CVPR_2019/html/Park_Semantic_Image_Synthesis_With_Spatially-Adaptive_Normalization_CVPR_2019_paper.html",
        "group": "#ffe5cc",
        "groupName": "Perturbation Based Paper",
        "bibTexContent": " \n@article nguyen2020artificial,\n  title= Artificial intelligence in the battle against coronavirus (COVID-19): a survey and future research directions ,\n  author= Nguyen, Thanh Thi and Nguyen, Quoc Viet Hung and Nguyen, Dung Tien and Yang, Samuel and Eklund, Peter W and Huynh-The, Thien and Nguyen, Thanh Tam and Pham, Quoc-Viet and Razzak, Imran and Hsu, Edbert B ,\n  journal= arXiv preprint arXiv:2008.07343 ,\n  year= 2020 \n \n"
    },
    {
        "id": 44,
        "reference": "J. W. Soh, G. Y. Park, J. Jo, and N. I. Cho, “Natural and realistic single image super-resolution with explicit natural manifold discrimination,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 8122–8131.",
        "bibtex": {
            "type": "inproceedings",
            "cite_key": "soh2019natural",
            "title": "Natural and realistic single image super-resolution with explicit natural manifold discrimination",
            "author": "Soh, J. W. and Park, G. Y. and Jo, J. and Cho, N. I.",
            "booktitle": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "pages": "8122--8131",
            "year": 2019
        },
        "citation_count": 146,
        "abstract": "Recently, many convolutional neural networks for single image super-resolution (SISR) have been proposed, which focus on reconstructing the high-resolution images in terms of objective distortion measures. However, the networks trained with objective loss functions generally fail to reconstruct the realistic fine textures and details that are essential for better perceptual quality. Recovering the realistic details remains a challenging problem, and only a few works have been proposed which aim at increasing the perceptual quality by generating enhanced textures. However, the generated fake details often make undesirable artifacts and the overall image looks somewhat unnatural. Therefore, in this paper, we present a new approach to reconstructing realistic super-resolved images with high perceptual quality, while maintaining the naturalness of the result. In particular, we focus on the domain prior properties of SISR problem. Specifically, we define the naturalness prior in the low-level domain and constrain the output image in the natural manifold, which eventually generates more natural and realistic images. Our results show better naturalness compared to the recent super-resolution algorithms including perception-oriented ones.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=J.+W.+Soh%2C+G.+Y.+Park%2C+J.+Jo%2C+and+N.+I.+Cho%2C+%E2%80%9CNatural+and+realistic+single+image+super-resolution+with+explicit+natural+manifold+dis-+JOURNAL+OF+LATEX+CLASS+FILES%2C+VOL.+14%2C+NO.+8%2C+AUGUST+2015+23+crimination%2C%E2%80%9D+in+Proceedings+of+the+IEEE%2FCVF+conference+on+computer+vision+and+pattern+recognition%2C+2019%2C+pp.+8122%E2%80%938131.&btnG=",
        "external_link": "https://openaccess.thecvf.com/content_CVPR_2019/html/Soh_Natural_and_Realistic_Single_Image_Super-Resolution_With_Explicit_Natural_Manifold_CVPR_2019_paper.html",
        "group": "#ffe5cc",
        "groupName": "Perturbation Based Paper",
        "bibTexContent": " \n@inproceedings soh2019natural,\n  title= Natural and realistic single image super-resolution with explicit natural manifold discrimination ,\n  author= Soh, Jae Woong and Park, Gu Yong and Jo, Junho and Cho, Nam Ik ,\n  booktitle= Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,\n  pages= 8122--8131 ,\n  year= 2019 \n \n"
    },
    {
        "id": 50,
        "reference": "K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside convolutional networks: Visualising image classification models and saliency maps,” arXiv preprint arXiv:1312.6034, 2013.",
        "bibtex": {
            "type": "article",
            "cite_key": "simonyan2013deep",
            "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
            "author": "Simonyan, Karen",
            "journal": "arXiv preprint arXiv:1312.6034",
            "year": 2013
        },
        "citation_count": 8702,
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score, thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=K.+Simonyan%2C+A.+Vedaldi%2C+and+A.+Zisserman%2C+%E2%80%9CDeep+inside+con-volutional+networks%3A+Visualising+image+classification+models+and+saliency+maps%2C%E2%80%9D+arXiv+preprint+arXiv%3A1312.6034%2C+2013.&btnG=",
        "external_link": "https://arxiv.org/pdf/1312.6034",
        "group": "#ffe5cc",
        "groupName": "Perturbation Based Paper",
        "bibTexContent": "\n@article simonyan2013deep,\n  title= Deep inside convolutional networks: Visualising image classification models and saliency maps ,\n  author= Simonyan, Karen ,\n  journal= arXiv preprint arXiv:1312.6034 ,\n  year= 2013 \n \n"
    },
    {
        "id": 51,
        "reference": "D. Balduzzi, M. Frean, L. Leary, J. Lewis, K. W.-D. Ma, and B. McWilliams, “The shattered gradients problem: If resnets are the answer, then what is the question?” in International Conference on Machine Learning. PMLR, 2017, pp. 342–350.",
        "bibtex": {
            "type": "inproceedings",
            "cite_key": "balduzzi2017shattered",
            "title": "The shattered gradients problem: If resnets are the answer, then what is the question?",
            "author": "Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian",
            "booktitle": "International conference on machine learning",
            "pages": "342--350",
            "year": 2017
        },
        "citation_count": 457,
        "abstract": "A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new 'looks linear' (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=D.+Balduzzi%2C+M.+Frean%2C+L.+Leary%2C+J.+Lewis%2C+K.+W.-D.+Ma%2C+and+B.+McWilliams%2C+%E2%80%9CThe+shattered+gradients+problem%3A+If+resnets+are+the+answer%2C+then+what+is+the+question%3F%E2%80%9D+in+International+Conference+on+Machine+Learning.+PMLR%2C+2017%2C+pp.+342%E2%80%93350.&btnG=",
        "external_link": "https://proceedings.mlr.press/v70/balduzzi17b.html",
        "group": "#ffe5cc",
        "groupName": "Perturbation Based Paper",
        "bibTexContent": "\n@inproceedings balduzzi2017shattered,\n  title= The shattered gradients problem: If resnets are the answer, then what is the question? ,\n  author= Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian ,\n  booktitle= International conference on machine learning ,\n  pages= 342--350 ,\n  year= 2017 ,\n  organization= PMLR \n \n"
    },
    {
        "id": 52,
        "reference": "M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep networks,” in International conference on machine learning. PMLR, 2017, pp. 3319–3328.",
        "bibtex": {
            "type": "inproceedings",
            "cite_key": "sundararajan2017axiomatic",
            "title": "Axiomatic attribution for deep networks",
            "author": "Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi",
            "booktitle": "International conference on machine learning",
            "pages": "3319--3328",
            "year": 2017,
            "organization": "PMLR"
        },
        "citation_count": 6652,
        "abstract": "We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=M.+Sundararajan%2C+A.+Taly%2C+and+Q.+Yan%2C+%E2%80%9CAxiomatic+attribution+for+deep+networks%2C%E2%80%9D+in+International+conference+on+machine+learning.+PMLR%2C+2017%2C+pp.+3319%E2%80%933328.&btnG=",
        "external_link": "https://proceedings.mlr.press/v70/sundararajan17a.html",
        "group": "#ffe5cc",
        "groupName": "Perturbation Based Paper",
        "bibTexContent": "\n@inproceedings sundararajan2017axiomatic,\n  title= Axiomatic attribution for deep networks ,\n  author= Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi ,\n  booktitle= International conference on machine learning ,\n  pages= 3319--3328 ,\n  year= 2017 ,\n  organization= PMLR \n \n"
    },
    {
        "id": 53,
        "reference": "M. T. Ribeiro, S. Singh, and C. Guestrin, “Why should I trust you?” explaining the predictions of any classifier,” in Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 2016, pp. 1135–1144.",
        "bibtex": {
            "type": "inproceedings",
            "cite_key": "ribeiro2016should",
            "title": "Why should I trust you? Explaining the predictions of any classifier",
            "author": "Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos",
            "booktitle": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining",
            "pages": "1135--1144",
            "year": 2016
        },
        "citation_count": 19603,
        "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g., random forests) and image classification (e.g., neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=M.+T.+Ribeiro%2C+S.+Singh%2C+and+C.+Guestrin%2C+%E2%80%9C%E2%80%9D+why+should+i+trust+you%3F%E2%80%9D+explaining+the+predictions+of+any+classifier%2C%E2%80%9D+in+Proceedings+of+the+22nd+ACM+SIGKDD+international+conference+on+knowledge+discovery+and+data+mining%2C+2016%2C+pp.+1135%E2%80%931144.&btnG=",
        "external_link": "https://dl.acm.org/doi/abs/10.1145/2939672.2939778",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": "\n@inproceedings ribeiro2016should,\n  title= \" Why should i trust you?\" Explaining the predictions of any classifier ,\n  author= Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos ,\n  booktitle= Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining ,\n  pages= 1135--1144 ,\n  year= 2016 \n \n"
    },
    {
        "id": 54,
        "reference": "B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning deep features for discriminative localization,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2921–2929.",
        "bibtex": {
            "type": "inproceedings",
            "cite_key": "zhou2016learning",
            "title": "Learning deep features for discriminative localization",
            "author": "Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio",
            "booktitle": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "pages": "2921--2929",
            "year": 2016
        },
        "citation_count": 11940,
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13] and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=B.+Zhou%2C+A.+Khosla%2C+A.+Lapedriza%2C+A.+Oliva%2C+and+A.+Torralba%2C+%E2%80%9CLearning+deep+features+for+discriminative+localization%2C%E2%80%9D+in+Proceedings+of+the+IEEE+conference+on+computer+vision+and+pattern+recognition%2C+2016%2C+pp.+2921%E2%80%932929.&btnG=",
        "external_link": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": " \n@inproceedings zhou2016learning,\n  title= Learning deep features for discriminative localization ,\n  author= Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio ,\n  booktitle= Proceedings of the IEEE conference on computer vision and pattern recognition ,\n  pages= 2921--2929 ,\n  year= 2016 \n \n"
    },
    {
        "id": 55,
        "reference": "R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, “Grad-cam: Visual explanations from deep networks via gradient-based localization,” in Proceedings of the IEEE international conference on computer vision, 2017, pp. 618–626.",
        "bibtex": {
            "type": "inproceedings",
            "cite_key": "selvaraju2017grad",
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "author": "Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv",
            "booktitle": "Proceedings of the IEEE international conference on computer vision",
            "pages": "618--626",
            "year": 2017
        },
        "citation_count": 20136,
        "abstract": "We propose a technique for producing 'visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for 'dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. VQA) or reinforcement learning, and needs no architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a 'stronger' deep network from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/ along with a demo on CloudCV.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=R.+R.+Selvaraju%2C+M.+Cogswell%2C+A.+Das%2C+R.+Vedantam%2C+D.+Parikh%2C+and+D.+Batra%2C+%E2%80%9CGrad-cam%3A+Visual+explanations+from+deep+networks+via+gradient-based+localization%2C%E2%80%9D+in+Proceedings+of+the+IEEE+international+conference+on+computer+vision%2C+2017%2C+pp.+618%E2%80%93626.&btnG=",
        "external_link": "https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": " \n@inproceedings selvaraju2017grad,\n  title= Grad-cam: Visual explanations from deep networks via gradient-based localization ,\n  author= Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv ,\n  booktitle= Proceedings of the IEEE international conference on computer vision ,\n  pages= 618--626 ,\n  year= 2017 \n \n"
    },
    {
        "id": 56,
        "reference": "A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian, “Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks,” in 2018 IEEE winter conference on applications of computer vision (WACV). IEEE, 2018, pp. 839–847.",
        "bibtex": {
            "type": "inproceedings",
            "cite_key": "chattopadhay2018grad",
            "title": "Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks",
            "author": "Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N",
            "booktitle": "2018 IEEE winter conference on applications of computer vision (WACV)",
            "pages": "839--847",
            "year": 2018,
            "organization": "IEEE"
        },
        "citation_count": 2944,
        "abstract": "Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision based problems. However, deep models are perceived as 'black box' methods considering the lack of understanding of their internal functioning. There has been a significant recent interest to develop explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose Grad-CAM++ to provide better visual explanations of CNN model predictions (when compared to Grad-CAM), in terms of better localization of objects as well as explaining occurrences of multiple objects of a class in a single image. We provide a mathematical explanation for the proposed method, Grad-CAM++, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the class label under consideration. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ indeed provides better visual explanations for a given CNN architecture when compared to Grad-CAM.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Chattopadhay%2C+A.+Sarkar%2C+P.+Howlader%2C+and+V.+N.+Balasubra-+manian%2C+%E2%80%9CGrad-cam%2B%2B%3A+Generalized+gradient-based+visual+expla-+nations+for+deep+convolutional+networks%2C%E2%80%9D+in+2018+IEEE+winter+conference+on+applications+of+computer+vision+%28WACV%29.+IEEE%2C+2018%2C+pp.+839%E2%80%93847.&btnG=",
        "external_link": "https://ieeexplore.ieee.org/document/8354201",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": " \n@inproceedings chattopadhay2018grad,\n  title= Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks ,\n  author= Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N ,\n  booktitle= 2018 IEEE winter conference on applications of computer vision (WACV) ,\n  pages= 839--847 ,\n  year= 2018 ,\n  organization= IEEE \n \n"
    },
    {
        "id": 57,
        "reference": "D. Smilkov, N. Thorat, B. Kim, F. Viégas, and M. Wattenberg, “Smoothgrad: removing noise by adding noise,” arXiv preprint arXiv:1706.03825, 2017.",
        "bibtex": {
            "type": "article",
            "cite_key": "smilkov2017smoothgrad",
            "title": "Smoothgrad: removing noise by adding noise",
            "author": "Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin",
            "journal": "arXiv preprint arXiv:1706.03825",
            "year": 2017
        },
        "citation_count": 2440,
        "abstract": "Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=D.+Smilkov%2C+N.+Thorat%2C+B.+Kim%2C+F.+Vi%C2%B4egas%2C+and+M.+Wattenberg%2C+%E2%80%9CSmoothgrad%3A+removing+noise+by+adding+noise%2C%E2%80%9D+arXiv+preprint+arXiv%3A1706.03825%2C+2017.&btnG=",
        "external_link": "https://arxiv.org/abs/1706.03825",
        "doi_link": "https://doi.org/10.48550/arXiv.1706.03825",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": " \n@article smilkov2017smoothgrad,\n  title= Smoothgrad: removing noise by adding noise ,\n  author= Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi 'e gas, Fernanda and Wattenberg, Martin ,\n  journal= arXiv preprint arXiv:1706.03825 ,\n  year= 2017 \n \n"
    },
    {
        "id": 58,
        "reference": "D. Omeiza, S. Speakman, C. Cintas, and K. Weldermariam, “Smooth grad-cam++: An enhanced inference level visualization technique for deep convolutional neural network models,” arXiv preprint arXiv:1908.01224, 2019.",
        "bibtex": {
            "type": "article",
            "citation_key": "omeiza2019smooth",
            "title": "Smooth grad-cam++: An enhanced inference level visualization technique for deep convolutional neural network models",
            "author": [
                "Omeiza, Daniel",
                "Speakman, Skyler",
                "Cintas, Celia",
                "Weldermariam, Komminist"
            ],
            "journal": "arXiv preprint arXiv:1908.01224",
            "year": 2019
        },
        "citation_count": 267,
        "abstract": "Gaining insight into how deep convolutional neural network models perform image classification and how to explain their outputs has been a concern for computer vision researchers and decision makers. These deep models are often referred to as black boxes due to the low comprehension of their internal workings. As an effort to develop explainable deep learning models, several methods have been proposed, such as finding gradients of class output with respect to the input image (sensitivity maps), class activation map (CAM), and Gradient-based Class Activation Maps (Grad-CAM). These methods underperform when localizing multiple occurrences of the same class and do not work for all CNNs. Additionally, Grad-CAM does not capture the entire object in completeness when used on single-object images, affecting performance on recognition tasks. With the intention to create an enhanced visual explanation in terms of visual sharpness, object localization, and explaining multiple occurrences of objects in a single image, we present Smooth Grad-CAM++, a technique that combines methods from two other recent techniques—SMOOTHGRAD and Grad-CAM++. Our Smooth Grad-CAM++ technique provides the capability of visualizing a layer, subset of feature maps, or subset of neurons within a feature map at each instance at the inference level (model prediction process). After experimenting with a few images, Smooth Grad-CAM++ produced more visually sharp maps with better localization of objects in the given input images when compared with other methods.",
        "keywords": [
            "Computer Vision",
            "Convolutional Neural Network",
            "Class Activation Maps"
        ],
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=D.+Omeiza%2C+S.+Speakman%2C+C.+Cintas%2C+and+K.+Weldermariam%2C+%E2%80%9CSmooth+grad-cam%2B%2B%3A+An+enhanced+inference+level+visualization+technique+for+deep+convolutional+neural+network+models%2C%E2%80%9D+arXiv+preprint+arXiv%3A1908.01224%2C+2019.&btnG=",
        "external_link": "https://arxiv.org/abs/1908.01224",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": " \n@article omeiza2019smooth,\n  title= Smooth grad-cam++: An enhanced inference level visualization technique for deep convolutional neural network models ,\n  author= Omeiza, Daniel and Speakman, Skyler and Cintas, Celia and Weldermariam, Komminist ,\n  journal= arXiv preprint arXiv:1908.01224 ,\n  year= 2019 \n \n"
    },
    {
        "id": 59,
        "reference": "M. D. Zeiler and R. Fergus, “Stochastic pooling for regularization of deep convolutional neural networks,” arXiv preprint arXiv:1301.3557, 2013.",
        "bibtex": {
            "type": "article",
            "citation_key": "zeiler2013stochastic",
            "title": "Stochastic pooling for regularization of deep convolutional neural networks",
            "author": [
                "Zeiler, Matthew D",
                "Fergus, Rob"
            ],
            "journal": "arXiv preprint arXiv:1301.3557",
            "year": 2013
        },
        "citation_count": 1358,
        "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=M.+D.+Zeiler+and+R.+Fergus%2C+%E2%80%9CStochastic+pooling+for+regular-+ization+of+deep+convolutional+neural+networks%2C%E2%80%9D+arXiv+preprint+arXiv%3A1301.3557%2C+2013.&btnG=",
        "external_link": "https://arxiv.org/abs/1301.3557",
        "doi_link": "https://doi.org/10.48550/arXiv.1301.3557",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": " \n@article zeiler2013stochastic,\n  title= Stochastic pooling for regularization of deep convolutional neural networks ,\n  author= Zeiler, Matthew D and Fergus, Rob ,\n  journal= arXiv preprint arXiv:1301.3557 ,\n  year= 2013 \n \n"
    },
    {
        "id": 60,
        "reference": "G. Zhao, B. Zhou, K. Wang, R. Jiang, and M. Xu, “Respond-cam: Analyzing deep models for 3d imaging data by visualizations,” in Medical Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part I. Springer, 2018, pp. 485–492.",
        "bibtex": {
            "type": "inproceedings",
            "citation_key": "zhao2018respond",
            "title": "Respond-cam: Analyzing deep models for 3d imaging data by visualizations",
            "author": [
                "Zhao, Guannan",
                "Zhou, Bo",
                "Wang, Kaiwen",
                "Jiang, Rui",
                "Xu, Min"
            ],
            "booktitle": "Medical Image Computing and Computer Assisted Intervention--MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part I",
            "pages": "485--492",
            "year": 2018,
            "organization": "Springer"
        },
        "citation_count": 58,
        "abstract": "The convolutional neural network (CNN) has become a powerful tool for various biomedical image analysis tasks, but there is a lack of visual explanation for the machinery of CNNs. In this paper, we present a novel algorithm, Respond-weighted Class Activation Mapping (Respond-CAM), for making CNN-based models interpretable by visualizing input regions that are important for predictions, especially for biomedical 3D imaging data inputs. Our method uses the gradients of any target concept (e.g. the score of the target class) that flow into a convolutional layer. The weighted feature maps are combined to produce a heatmap that highlights the important regions in the image for predicting the target concept. We prove a preferable sum-to-score property of the Respond-CAM and verify its significant improvement on 3D images from the current state-of-the-art approach. Our tests on Cellular Electron Cryo-Tomography 3D images show that Respond-CAM achieves superior performance on visualizing the CNNs with 3D biomedical image inputs, and is able to get reasonably good results on visualizing the CNNs with natural image inputs. The Respond-CAM is an efficient and reliable approach for visualizing the CNN machinery and is applicable to a wide variety of CNN model families and image analysis tasks. Our code is available at: https://github.com/xulabs/projects/tree/master/respond_cam.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=G.+Zhao%2C+B.+Zhou%2C+K.+Wang%2C+R.+Jiang%2C+and+M.+Xu%2C+%E2%80%9CRespond-cam%3A+Analyzing+deep+models+for+3d+imaging+data+by+visualizations%2C%E2%80%9D+in+Medical+Image+Computing+and+Computer+Assisted+Intervention%E2%80%93+MICCAI+2018%3A+21st+International+Conference%2C+Granada%2C+Spain%2C+Septem-+ber+16-20%2C+2018%2C+Proceedings%2C+Part+I.+Springer%2C+2018%2C+pp.+485%E2%80%93492.&btnG=",
        "external_link": "https://link.springer.com/chapter/10.1007/978-3-030-00928-1_55",
        "group": "#c7e9c0",
        "groupName": "Decomposition Based Paper",
        "bibTexContent": "\n@inproceedings zhao2018respond,\n  title= Respond-cam: Analyzing deep models for 3d imaging data by visualizations ,\n  author= Zhao, Guannan and Zhou, Bo and Wang, Kaiwen and Jiang, Rui and Xu, Min ,\n  booktitle= Medical Image Computing and Computer Assisted Intervention--MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part I ,\n  pages= 485--492 ,\n  year= 2018 ,\n  organization= Springer \n \n"
    },
    {
        "id": 61,
        "reference": "A. Mordvintsev, C. Olah, and M. Tyka, “Inceptionism: Going deeper into neural networks,” Google research blog, vol. 20, no. 14, p. 5, 2015.",
        "bibtex": {
            "type": "article",
            "cite_key": "mordvintsev2015inceptionism",
            "title": "Inceptionism: Going deeper into neural networks",
            "author": "Mordvintsev, Alexander and Olah, Christopher and Tyka, Mike",
            "journal": "Google research blog",
            "volume": 20,
            "number": 14,
            "pages": 5,
            "year": 2015
        },
        "citation_count": 784,
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Mordvintsev%2C+C.+Olah%2C+and+M.+Tyka%2C+%E2%80%9CInceptionism%3A+Going+deeper+into+neural+networks%2C%E2%80%9D+Google+research+blog%2C+vol.+20%2C+no.+14%2C+p.+5%2C+2015.&btnG=",
        "external_link": "https://research.google/pubs/inceptionism-going-deeper-into-neural-networks/",
        "group": "#c7e9c0",
        "groupName": "Decomposition Based Paper",
        "bibTexContent": " \n@article mordvintsev2015inceptionism,\n  title= Inceptionism: Going deeper into neural networks ,\n  author= Mordvintsev, Alexander and Olah, Christopher and Tyka, Mike ,\n  journal= Google research blog ,\n  volume= 20 ,\n  number= 14 ,\n  pages= 5 ,\n  year= 2015 \n \n"
    },
    {
        "id": 62,
        "reference": "S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Müller, and W. Samek, “On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation,” PloS one, vol. 10, no. 7, p. e0130140, 2015.",
        "bibtex": {
            "type": "article",
            "cite_key": "bach2015pixel",
            "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
            "author": "Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech",
            "journal": "PloS one",
            "volume": 10,
            "number": 7,
            "pages": "e0130140",
            "year": 2015,
            "publisher": "Public Library of Science San Francisco, CA USA"
        },
        "citation_count": 5036,
        "abstract": "Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=S.+Bach%2C+A.+Binder%2C+G.+Montavon%2C+F.+Klauschen%2C+K.-R.+M+%C2%A8uller%2C+and+W.+Samek%2C+%E2%80%9COn+pixel-wise+explanations+for+non-linear+classifier+decisions+by+layer-wise+relevance+propagation%2C%E2%80%9D+PloS+one%2C+vol.+10%2C+no.+7%2C+p.+e0130140%2C+2015.&btnG=",
        "external_link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140",
        "group": "#c7e9c0",
        "groupName": "Decomposition Based Paper",
        "bibTexContent": " \n@article bach2015pixel,\n  title= On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation ,\n  author= Bach, Sebastian and Binder, Alexander and Montavon, Gr 'e goire and Klauschen, Frederick and M \"u ller, Klaus-Robert and Samek, Wojciech ,\n  journal= PloS one ,\n  volume= 10 ,\n  number= 7 ,\n  pages= e0130140 ,\n  year= 2015 ,\n  publisher= Public Library of Science San Francisco, CA USA \n \n"
    },
    {
        "id": 63,
        "reference": "M. Ancona, E. Ceolini, C. Öztireli, and M. Gross, “Towards better understanding of gradient-based attribution methods for deep neural networks,” arXiv preprint arXiv:1711.06104, 2017.",
        "bibtex": {
            "type": "article",
            "cite_key": "ancona2017towards",
            "title": "Towards better understanding of gradient-based attribution methods for deep neural networks",
            "author": "Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus",
            "journal": "arXiv preprint arXiv:1711.06104",
            "year": 2017
        },
        "citation_count": 1185,
        "abstract": "Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gained increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n, and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=M.+Ancona%2C+E.+Ceolini%2C+C.+%C2%A8Oztireli%2C+and+M.+Gross%2C+%E2%80%9CTowards+better+understanding+of+gradient-based+attribution+methods+for+deep+neural+networks%2C%E2%80%9D+arXiv+preprint+arXiv%3A1711.06104%2C+2017.&btnG=",
        "external_link": "https://arxiv.org/abs/1711.06104",
        "doi_link": "https://doi.org/10.48550/arXiv.1711.06104",
        "group": "#c7e9c0",
        "groupName": "Decomposition Based Paper",
        "bibTexContent": " \n@article ancona2017towards,\n  title= Towards better understanding of gradient-based attribution methods for deep neural networks ,\n  author= Ancona, Marco and Ceolini, Enea and  \"O ztireli, Cengiz and Gross, Markus ,\n  journal= arXiv preprint arXiv:1711.06104 ,\n  year= 2017 \n \n"
    },
    {
        "id": 64,
        "reference": "J. Gu, Y. Yang, and V. Tresp, “Understanding individual decisions of cnns via contrastive backpropagation,” in Computer Vision–ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2–6, 2018, Revised Selected Papers, Part III 14. Springer, 2019, pp. 119–134.",
        "citation_count": 112,
        "abstract": "A number of backpropagation-based approaches such as DeConvNets, vanilla Gradient Visualization and Guided Backpropagation have been proposed to better understand individual decisions of deep convolutional neural networks. The saliency maps produced by them are proven to be non-discriminative. Recently, the Layer-wise Relevance Propagation (LRP) approach was proposed to explain the classification decisions of rectifier neural networks. In this work, we evaluate the discriminativeness of the generated explanations and analyze the theoretical foundation of LRP, i.e. Deep Taylor Decomposition. The experiments and analysis conclude that the explanations generated by LRP are not class-discriminative. Based on LRP, we propose Contrastive Layer-wise Relevance Propagation (CLRP), which is capable of producing instance-specific, class-discriminative, pixel-wise explanations. In the experiments, we use the CLRP to explain the decisions and understand the difference between neurons in individual classification decisions. We also evaluate the explanations quantitatively with a Pointing Game and an ablation study. Both qualitative and quantitative evaluations show that the CLRP generates better explanations than the LRP.",
        "keywords": [
            "Explainable deep learning",
            "LRP",
            "Discriminative saliency maps"
        ],
        "google_scholar": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=J.+Gu%2C+Y.+Yang%2C+and+V.+Tresp%2C+%E2%80%9CUnderstanding+individual+de-+cisions+of+cnns+via+contrastive+backpropagation%2C%E2%80%9D+in+Computer+Vision%E2%80%93ACCV+2018%3A+14th+Asian+Conference+on+Computer+Vision%2C+Perth%2C+Australia%2C+December+2%E2%80%936%2C+2018%2C+Revised+Selected+Papers%2C+Part+III+14.+Springer%2C+2019%2C+pp.+119%E2%80%93134.&btnG=",
        "external_link": "https://link.springer.com/chapter/10.1007/978-3-030-20893-6_8",
        "bibtex": {
            "type": "inproceedings",
            "citation_key": "gu2019understanding",
            "title": "Understanding individual decisions of cnns via contrastive backpropagation",
            "author": [
                "Gu, Jindong",
                "Yang, Yinchong",
                "Tresp, Volker"
            ],
            "booktitle": "Computer Vision--ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2--6, 2018, Revised Selected Papers, Part III 14",
            "pages": "119--134",
            "year": 2019,
            "organization": "Springer"
        },
        "group": "#c7e9c0",
        "groupName": "Decomposition Based Paper",
        "bibTexContent": " \n@inproceedings gu2019understanding,\n  title= Understanding individual decisions of cnns via contrastive backpropagation ,\n  author= Gu, Jindong and Yang, Yinchong and Tresp, Volker ,\n  booktitle= Computer Vision--ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2--6, 2018, Revised Selected Papers, Part III 14 ,\n  pages= 119--134 ,\n  year= 2019 ,\n  organization= Springer \n \n"
    },
    {
        "id": 65,
        "reference": "B. K. Iwana, R. Kuroki, and S. Uchida, “Explaining convolutional neural networks using softmax gradient layer-wise relevance propagation,” in 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW). IEEE, 2019, pp. 4176–4185.",
        "citation_count": 101,
        "abstract": "Convolutional Neural Networks (CNN) have become state-of-the-art in the field of image classification. However, not everything is understood about their inner representations. This paper tackles the interpretability and explainability of the predictions of CNNs for multi-class classification problems. Specifically, we propose a novel visualization method of pixel-wise input attribution called Softmax-Gradient Layer-wise Relevance Propagation (SGLRP). The proposed model is a class discriminate extension to Deep Taylor Decomposition (DTD) using the gradient of softmax to back propagate the relevance of the output probability to the input image. Through qualitative and quantitative analysis, we demonstrate that SGLRP can successfully localize and attribute the regions on input images which contribute to a target object's classification. We show that the proposed method excels at discriminating the target objects class from the other possible objects in the images. We confirm that SGLRP performs better than existing Layer-wise Relevance Propagation (LRP) based methods and can help in the understanding of the decision process of CNNs.",
        "ieee_keywords": [
            "Visualization",
            "Heating systems",
            "Convolutional neural networks",
            "Machine learning",
            "Robustness",
            "Computer vision"
        ],
        "google_scholar": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=B.+K.+Iwana%2C+R.+Kuroki%2C+and+S.+Uchida%2C+%E2%80%9CExplaining+convolu-+tional+neural+networks+using+softmax+gradient+layer-wise+rele-+vance+propagation%2C%E2%80%9D+in+2019+IEEE%2FCVF+International+Conference+on+Computer+Vision+Workshop+%28ICCVW%29.+IEEE%2C+2019%2C+pp.+4176%E2%80%934185.&btnG=",
        "external_link": "https://ieeexplore.ieee.org/document/9022542",
        "bibtex": {
            "type": "inproceedings",
            "citation_key": "iwana2019explaining",
            "title": "Explaining convolutional neural networks using softmax gradient layer-wise relevance propagation",
            "author": [
                "Iwana, Brian Kenji",
                "Kuroki, Ryohei",
                "Uchida, Seiichi"
            ],
            "booktitle": "2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)",
            "pages": "4176--4185",
            "year": 2019,
            "organization": "IEEE"
        },
        "group": "#c7e9c0",
        "groupName": "Decomposition Based Paper",
        "bibTexContent": " \n@inproceedings iwana2019explaining,\n  title= Explaining convolutional neural networks using softmax gradient layer-wise relevance propagation ,\n  author= Iwana, Brian Kenji and Kuroki, Ryohei and Uchida, Seiichi ,\n  booktitle= 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW) ,\n  pages= 4176--4185 ,\n  year= 2019 ,\n  organization= IEEE \n \n"
    },
    {
        "id": 66,
        "reference": "R. Achtibat, M. Dreyer, I. Eisenbraun, S. Bosse, T. Wiegand, W. Samek, and S. Lapuschkin, “From attribution maps to human-understandable explanations through concept relevance propagation,” Nature Machine Intelligence, vol. 5, no. 9, pp. 1006–1019, 2023.",
        "citation_count": 71,
        "abstract": "The field of explainable artificial intelligence (XAI) aims to bring transparency to today’s powerful but opaque deep learning models. While local XAI methods explain individual predictions in the form of attribution maps, thereby identifying ‘where’ important features occur (but not providing information about ‘what’ they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of method thus provide only partial insights and leave the burden of interpreting the model’s reasoning to the user. Here we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives and thus allows answering both the ‘where’ and ‘what’ questions for individual predictions. We demonstrate the capability of our method in various settings, showcasing that CRP leads to more human interpretable explanations and provides deep insights into the model’s representation and reasoning through concept atlases, concept-composition analyses, and quantitative investigations of concept subspaces and their role in fine-grained decision-making.",
        "google_scholar": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=R.+Achtibat%2C+M.+Dreyer%2C+I.+Eisenbraun%2C+S.+Bosse%2C+T.+Wiegand%2C+W.+Samek%2C+and+S.+Lapuschkin%2C+%E2%80%9CFrom+attribution+maps+to+human-+understandable+explanations+through+concept+relevance+propaga-+tion%2C%E2%80%9D+Nature+Machine+Intelligence%2C+vol.+5%2C+no.+9%2C+pp.+1006%E2%80%931019%2C+2023.&btnG=",
        "external_link": "https://www.nature.com/articles/s42256-023-00711-8",
        "bibtex": {
            "type": "article",
            "citation_key": "achtibat2023attribution",
            "title": "From attribution maps to human-understandable explanations through concept relevance propagation",
            "author": [
                "Achtibat, Reduan",
                "Dreyer, Maximilian",
                "Eisenbraun, Ilona",
                "Bosse, Sebastian",
                "Wiegand, Thomas",
                "Samek, Wojciech",
                "Lapuschkin, Sebastian"
            ],
            "journal": "Nature Machine Intelligence",
            "volume": 5,
            "number": 9,
            "pages": "1006--1019",
            "year": 2023,
            "publisher": "Nature Publishing Group UK London"
        },
        "group": "#c7e9c0",
        "groupName": "Decomposition Based Paper",
        "bibTexContent": " \n@article achtibat2023attribution,\n  title= From attribution maps to human-understandable explanations through concept relevance propagation ,\n  author= Achtibat, Reduan and Dreyer, Maximilian and Eisenbraun, Ilona and Bosse, Sebastian and Wiegand, Thomas and Samek, Wojciech and Lapuschkin, Sebastian ,\n  journal= Nature Machine Intelligence ,\n  volume= 5 ,\n  number= 9 ,\n  pages= 1006--1019 ,\n  year= 2023 ,\n  publisher= Nature Publishing Group UK London \n \n"
    },
    {
        "id": 67,
        "reference": "A. Shrikumar, P. Greenside, and A. Kundaje, “Learning important features through propagating activation differences,” in International conference on machine learning. PMLR, 2017, pp. 3145–3153.",
        "bibtex": {
            "type": "inproceedings",
            "citation_key": "shrikumar2017learning",
            "title": "Learning important features through propagating activation differences",
            "author": [
                "Shrikumar, Avanti",
                "Greenside, Peyton",
                "Kundaje, Anshul"
            ],
            "booktitle": "International conference on machine learning",
            "pages": "3145--3153",
            "year": 2017,
            "organization": "PMLR"
        },
        "citation_count": 4683,
        "abstract": "The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its reference activation and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL code: http://goo.gl/RM8jvH.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Shrikumar%2C+P.+Greenside%2C+and+A.+Kundaje%2C+%E2%80%9CLearning+important+features+through+propagating+activation+differences%2C%E2%80%9D+in+International+conference+on+machine+learning.+PMLR%2C+2017%2C+pp.+3145%E2%80%933153.&btnG=",
        "external_link": "https://proceedings.mlr.press/v70/shrikumar17a",
        "group": "#c7e9c0",
        "groupName": "Decomposition Based Paper",
        "bibTexContent": " \n@inproceedings shrikumar2017learning,\n  title= Learning important features through propagating activation differences ,\n  author= Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul ,\n  booktitle= International conference on machine learning ,\n  pages= 3145--3153 ,\n  year= 2017 ,\n  organization= PMlR \n \n"
    },
    {
        "id": 68,
        "reference": "S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting model predictions,” Advances in neural information processing systems, vol. 30, 2017.",
        "bibtex": {
            "type": "article",
            "citation_key": "lundberg2017unified",
            "title": "A unified approach to interpreting model predictions",
            "author": [
                "Lundberg, Scott"
            ],
            "journal": "arXiv preprint arXiv:1705.07874",
            "year": 2017
        },
        "citation_count": 26651,
        "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=S.+M.+Lundberg+and+S.-I.+Lee%2C+%E2%80%9CA+unified+approach+to+interpreting+model+predictions%2C%E2%80%9D+Advances+in+neural+information+processing+systems%2C+vol.+30%2C+2017.&btnG=",
        "external_link": "https://arxiv.org/abs/1705.07874",
        "doi_link": "https://doi.org/10.48550/arXiv.1705.07874",
        "group": "#c7e9c0",
        "groupName": "Decomposition Based Paper",
        "bibTexContent": " \n@article lundberg2017unified,\n  title= A unified approach to interpreting model predictions ,\n  author= Lundberg, Scott ,\n  journal= arXiv preprint arXiv:1705.07874 ,\n  year= 2017 \n \n"
    },
    {
        "id": 69,
        "reference": "S. M. Lundberg, G. Erion, H. Chen, A. DeGrave, J. M. Prutkin, B. Nair, R. Katz, J. Himmelfarb, N. Bansal, and S.-I. Lee, “Explainable AI for trees: From local explanations to global understanding,” arXiv preprint arXiv:1905.04610, 2019.",
        "bibtex": {
            "type": "article",
            "citation_key": "lundberg2019explainable",
            "title": "Explainable AI for trees: From local explanations to global understanding",
            "author": [
                "Lundberg, Scott M",
                "Erion, Gabriel",
                "Chen, Hugh",
                "DeGrave, Alex",
                "Prutkin, Jordan M",
                "Nair, Bala",
                "Katz, Ronit",
                "Himmelfarb, Jonathan",
                "Bansal, Nisha",
                "Lee, Su-In"
            ],
            "journal": "arXiv preprint arXiv:1905.04610",
            "year": 2019
        },
        "citation_count": 383,
        "abstract": "Tree-based machine learning models such as random forests, decision trees, and gradient boosted trees are the most popular non-linear predictive models used in practice today, yet comparatively little attention has been paid to explaining their predictions. Here we significantly improve the interpretability of tree-based models through three main contributions: 1) The first polynomial time algorithm to compute optimal explanations based on game theory. 2) A new type of explanation that directly measures local feature interaction effects. 3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the general US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=S.+M.+Lundberg%2C+G.+Erion%2C+H.+Chen%2C+A.+DeGrave%2C+J.+M.+Prutkin%2C+B.+Nair%2C+R.+Katz%2C+J.+Himmelfarb%2C+N.+Bansal%2C+and+S.-I.+Lee%2C+%E2%80%9CExplain-+able+ai+for+trees%3A+From+local+explanations+to+global+understanding%2C%E2%80%9D+arXiv+preprint+arXiv%3A1905.04610%2C+2019.&btnG=",
        "external_link": "https://arxiv.org/abs/1905.04610",
        "doi_link": "https://doi.org/10.48550/arXiv.1905.04610",
        "group": "#c7e9c0",
        "groupName": "Decomposition Based Paper",
        "bibTexContent": " \n@article lundberg2019explainable,\n  title= Explainable AI for trees: From local explanations to global understanding ,\n  author= Lundberg, Scott M and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In ,\n  journal= arXiv preprint arXiv:1905.04610 ,\n  year= 2019 \n \n"
    },
    {
        "id": 70,
        "reference": "X. Huang, S. Jamonnak, Y. Zhao, T. H. Wu, and W. Xu, “A visual designer of layer-wise relevance propagation models,” in Computer Graphics Forum, vol. 40, no. 3. Wiley Online Library, 2021, pp. 227–238.",
        "citation_count": 16,
        "abstract": "Layer-wise Relevance Propagation (LRP) is an emerging and widely-used method for interpreting the prediction results of convolutional neural networks (CNN). LRP developers often select and employ different relevance backpropagation rules and parameters to compute relevance scores on input images. However, there exists no obvious solution to define a “best” LRP model. A satisfied model is highly reliant on pertinent images and designers' goals. We develop a visual model designer, named as VisLRPDesigner, to overcome the challenges in the design and use of LRP models. Various LRP rules are unified into an integrated framework with an intuitive workflow of parameter setup. VisLRPDesigner thus allows users to interactively configure and compare LRP models. It also facilitates relevance-based visual analysis with two important functions: relevance-based pixel flipping and neuron ablation. Several use cases illustrate the benefits of VisLRPDesigner. The usability and limitation of the visual designer is evaluated by LRP users.",
        "google_scholar": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=X.+Huang%2C+S.+Jamonnak%2C+Y.+Zhao%2C+T.+H.+Wu%2C+and+W.+Xu%2C+%E2%80%9CA+visual+designer+of+layer-wise+relevance+propagation+models%2C%E2%80%9D+in+Computer+Graphics+Forum%2C+vol.+40%2C+no.+3.+Wiley+Online+Library%2C+2021%2C+pp.+227%E2%80%93238.&btnG=",
        "external_link": "https://doi.org/10.1111/cgf.14302",
        "bibtex": {
            "type": "inproceedings",
            "citation_key": "huang2021visual",
            "title": "A Visual Designer of Layer-wise Relevance Propagation Models",
            "author": [
                "Huang, Xinyi",
                "Jamonnak, Suphanut",
                "Zhao, Ye",
                "Wu, Tsung Heng",
                "Xu, Wei"
            ],
            "booktitle": "Computer Graphics Forum",
            "volume": 40,
            "number": 3,
            "pages": "227--238",
            "year": 2021,
            "organization": "Wiley Online Library"
        },
        "group": "#c7e9c0",
        "groupName": "Decomposition Based Paper",
        "bibTexContent": " \n@inproceedings huang2021visual,\n  title= A Visual Designer of Layer-wise Relevance Propagation Models ,\n  author= Huang, Xinyi and Jamonnak, Suphanut and Zhao, Ye and Wu, Tsung Heng and Xu, Wei ,\n  booktitle= Computer Graphics Forum ,\n  volume= 40 ,\n  number= 3 ,\n  pages= 227--238 ,\n  year= 2021 ,\n  organization= Wiley Online Library \n \n"
    },
    {
        "id": 71,
        "reference": "L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Kagal, “Explaining explanations: An overview of interpretability of machine learning,” in 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA). IEEE, 2018, pp. 80–89.",
        "citation_count": 2818,
        "abstract": "There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.",
        "ieee_keywords": [
            "Artificial intelligence",
            "Computational modeling",
            "Decision trees",
            "Biological neural networks",
            "Taxonomy",
            "Complexity theory"
        ],
        "google_scholar": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=L.+H.+Gilpin%2C+D.+Bau%2C+B.+Z.+Yuan%2C+A.+Bajwa%2C+M.+Specter%2C+and+L.+Ka-+gal%2C+%E2%80%9CExplaining+explanations%3A+An+overview+of+interpretability+of+machine+learning%2C%E2%80%9D+in+2018+IEEE+5th+International+Conference+on+data+science+and+advanced+analytics+%28DSAA%29.+IEEE%2C+2018%2C+pp.+80%E2%80%9389.&btnG=",
        "external_link": "https://ieeexplore.ieee.org/document/8631448",
        "bibtex": {
            "type": "inproceedings",
            "citation_key": "gilpin2018explaining",
            "title": "Explaining explanations: An overview of interpretability of machine learning",
            "author": [
                "Gilpin, Leilani H",
                "Bau, David",
                "Yuan, Ben Z",
                "Bajwa, Ayesha",
                "Specter, Michael",
                "Kagal, Lalana"
            ],
            "booktitle": "2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)",
            "pages": "80--89",
            "year": 2018,
            "organization": "IEEE"
        },
        "group": "#c7e9c0",
        "groupName": "Decomposition Based Paper",
        "bibTexContent": " \n@inproceedings gilpin2018explaining,\n  title= Explaining explanations: An overview of interpretability of machine learning ,\n  author= Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana ,\n  booktitle= 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA) ,\n  pages= 80--89 ,\n  year= 2018 ,\n  organization= IEEE \n \n"
    },
    {
        "id": 72,
        "reference": "A. Binder, G. Montavon, S. Lapuschkin, K.-R. M ¨uller, and W. Samek, “Layer-wise relevance propagation for neural networks with local renormalization layers,” in Artificial Neural Networks and Machine Learning–ICANN 2016: 25th International Conference on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016, Proceedings, Part II 25. Springer, 2016, pp. 63–71.",
        "citation_count": 547,
        "abstract": "Layer-wise relevance propagation is a framework which allows to decompose the prediction of a deep neural network computed over a sample, e.g. an image, down to relevance scores for the single input dimensions of the sample such as subpixels of an image. While this approach can be applied directly to generalized linear mappings, product type non-linearities are not covered. This paper proposes an approach to extend layer-wise relevance propagation to neural networks with local renormalization layers, which is a very common product-type non-linearity in convolutional neural networks. We evaluate the proposed method for local renormalization layers on the CIFAR-10, Imagenet and MIT Places datasets.",
        "keywords": [
            "Neural networks",
            "Image classification",
            "Interpretability"
        ],
        "google_scholar": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Binder%2C+G.+Montavon%2C+S.+Lapuschkin%2C+K.-R.+M+%C2%A8uller%2C+and+W.+Samek%2C+%E2%80%9CLayer-wise+relevance+propagation+for+neural+networks+with+local+renormalization+layers%2C%E2%80%9D+in+Artificial+Neural+Networks+and+Machine+Learning%E2%80%93ICANN+2016%3A+25th+International+Conference+on+Artificial+Neural+Networks%2C+Barcelona%2C+Spain%2C+September+6-9%2C+2016%2C+Proceedings%2C+Part+II+25.+Springer%2C+2016%2C+pp.+63%E2%80%9371.&btnG=",
        "external_link": "https://link.springer.com/chapter/10.1007/978-3-319-44781-0_8",
        "bibtex": {
            "type": "inproceedings",
            "citation_key": "binder2016layer",
            "title": "Layer-wise relevance propagation for neural networks with local renormalization layers",
            "author": [
                "Binder, Alexander",
                "Montavon, Grégoire",
                "Lapuschkin, Sebastian",
                "Müller, Klaus-Robert",
                "Samek, Wojciech"
            ],
            "booktitle": "Artificial Neural Networks and Machine Learning--ICANN 2016: 25th International Conference on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016, Proceedings, Part II 25",
            "pages": "63--71",
            "year": 2016,
            "organization": "Springer"
        },
        "group": "#c7e9c0",
        "groupName": "Decomposition Based Paper",
        "bibTexContent": " \n@inproceedings binder2016layer,\n  title= Layer-wise relevance propagation for neural networks with local renormalization layers ,\n  author= Binder, Alexander and Montavon, Gr 'e goire and Lapuschkin, Sebastian and M \"u ller, Klaus-Robert and Samek, Wojciech ,\n  booktitle= Artificial Neural Networks and Machine Learning--ICANN 2016: 25th International Conference on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016, Proceedings, Part II 25 ,\n  pages= 63--71 ,\n  year= 2016 ,\n  organization= Springer \n \n"
    },
    {
        "id": 73,
        "reference": "B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas et al., “Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav),” in International conference on machine learning. PMLR, 2018, pp. 2668–2677.",
        "bibtex": {
            "type": "inproceedings",
            "citation_key": "kim2018interpretability",
            "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
            "author": [
                "Kim, Been",
                "Wattenberg, Martin",
                "Gilmer, Justin",
                "Cai, Carrie",
                "Wexler, James",
                "Viegas, Fernanda"
            ],
            "booktitle": "International conference on machine learning",
            "pages": "2668--2677",
            "year": 2018,
            "organization": "PMLR"
        },
        "citation_count": 2030,
        "abstract": "The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result–for example, how sensitive a prediction of 'zebra' is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+B.+Kim%2C+M.+Wattenberg%2C+J.+Gilmer%2C+C.+Cai%2C+J.+Wexler%2C+F.+Viegas+et+al.%2C+%E2%80%9CInterpretability+beyond+feature+attribution%3A+Quantitative+testing+with+concept+activation+vectors+%28tcav%29%2C%E2%80%9D+in+International+conference+on+machine+learning.+PMLR%2C+2018%2C+pp.+2668%E2%80%932677.&btnG=",
        "externalLink": "https://proceedings.mlr.press/v80/kim18d.html",
        "group": "#f4cccc",
        "groupName": "Concept Based Paper",
        "bibTexContent": " \n@inproceedings kim2018interpretability,\n  title= Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav) ,\n  author= Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and others ,\n  booktitle= International conference on machine learning ,\n  pages= 2668--2677 ,\n  year= 2018 ,\n  organization= PMLR \n \n"
    },
    {
        "id": 74,
        "reference": "Y. Goyal, A. Feder, U. Shalit, and B. Kim, “Explaining classifiers with causal concept effect (cace),” arXiv preprint arXiv:1907.07165, 2019.",
        "bibtex": {
            "type": "article",
            "citation_key": "goyal2019explaining",
            "title": "Explaining classifiers with causal concept effect (cace)",
            "author": [
                "Goyal, Yash",
                "Feder, Amir",
                "Shalit, Uri",
                "Kim, Been"
            ],
            "journal": "arXiv preprint arXiv:1907.07165",
            "year": 2019
        },
        "citation_count": 178,
        "abstract": "How can we understand classification decisions made by deep neural networks? Many existing explainability methods rely solely on correlations and fail to account for confounding, which may result in potentially misleading explanations. To overcome this problem, we define the Causal Concept Effect (CaCE) as the causal effect of (the presence or absence of) a human-interpretable concept on a deep neural net's predictions. We show that the CaCE measure can avoid errors stemming from confounding. Estimating CaCE is difficult in situations where we cannot easily simulate the do-operator. To mitigate this problem, we use a generative model, specifically a Variational AutoEncoder (VAE), to measure VAE-CaCE. In an extensive experimental analysis, we show that the VAE-CaCE is able to estimate the true concept causal effect, compared to baselines for a number of datasets including high dimensional images.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=Y.+Goyal%2C+A.+Feder%2C+U.+Shalit%2C+and+B.+Kim%2C+%E2%80%9CExplaining+classifiers+with+causal+concept+effect+%28cace%29%2C%E2%80%9D+arXiv+preprint+arXiv%3A1907.07165%2C+2019.&btnG=",
        "externalLink": "https://arxiv.org/abs/1907.07165",
        "doiLink": "https://doi.org/10.48550/arXiv.1907.07165",
        "group": "#f4cccc",
        "groupName": "Concept Based Paper",
        "bibTexContent": " \n@article goyal2019explaining,\n  title= Explaining classifiers with causal concept effect (cace) ,\n  author= Goyal, Yash and Feder, Amir and Shalit, Uri and Kim, Been ,\n  journal= arXiv preprint arXiv:1907.07165 ,\n  year= 2019 \n \n"
    },
    {
        "id": 75,
        "reference": "C.-K. Yeh, B. Kim, S. Arik, C.-L. Li, T. Pfister, and P. Ravikumar, “On completeness-aware concept-based explanations in deep neural networks,” Advances in neural information processing systems, vol. 33, pp. 20 554–20 565, 2020.",
        "bibtex": {
            "type": "article",
            "citation_key": "yeh2020completeness",
            "title": "On completeness-aware concept-based explanations in deep neural networks",
            "author": [
                "Yeh, Chih-Kuan",
                "Kim, Been",
                "Arik, Sercan",
                "Li, Chun-Liang",
                "Pfister, Tomas",
                "Ravikumar, Pradeep"
            ],
            "journal": "Advances in neural information processing systems",
            "volume": 33,
            "pages": "20554--20565",
            "year": 2020
        },
        "citation_count": 311,
        "abstract": "Human explanations of high-level decisions are often expressed in terms of key concepts the decisions are based on. In this paper, we study such concept-based explainability for Deep Neural Networks (DNNs). First, we define the notion of completeness, which quantifies how sufficient a particular set of concepts is in explaining a model's prediction behavior based on the assumption that complete concept scores are sufficient statistics of the model prediction. Next, we propose a concept discovery method that aims to infer a complete set of concepts that are additionally encouraged to be interpretable, which addresses the limitations of existing methods on concept explanations. To define an importance score for each discovered concept, we adapt game-theoretic notions to aggregate over sets and propose ConceptSHAP. Via proposed metrics and user studies, on a synthetic dataset with a priori-known concept explanations, as well as on real-world image and language datasets, we validate the effectiveness of our method in finding concepts that are both complete in explaining the decisions and interpretable.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=C.-K.+Yeh%2C+B.+Kim%2C+S.+Arik%2C+C.-L.+Li%2C+T.+Pfister%2C+and+P.+Ravikumar%2C+%E2%80%9COn+completeness-aware+concept-based+explanations+in+deep+neu-ral+networks%2C%E2%80%9D+Advances+in+neural+information+processing+systems%2C+vol.+33%2C+pp.+20+554%E2%80%9320+565%2C+2020.&btnG=",
        "externalLink": "https://proceedings.neurips.cc/paper/2020/hash/ecb287ff763c169694f682af52c1f309-Abstract.html",
        "group": "#f4cccc",
        "groupName": "Concept Based Paper",
        "bibTexContent": " \n@article yeh2020completeness,\n  title= On completeness-aware concept-based explanations in deep neural networks ,\n  author= Yeh, Chih-Kuan and Kim, Been and Arik, Sercan and Li, Chun-Liang and Pfister, Tomas and Ravikumar, Pradeep ,\n  journal= Advances in neural information processing systems ,\n  volume= 33 ,\n  pages= 20554--20565 ,\n  year= 2020 \n \n"
    },
    {
        "id": 76,
        "reference": "N. Kokhlikyan, V. Miglani, M. Martin, E. Wang, B. Alsallakh, J. Reynolds, A. Melnikov, N. Kliushkina, C. Araya, S. Yan et al., “Captum: A unified and generic model interpretability library for pytorch,” arXiv preprint arXiv:2009.07896, 2020.",
        "citation_count": 798,
        "abstract": "In this paper we introduce a novel, unified, open-source model interpretability library for PyTorch. The library contains generic implementations of a number of gradient and perturbation-based attribution algorithms, also known as feature, neuron and layer importance algorithms, as well as a set of evaluation metrics for these algorithms. It can be used for both classification and non-classification models including graph-structured models built on Neural Networks (NN). In this paper we give a high-level overview of supported attribution algorithms and show how to perform memory-efficient and scalable computations. We emphasize that the three main characteristics of the library are multimodality, extensibility and ease of use. Multimodality supports different modality of inputs such as image, text, audio or video. Extensibility allows adding new algorithms and features. The library is also designed for easy understanding and use. Besides, we also introduce an interactive visualization tool called Captum Insights that is built on top of Captum library and allows sample-based model debugging and visualization using feature importance metrics.",
        "keywords": [
            "Interpretability",
            "Attribution",
            "Multi-Modal",
            "Model Understanding"
        ],
        "google_scholar": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=N.+Kokhlikyan%2C+V.+Miglani%2C+M.+Martin%2C+E.+Wang%2C+B.+Alsallakh%2C+J.+Reynolds%2C+A.+Melnikov%2C+N.+Kliushkina%2C+C.+Araya%2C+S.+Yan+et+al.%2C+%E2%80%9CCaptum%3A+A+unified+and+generic+model+interpretability+library+for+pytorch%2C%E2%80%9D+arXiv+preprint+arXiv%3A2009.07896%2C+2020.&btnG=",
        "external_link": "https://arxiv.org/abs/2009.07896",
        "doi_link": "https://doi.org/10.48550/arXiv.2009.07896",
        "bibtex": {
            "type": "article",
            "citation_key": "kokhlikyan2020captum",
            "title": "Captum: A unified and generic model interpretability library for pytorch",
            "author": [
                "Kokhlikyan, Narine",
                "Miglani, Vivek",
                "Martin, Miguel",
                "Wang, Edward",
                "Alsallakh, Bilal",
                "Reynolds, Jonathan",
                "Melnikov, Alexander",
                "Kliushkina, Natalia",
                "Araya, Carlos",
                "Yan, Siqi",
                "others"
            ],
            "journal": "arXiv preprint arXiv:2009.07896",
            "year": 2020
        },
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": " \n@article kokhlikyan2020captum,\n  title= Captum: A unified and generic model interpretability library for pytorch ,\n  author= Kokhlikyan, Narine and Miglani, Vivek and Martin, Miguel and Wang, Edward and Alsallakh, Bilal and Reynolds, Jonathan and Melnikov, Alexander and Kliushkina, Natalia and Araya, Carlos and Yan, Siqi and others ,\n  journal= arXiv preprint arXiv:2009.07896 ,\n  year= 2020 \n \n"
    },
    {
        "id": 77,
        "reference": "A. Hedstr\"om, L. Weber, D. Krakowczyk, D. Bareeva, F. Motzkus, W. Samek, S. Lapuschkin, and M. M.-C. H\"ohne, “Quantus: An explainable ai toolkit for responsible evaluation of neural network explanations and beyond,” Journal of Machine Learning Research, vol. 24, no. 34, pp. 1–11, 2023.",
        "citation_count": 0,
        "abstract": "The evaluation of explanation methods is a research topic that has not yet been explored deeply, however, since explainability is supposed to strengthen trust in artificial intelligence, it is necessary to systematically review and compare explanation methods in order to confirm their correctness. Until now, no tool with focus on XAI evaluation exists that exhaustively and speedily allows researchers to evaluate the performance of explanations of neural network predictions. To increase transparency and reproducibility in the field, we therefore built Quantus--a comprehensive, evaluation toolkit in Python that includes a growing, well-organised collection of evaluation metrics and tutorials for evaluating explainable methods. The toolkit has been thoroughly tested and is available under an open-source license on PyPi (or on https://github.com/understandable-machine-intelligence-lab/Quantus/).",
        "google_scholar": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=A.+Hedstr+%C2%A8om%2C+L.+Weber%2C+D.+Krakowczyk%2C+D.+Bareeva%2C+F.+Motzkus%2C+W.+Samek%2C+S.+Lapuschkin%2C+and+M.+M.-C.+H+%C2%A8ohne%2C+%E2%80%9CQuantus%3A+An+explainable+ai+toolkit+for+responsible+evaluation+of+neural+network+explanations+and+beyond%2C%E2%80%9D+Journal+of+Machine+Learning+Research%2C+vol.+24%2C+no.+34%2C+pp.+1%E2%80%9311%2C+2023.&btnG=",
        "external_link": "https://www.jmlr.org/papers/v24/22-0142.html",
        "bibtex": {
            "type": "article",
            "citation_key": "hedstrom2023quantus",
            "title": "Quantus: An explainable ai toolkit for responsible evaluation of neural network explanations and beyond",
            "author": [
                "Hedström, Anna",
                "Weber, Leander",
                "Krakowczyk, Daniel",
                "Bareeva, Dilyara",
                "Motzkus, Franz",
                "Samek, Wojciech",
                "Lapuschkin, Sebastian",
                "Höhne, Marina M.-C"
            ],
            "journal": "Journal of Machine Learning Research",
            "volume": 24,
            "number": 34,
            "pages": "1--11",
            "year": 2023
        },
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": "\n@article hedstrom2023quantus,\n  title= Quantus: An explainable ai toolkit for responsible evaluation of neural network explanations and beyond ,\n  author= Hedstr \"o m, Anna and Weber, Leander and Krakowczyk, Daniel and Bareeva, Dilyara and Motzkus, Franz and Samek, Wojciech and Lapuschkin, Sebastian and H \"o hne, Marina M-C ,\n  journal= Journal of Machine Learning Research ,\n  volume= 24 ,\n  number= 34 ,\n  pages= 1--11 ,\n  year= 2023 \n \n"
    },
    {
        "id": 78,
        "reference": "C.-K. Yeh, C.-Y. Hsieh, A. Suggala, D. I. Inouye, and P. K. Ravikumar, “On the (in) fidelity and sensitivity of explanations,” Advances in neural information processing systems, vol. 32, 2019.",
        "citation_count": 0,
        "abstract": "We consider objective evaluation measures of saliency explanations for complex black-box machine learning models. We propose simple robust variants of two notions that have been considered in recent literature:(in) fidelity, and sensitivity. We analyze optimal explanations with respect to both these measures, and while the optimal explanation for sensitivity is a vacuous constant explanation, the optimal explanation for infidelity is a novel combination of two popular explanation methods. By varying the perturbation distribution that defines infidelity, we obtain novel explanations by optimizing infidelity, which we show to out-perform existing explanations in both quantitative and qualitative measurements. Another salient question given these measures is how to modify any given explanation to have better values with respect to these measures. We propose a simple modification based on lowering sensitivity, and moreover show that when done appropriately, we could simultaneously improve both sensitivity as well as fidelity.",
        "google_scholar": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=C.-K.+Yeh%2C+C.-Y.+Hsieh%2C+A.+Suggala%2C+D.+I.+Inouye%2C+and+P.+K.+Raviku-+mar%2C+%E2%80%9COn+the+%28in%29+fidelity+and+sensitivity+of+explanations%2C%E2%80%9D+Advances+in+neural+information+processing+systems%2C+vol.+32%2C+2019.&btnG=",
        "external_link": "https://proceedings.neurips.cc/paper/2019/hash/a7471fdc77b3435276507cc8f2dc2569-Abstract.html",
        "bibtex": {
            "type": "article",
            "citation_key": "yeh2019fidelity",
            "title": "On the (in) fidelity and sensitivity of explanations",
            "author": [
                "Yeh, Chih-Kuan",
                "Hsieh, Cheng-Yu",
                "Suggala, Arun",
                "Inouye, David I",
                "Ravikumar, Pradeep K"
            ],
            "journal": "Advances in neural information processing systems",
            "volume": 32,
            "year": 2019
        },
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": "\n@article yeh2019fidelity,\n  title= On the (in) fidelity and sensitivity of explanations ,\n  author= Yeh, Chih-Kuan and Hsieh, Cheng-Yu and Suggala, Arun and Inouye, David I and Ravikumar, Pradeep K ,\n  journal= Advances in neural information processing systems ,\n  volume= 32 ,\n  year= 2019 \n \n"
    },
    {
        "id": 79,
        "reference": "P. Chalasani, J. Chen, A. R. Chowdhury, X. Wu, and S. Jha, “Concise explanations of neural networks using adversarial training,” in International Conference on Machine Learning. PMLR, 2020, pp. 1383–1391.",
        "bibtex": {
            "type": "inproceedings",
            "citation_key": "chalasani2020concise",
            "title": "Concise explanations of neural networks using adversarial training",
            "author": [
                "Chalasani, Prasad",
                "Chen, Jiefeng",
                "Chowdhury, Amrita Roy",
                "Wu, Xi",
                "Jha, Somesh"
            ],
            "booktitle": "International Conference on Machine Learning",
            "pages": "1383--1391",
            "year": 2020,
            "organization": "PMLR"
        },
        "citation_count": 0,
        "abstract": "We show new connections between adversarial learning and explainability for deep neural networks (DNNs). One form of explanation of the output of a neural network model in terms of its input features, is a vector of feature-attributions, which can be generated by various techniques such as Integrated Gradients (IG), DeepSHAP, LIME, and CXPlain. Two desirable characteristics of an attribution-based explanation are: (1) sparseness: the attributions of irrelevant or weakly relevant features should be negligible, thus resulting in concise explanations in terms of the significant features, and (2) stability: it should not vary significantly within a small local neighborhood of the input. Our first contribution is a theoretical exploration of how these two properties (when using IG-based attributions) are related to adversarial training, for a class of 1-layer networks (which includes logistic regression models for binary and multi-class classification); for these networks we show that (a) adversarial training using an ε-bounded adversary produces models with sparse attribution vectors, and (b) natural model-training while encouraging stable explanations (via an extra term in the loss function), is equivalent to adversarial training. Our second contribution is an empirical verification of phenomenon (a), which we show, somewhat surprisingly, occurs not only in 1-layer networks, but also DNNs trained on standard image datasets, and extends beyond IG-based attributions, to those based on DeepSHAP: adversarial training with ℓ∞-bounded perturbations yields significantly sparser attribution vectors, with little degradation in performance on natural test data, compared to natural training. Moreover, the sparseness of the attribution vectors is significantly better than that achievable via ε-regularized natural training.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=P.+Chalasani%2C+J.+Chen%2C+A.+R.+Chowdhury%2C+X.+Wu%2C+and+S.+Jha%2C+%E2%80%9CCon-cise+explanations+of+neural+networks+using+adversarial+training%2C%E2%80%9D+in+International+Conference+on+Machine+Learning.+PMLR%2C+2020%2C+pp.+1383%E2%80%931391.&btnG=",
        "externalLink": "https://proceedings.mlr.press/v119/chalasani20a.html",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": "\n@inproceedings chalasani2020concise,\n  title= Concise explanations of neural networks using adversarial training ,\n  author= Chalasani, Prasad and Chen, Jiefeng and Chowdhury, Amrita Roy and Wu, Xi and Jha, Somesh ,\n  booktitle= International Conference on Machine Learning ,\n  pages= 1383--1391 ,\n  year= 2020 ,\n  organization= PMLR \n \n"
    },
    {
        "id": 80,
        "reference": "J. Zhang, S. A. Bargal, Z. Lin, J. Brandt, X. Shen, and S. Sclaroff, “Top-down neural attention by excitation backprop,” International Journal of Computer Vision, vol. 126, no. 10, pp. 1084–1102, 2018.",
        "bibtex": {
            "type": "article",
            "citation_key": "zhang2018top",
            "title": "Top-down neural attention by excitation backprop",
            "author": [
                "Zhang, Jianming",
                "Bargal, Sarah Adel",
                "Lin, Zhe",
                "Brandt, Jonathan",
                "Shen, Xiaohui",
                "Sclaroff, Stan"
            ],
            "journal": "International Journal of Computer Vision",
            "volume": 126,
            "number": 10,
            "pages": "1084--1102",
            "year": 2018,
            "publisher": "Springer"
        },
        "citation_count": 0,
        "abstract": "We aim to model the top-down attention of a convolutional neural network (CNN) classifier for generating task-specific attention maps. Inspired by a top-down human visual attention model, we propose a new backpropagation scheme, called Excitation Backprop, to pass along top-down signals downwards in the network hierarchy via a probabilistic Winner-Take-All process. Furthermore, we introduce the concept of contrastive attention to make the top-down attention maps more discriminative. We show a theoretic connection between the proposed contrastive attention formulation and the Class Activation Map computation. Efficient implementation of Excitation Backprop for common neural network layers is also presented. In experiments, we visualize the evidence of a model’s classification decision by computing the proposed top-down attention maps. For quantitative evaluation, we report the accuracy of our method in weakly supervised localization tasks on the MS COCO, PASCAL VOC07 and ImageNet datasets. The usefulness of our method is further validated in the text-to-region association task. On the Flickr30k Entities dataset, we achieve promising performance in phrase localization by leveraging the top-down attention of a CNN model that has been trained on weakly labeled web images. Finally, we demonstrate applications of our method in model interpretation and data annotation assistance for facial expression analysis and medical imaging tasks.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=J.+Zhang%2C+S.+A.+Bargal%2C+Z.+Lin%2C+J.+Brandt%2C+X.+Shen%2C+and+S.+Sclaroff%2C+%E2%80%9CTop-down+neural+attention+by+excitation+backprop%2C%E2%80%9D+International+Journal+of+Computer+Vision%2C+vol.+126%2C+no.+10%2C+pp.+1084%E2%80%931102%2C+2018.&btnG=",
        "externalLink": "https://link.springer.com/article/10.1007/s11263-017-1059-x",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": "\n@article zhang2018top,\n  title= Top-down neural attention by excitation backprop ,\n  author= Zhang, Jianming and Bargal, Sarah Adel and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan ,\n  journal= International Journal of Computer Vision ,\n  volume= 126 ,\n  number= 10 ,\n  pages= 1084--1102 ,\n  year= 2018 ,\n  publisher= Springer \n \n"
    },
    {
        "id": 81,
        "reference": "L. Sixt, M. Granz, and T. Landgraf, “When explanations lie: Why many modified bp attributions fail,” in International conference on machine learning. PMLR, 2020, pp. 9046–9057.",
        "bibtex": {
            "type": "inproceedings",
            "citation_key": "sixt2020explanations",
            "title": "When explanations lie: Why many modified bp attributions fail",
            "author": [
                "Sixt, Leon",
                "Granz, Maximilian",
                "Landgraf, Tim"
            ],
            "booktitle": "International conference on machine learning",
            "pages": "9046--9057",
            "year": 2020,
            "organization": "PMLR"
        },
        "citation_count": 0,
        "abstract": "Attribution methods aim to explain a neural network’s prediction by highlighting the most relevant image areas. A popular approach is to backpropagate (BP) a custom relevance score using modified rules, rather than the gradient. We analyze an extensive set of modified BP methods: Deep Taylor Decomposition, Layer-wise Relevance Propagation (LRP), Excitation BP, PatternAttribution, DeepLIFT, Deconv, RectGrad, and Guided BP. We find empirically that the explanations of all mentioned methods, except for DeepLIFT, are independent of the parameters of later layers. We provide theoretical insights for this surprising behavior and also analyze why DeepLIFT does not suffer from this limitation. Empirically, we measure how information of later layers is ignored by using our new metric, cosine similarity convergence (CSC). The paper provides a framework to assess the faithfulness of new and existing modified BP methods theoretically and empirically.",
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=L.+Sixt%2C+M.+Granz%2C+and+T.+Landgraf%2C+%E2%80%9CWhen+explanations+lie%3A+Why+many+modified+bp+attributions+fail%2C%E2%80%9D+in+International+conference+on+machine+learning.+PMLR%2C+2020%2C+pp.+9046%E2%80%939057.&btnG=",
        "externalLink": "https://proceedings.mlr.press/v119/sixt20a.html",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": "\n@inproceedings sixt2020explanations,\n  title= When explanations lie: Why many modified bp attributions fail ,\n  author= Sixt, Leon and Granz, Maximilian and Landgraf, Tim ,\n  booktitle= International conference on machine learning ,\n  pages= 9046--9057 ,\n  year= 2020 ,\n  organization= PMLR \n \n"
    },
    {
        "id": 82,
        "reference": "V. Dhore, A. Bhat, V. Nerlekar, K. Chavhan, and A. Umare, “Enhancing explainable ai: A hybrid approach combining gradcam and lrp for cnn interpretability,” arXiv preprint arXiv:2405.12175, 2024.",
        "bibtex": {
            "type": "article",
            "citation_key": "dhore2024enhancing",
            "title": "Enhancing Explainable AI: A Hybrid Approach Combining GradCAM and LRP for CNN Interpretability",
            "author": [
                "Dhore, Vaibhav",
                "Bhat, Achintya",
                "Nerlekar, Viraj",
                "Chavhan, Kashyap",
                "Umare, Aniket"
            ],
            "journal": "arXiv preprint arXiv:2405.12175",
            "year": 2024
        },
        "citation_count": 0,
        "abstract": "We present a new technique that explains the output of a CNN-based model using a combination of GradCAM and LRP methods. Both of these methods produce visual explanations by highlighting input regions that are important for predictions. In the new method, the explanation produced by GradCAM is first processed to remove noises. The processed output is then multiplied elementwise with the output of LRP. Finally, a Gaussian blur is applied on the product. We compared the proposed method with GradCAM and LRP on the metrics of Faithfulness, Robustness, Complexity, Localisation and Randomisation. It was observed that this method performs better on Complexity than both GradCAM and LRP and is better than at least one of them in the other metrics.",
        "google_scholar": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=V.+Dhore%2C+A.+Bhat%2C+V.+Nerlekar%2C+K.+Chavhan%2C+and+A.+Umare%2C+%E2%80%9CEnhancing+explainable+ai%3A+A+hybrid+approach+combining+gradcam+and+lrp+for+cnn+interpretability%2C%E2%80%9D+arXiv+preprint+arXiv%3A2405.12175%2C+2024.&btnG=",
        "external_link": "https://arxiv.org/abs/2405.12175",
        "doi_link": "https://doi.org/10.48550/arXiv.2405.12175",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": "\n@article dhore2024enhancing,\n  title= Enhancing Explainable AI: A Hybrid Approach Combining GradCAM and LRP for CNN Interpretability ,\n  author= Dhore, Vaibhav and Bhat, Achintya and Nerlekar, Viraj and Chavhan, Kashyap and Umare, Aniket ,\n  journal= arXiv preprint arXiv:2405.12175 ,\n  year= 2024 \n \n"
    },
    {
        "id": 83,
        "reference": "J. Wang, S. Liu, and W. Zhang, “Visual analytics for machine learning: A data perspective survey,” IEEE transactions on visualization and computer graphics, 2024.",
        "bibtex": {
            "type": "article",
            "citation_key": "wang2024visual",
            "title": "Visual analytics for machine learning: A data perspective survey",
            "author": [
                "Wang, Junpeng",
                "Liu, Shixia",
                "Zhang, Wei"
            ],
            "journal": "IEEE Transactions on Visualization and Computer Graphics",
            "year": 2024,
            "publisher": "IEEE"
        },
        "citation_count": 0,
        "abstract": "The past decade has witnessed a plethora of works that leverage the power of visualization (VIS) to interpret machine learning (ML) models. The corresponding research topic, VIS4ML, keeps growing at a fast pace. To better organize the enormous works and shed light on the developing trend of VIS4ML, we provide a systematic review of these works through this survey. Since data quality greatly impacts the performance of ML models, our survey focuses specifically on summarizing VIS4ML works from the data perspective. First, we categorize the common data handled by ML models into five types, explain the unique features of each type, and highlight the corresponding ML models that are good at learning from them. Second, from the large number of VIS4ML works, we tease out six tasks that operate on these types of data (i.e., data-centric tasks) at different stages of the ML pipeline to understand, diagnose, and refine ML models. Lastly, by studying the distribution of 143 surveyed papers across the five data types, six data-centric tasks, and their intersections, we analyze the prospective research directions and envision future research trends.",
        "google_scholar": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=J.+Wang%2C+S.+Liu%2C+and+W.+Zhang%2C+%E2%80%9CVisual+analytics+for+machine+learning%3A+A+data+perspective+survey%2C%E2%80%9D+IEEE+transactions+on+visualization+and+computer+graphics%2C+2024.&btnG=",
        "external_link": "https://ieeexplore.ieee.org/document/10412199",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": "\n@article wang2024visual,\n  title= Visual analytics for machine learning: A data perspective survey ,\n  author= Wang, Junpeng and Liu, Shixia and Zhang, Wei ,\n  journal= IEEE Transactions on Visualization and Computer Graphics ,\n  year= 2024 ,\n  publisher= IEEE \n \n"
    },
    {
        "id": 84,
        "reference": "Y. Li, J. Wang, T. Fujiwara, and K.-L. Ma, “Visual analytics of neuron vulnerability to adversarial attacks on convolutional neural networks,” ACM Transactions on Interactive Intelligent Systems, vol. 13, no. 4, pp. 1–26, 2023.",
        "bibtex": {
            "type": "article",
            "citation_key": "li2023visual",
            "title": "Visual analytics of neuron vulnerability to adversarial attacks on convolutional neural networks",
            "author": [
                "Li, Yiran",
                "Wang, Junpeng",
                "Fujiwara, Takanori",
                "Ma, Kwan-Liu"
            ],
            "journal": "ACM Transactions on Interactive Intelligent Systems",
            "volume": 13,
            "number": 4,
            "pages": "1--26",
            "year": 2023,
            "publisher": "ACM New York, NY"
        },
        "citation_count": 7,
        "abstract": "Adversarial attacks on a convolutional neural network (CNN)—injecting human-imperceptible perturbations into an input image—could fool a high-performance CNN into making incorrect predictions. The success of adversarial attacks raises serious concerns about the robustness of CNNs, and prevents them from being used in safety-critical applications, such as medical diagnosis and autonomous driving. Our work introduces a visual analytics approach to understanding adversarial attacks by answering two questions: (1) Which neurons are more vulnerable to attacks? and (2) Which image features do these vulnerable neurons capture during the prediction? For the first question, we introduce multiple perturbation-based measures to break down the attacking magnitude into individual CNN neurons and rank the neurons by their vulnerability levels. For the second, we identify image features (e.g., cat ears) that highly stimulate a user-selected neuron to augment and validate the neuron’s responsibility. Furthermore, we support an interactive exploration of a large number of neurons by aiding with hierarchical clustering based on the neurons’ roles in the prediction. To this end, a visual analytics system is designed to incorporate visual reasoning for interpreting adversarial attacks. We validate the effectiveness of our system through multiple case studies as well as feedback from domain experts.",
        "keywords": [
            "Convolutional neural networks",
            "adversarial attack",
            "explainable machine learning"
        ],
        "google_scholar": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=Y.+Li%2C+J.+Wang%2C+T.+Fujiwara%2C+and+K.-L.+Ma%2C+%E2%80%9CVisual+analytics+of+neuron+vulnerability+to+adversarial+attacks+on+convolutional+neural+networks%2C%E2%80%9D+ACM+Transactions+on+Interactive+Intelligent+Systems%2C+vol.+13%2C+no.+4%2C+pp.+1%E2%80%9326%2C+2023.&btnG=",
        "external_link": "https://dl.acm.org/doi/full/10.1145/3587470",
        "doi_link": "https://doi.org/10.1145/3587470",
        "group": "#ffe5cc",
        "groupName": "Perturbation Based Paper",
        "bibTexContent": "\n@article li2023visual,\n  title= Visual analytics of neuron vulnerability to adversarial attacks on convolutional neural networks ,\n  author= Li, Yiran and Wang, Junpeng and Fujiwara, Takanori and Ma, Kwan-Liu ,\n  journal= ACM Transactions on Interactive Intelligent Systems ,\n  volume= 13 ,\n  number= 4 ,\n  pages= 1--26 ,\n  year= 2023 ,\n  publisher= ACM New York, NY \n \n"
    },
    {
        "id": 85,
        "reference": "D. Collaris and J. J. van Wijk, “Strategyatlas: Strategy analysis for machine learning interpretability,” IEEE Transactions on Visualization and Computer Graphics, vol. 29, no. 6, pp. 2996–3008, 2022.",
        "bibtex": {
            "type": "article",
            "citation_key": "collaris2022strategyatlas",
            "title": "Strategyatlas: Strategy analysis for machine learning interpretability",
            "author": [
                "Collaris, Dennis",
                "van Wijk, Jarke J"
            ],
            "journal": "IEEE Transactions on Visualization and Computer Graphics",
            "volume": 29,
            "number": 6,
            "pages": "2996--3008",
            "year": 2022,
            "publisher": "IEEE"
        },
        "citation_count": 15,
        "abstract": "Businesses in high-risk environments have been reluctant to adopt modern machine learning approaches due to their complex and uninterpretable nature. Most current solutions provide local, instance-level explanations, but this is insufficient for understanding the model as a whole. In this work, we show that strategy clusters (i.e., groups of data instances that are treated distinctly by the model) can be used to understand the global behavior of a complex ML model. To support effective exploration and understanding of these clusters, we introduce StrategyAtlas, a system designed to analyze and explain model strategies. Furthermore, it supports multiple ways to utilize these strategies for simplifying and improving the reference model. In collaboration with a large insurance company, we present a use case in automatic insurance acceptance, and show how professional data scientists were enabled to understand a complex model and improve the production model based on these insights.",
        "ieeeKeywords": [
            "Data models",
            "Analytical models",
            "Machine learning",
            "Predictive models",
            "Computational modeling",
            "Insurance",
            "Data visualization"
        ],
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=D.+Collaris+and+J.+J.+van+Wijk%2C+%E2%80%9CStrategyatlas%3A+Strategy+analysis+for+machine+learning+interpretability%2C%E2%80%9D+IEEE+Transactions+on+Visualiza-+tion+and+Computer+Graphics%2C+vol.+29%2C+no.+6%2C+pp.+2996%E2%80%933008%2C+2022.&btnG=",
        "externalLink": "https://ieeexplore.ieee.org/document/9695246",
        "group": "#ffe5cc",
        "groupName": "Perturbation Based Paper",
        "bibTexContent": " \n@article collaris2022strategyatlas,\n  title= Strategyatlas: Strategy analysis for machine learning interpretability ,\n  author= Collaris, Dennis and van Wijk, Jarke J ,\n  journal= IEEE Transactions on Visualization and Computer Graphics ,\n  volume= 29 ,\n  number= 6 ,\n  pages= 2996--3008 ,\n  year= 2022 ,\n  publisher= IEEE \n \n"
    },
    {
        "id": 86,
        "reference": "F. Hohman, H. Park, C. Robinson, and D. H. P. Chau, “Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations,” IEEE Transactions on Visualization and Computer Graphics, vol. 26, no. 1, pp. 1096–1106, 2019.",
        "bibtex": {
            "type": "article",
            "citation_key": "hohman2019s",
            "title": "Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations",
            "author": [
                "Hohman, Fred",
                "Park, Haekyu",
                "Robinson, Caleb",
                "Chau, Duen Horng Polo"
            ],
            "journal": "IEEE Transactions on Visualization and Computer Graphics",
            "volume": 26,
            "number": 1,
            "pages": "1096--1106",
            "year": 2019,
            "publisher": "IEEE"
        },
        "citation_count": 256,
        "abstract": "Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier's learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.",
        "ieeeKeywords": [
            "Neurons",
            "Biological neural networks",
            "Feature extraction",
            "Data visualization",
            "Computational modeling",
            "Predictive models",
            "Visualization"
        ],
        "googleScholarLink": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=F.+Hohman%2C+H.+Park%2C+C.+Robinson%2C+and+D.+H.+P.+Chau%2C+%E2%80%9CS+ummit%3A+Scaling+deep+learning+interpretability+by+visualizing+activation+and+attribution+summarizations%2C%E2%80%9D+IEEE+transactions+on+visualization+and+computer+graphics%2C+vol.+26%2C+no.+1%2C+pp.+1096%E2%80%931106%2C+2019.&btnG=",
        "externalLink": "https://ieeexplore.ieee.org/document/8807294",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": " \n@article hohman2019s,\n  title= S ummit: Scaling deep learning interpretability by visualizing activation and attribution summarizations ,\n  author= Hohman, Fred and Park, Haekyu and Robinson, Caleb and Chau, Duen Horng Polo ,\n  journal= IEEE transactions on visualization and computer graphics ,\n  volume= 26 ,\n  number= 1 ,\n  pages= 1096--1106 ,\n  year= 2019 ,\n  publisher= IEEE \n \n"
    },
    {
        "id": 87,
        "reference": "Y. Ouyang, Y. Wu, H. Wang, C. Zhang, F. Cheng, C. Jiang, L. Jin, Y. Cao, and Q. Li, “Leveraging historical medical records as a proxy via multimodal modeling and visualization to enrich medical diagnostic learning,” IEEE Transactions on Visualization and Computer Graphics, 2023.",
        "bibtex": {
            "type": "article",
            "citation_key": "ouyang2023leveraging",
            "title": "Leveraging historical medical records as a proxy via multimodal modeling and visualization to enrich medical diagnostic learning",
            "author": [
                "Ouyang, Yang",
                "Wu, Yuchen",
                "Wang, He",
                "Zhang, Chenyang",
                "Cheng, Furui",
                "Jiang, Chang",
                "Jin, Lixia",
                "Cao, Yuanwu",
                "Li, Quan"
            ],
            "journal": "IEEE Transactions on Visualization and Computer Graphics",
            "year": 2023,
            "publisher": "IEEE"
        },
        "citation_count": 8,
        "abstract": "Simulation-based Medical Education (SBME) has been developed as a cost-effective means of enhancing the diagnostic skills of novice physicians and interns, thereby mitigating the need for resource-intensive mentor-apprentice training. However, feedback provided in most SBME is often directed towards improving the operational proficiency of learners, rather than providing summative medical diagnoses that result from experience and time. Additionally, the multimodal nature of medical data during diagnosis poses significant challenges for interns and novice physicians, including the tendency to overlook or over-rely on data from certain modalities, and difficulties in comprehending potential associations between modalities. To address these challenges, we present DiagnosisAssistant, a visual analytics system that leverages historical medical records as a proxy for multimodal modeling and visualization to enhance the learning experience of interns and novice physicians. The system employs elaborately designed visualizations to explore different modality data, offer diagnostic interpretive hints based on the constructed model, and enable comparative analyses of specific patients. Our approach is validated through two case studies and expert interviews, demonstrating its effectiveness in enhancing medical training.",
        "keywords": [
            "Medical diagnostic imaging",
            "Solid modeling",
            "Computational modeling",
            "Medical services",
            "Data visualization",
            "Data models",
            "Training"
        ],
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=Y.+Ouyang%2C+Y.+Wu%2C+H.+Wang%2C+C.+Zhang%2C+F.+Cheng%2C+C.+Jiang%2C+L.+Jin%2C+Y.+Cao%2C+and+Q.+Li%2C+%E2%80%9CLeveraging+historical+medical+records+as+a+proxy+via+multimodal+modeling+and+visualization+to+enrich+medical+diagnostic+learning%2C%E2%80%9D+IEEE+Transactions+on+Visualization+and+Computer+Graphics%2C+2023.&btnG=",
        "external_link": "https://ieeexplore.ieee.org/document/10295394",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": " \n@article ouyang2023leveraging,\n  title= Leveraging historical medical records as a proxy via multimodal modeling and visualization to enrich medical diagnostic learning ,\n  author= Ouyang, Yang and Wu, Yuchen and Wang, He and Zhang, Chenyang and Cheng, Furui and Jiang, Chang and Jin, Lixia and Cao, Yuanwu and Li, Quan ,\n  journal= IEEE Transactions on Visualization and Computer Graphics ,\n  year= 2023 ,\n  publisher= IEEE \n \n"
    },
    {
        "id": 88,
        "reference": "R. Fong, M. Patrick, and A. Vedaldi, “Understanding deep networks via extremal perturbations and smooth masks,” in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 2950–2958.",
        "bibtex": {
            "type": "inproceedings",
            "citation_key": "fong2019understanding",
            "title": "Understanding deep networks via extremal perturbations and smooth masks",
            "author": [
                "Fong, Ruth",
                "Patrick, Mandela",
                "Vedaldi, Andrea"
            ],
            "booktitle": "Proceedings of the IEEE/CVF international conference on computer vision",
            "pages": "2950--2958",
            "year": 2019
        },
        "citation_count": 469,
        "abstract": "Attribution is the problem of finding which parts of an image are the most responsible for the output of a deep neural network. An important family of attribution methods is based on measuring the effect of perturbations applied to the input image, either via exhaustive search or by finding representative perturbations via optimization. In this paper, we discuss some of the shortcomings of existing approaches to perturbation analysis and address them by introducing the concept of extremal perturbations, which are theoretically grounded and interpretable. We also introduce a number of technical innovations to compute these extremal perturbations, including a new area constraint and a parametric family of smooth perturbations, which allow us to remove all tunable weighing factors from the optimization problem. We analyze the effect of perturbations as a function of their area, demonstrating excellent sensitivity to the spatial properties of the network under stimulation. We also extend perturbation analysis to the intermediate layers of a deep neural network. This application allows us to show how compactly an image can be represented (in terms of the number of channels it requires). We also demonstrate that the consistency with which images of a given class rely on the same intermediate channel correlates well with class accuracy.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=R.+Fong%2C+M.+Patrick%2C+and+A.+Vedaldi%2C+%E2%80%9CUnderstanding+deep+net-+works+via+extremal+perturbations+and+smooth+masks%2C%E2%80%9D+in+Proceed-+ings+of+the+IEEE%2FCVF+international+conference+on+computer+vision%2C+2019%2C+pp.+2950%E2%80%932958.&btnG=",
        "external_link": "https://openaccess.thecvf.com/content_ICCV_2019/html/Fong_Understanding_Deep_Networks_via_Extremal_Perturbations_and_Smooth_Masks_ICCV_2019_paper.html",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": " \n@inproceedings fong2019understanding,\n  title= Understanding deep networks via extremal perturbations and smooth masks ,\n  author= Fong, Ruth and Patrick, Mandela and Vedaldi, Andrea ,\n  booktitle= Proceedings of the IEEE/CVF international conference on computer vision ,\n  pages= 2950--2958 ,\n  year= 2019 \n \n"
    },
    {
        "id": 89,
        "reference": "M. Bianchi, A. De Santis, A. Tocchetti, and M. Brambilla, “Interpretable network visualizations: A human-in-the-loop approach for post-hoc explainability of cnn-based image classification,” arXiv preprint arXiv:2405.03301, 2024.",
        "bibtex": {
            "type": "article",
            "citation_key": "bianchi2024interpretable",
            "title": "Interpretable Network Visualizations: A Human-in-the-Loop Approach for Post-hoc Explainability of CNN-based Image Classification",
            "author": [
                "Bianchi, Matteo",
                "De Santis, Antonio",
                "Tocchetti, Andrea",
                "Brambilla, Marco"
            ],
            "journal": "arXiv preprint arXiv:2405.03301",
            "year": 2024
        },
        "citation_count": 0,
        "abstract": "Transparency and explainability in image classification are essential for establishing trust in machine learning models and detecting biases and errors. State-of-the-art explainability methods generate saliency maps to show where a specific class is identified, without providing a detailed explanation of the model's decision process. Striving to address such a need, we introduce a post-hoc method that explains the entire feature extraction process of a Convolutional Neural Network. These explanations include a layer-wise representation of the features the model extracts from the input. Such features are represented as saliency maps generated by clustering and merging similar feature maps, to which we associate a weight derived by generalizing Grad-CAM for the proposed methodology. To further enhance these explanations, we include a set of textual labels collected through a gamified crowdsourcing activity and processed using NLP techniques and Sentence-BERT. Finally, we show an approach to generate global explanations by aggregating labels across multiple images.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=M.+Bianchi%2C+A.+De+Santis%2C+A.+Tocchetti%2C+and+M.+Brambilla%2C+%E2%80%9CInter-+pretable+network+visualizations%3A+A+human-in-the-loop+approach+for+post-hoc+explainability+of+cnn-based+image+classification%2C%E2%80%9D+arXiv+preprint+arXiv%3A2405.03301%2C+2024.&btnG=",
        "external_link": "https://arxiv.org/abs/2405.03301",
        "doi": "https://doi.org/10.48550/arXiv.2405.03301",
        "group": "#c7e9c0",
        "groupName": "Gradient Based Paper",
        "bibTexContent": " \n@article bianchi2024interpretable,\n  title= Interpretable Network Visualizations: A Human-in-the-Loop Approach for Post-hoc Explainability of CNN-based Image Classification ,\n  author= Bianchi, Matteo and De Santis, Antonio and Tocchetti, Andrea and Brambilla, Marco ,\n  journal= arXiv preprint arXiv:2405.03301 ,\n  year= 2024 \n \n"
    },
    {
        "id": 90,
        "reference": "C. J. Anders, D. Neumann, W. Samek, K.-R. Müller, and S. Lapuschkin, “Software for dataset-wide xai: from local explanations to global insights with zennit, corelay, and virelay,” arXiv preprint arXiv:2106.13200, 2021.",
        "bibtex": {
            "type": "article",
            "key": "anders2021software",
            "title": "Software for dataset-wide XAI: from local explanations to global insights with Zennit, CoRelAy, and ViRelAy",
            "author": [
                "Anders, Christopher J",
                "Neumann, David",
                "Samek, Wojciech",
                "Müller, Klaus-Robert",
                "Lapuschkin, Sebastian"
            ],
            "journal": "arXiv preprint arXiv:2106.13200",
            "year": 2021
        },
        "citation_count": 71,
        "abstract": "Deep Neural Networks (DNNs) are known to be strong predictors, but their prediction strategies can rarely be understood. With recent advances in Explainable Artificial Intelligence (XAI), approaches are available to explore the reasoning behind those complex models' predictions. Among post-hoc attribution methods, Layer-wise Relevance Propagation (LRP) shows high performance. For deeper quantitative analysis, manual approaches exist, but without the right tools they are unnecessarily labor intensive. In this software paper, we introduce three software packages targeted at scientists to explore model reasoning using attribution approaches and beyond: (1) Zennit - a highly customizable and intuitive attribution framework implementing LRP and related approaches in PyTorch, (2) CoRelAy - a framework to easily and quickly construct quantitative analysis pipelines for dataset-wide analyses of explanations, and (3) ViRelAy - a web-application to interactively explore data, attributions, and analysis results. With this, we provide a standardized implementation solution for XAI, to contribute towards more reproducibility in our field.",
        "apa": "Anders, C. J., Neumann, D., Samek, W., Müller, K. R., & Lapuschkin, S. (2021). Software for dataset-wide XAI: from local explanations to global insights with Zennit, CoRelAy, and ViRelAy. arXiv preprint arXiv:2106.13200.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=+C.+J.+Anders%2C+D.+Neumann%2C+W.+Samek%2C+K.-R.+M+%C2%A8uller%2C+and+S.+La-+puschkin%2C+%E2%80%9CSoftware+for+dataset-wide+xai%3A+from+local+explanations+to+global+insights+with+zennit%2C+corelay%2C+and+virelay%2C%E2%80%9D+arXiv+preprint+arXiv%3A2106.13200%2C+2021.&btnG=",
        "external_link": "https://arxiv.org/abs/2106.13200",
        "doi": "https://doi.org/10.48550/arXiv.2106.13200",
        "group": "#c7e9c0",
        "groupName": "Decomposition Based Paper",
        "bibTexContent": " \n@article anders2021software,\n  title= Software for dataset-wide XAI: from local explanations to global insights with Zennit, CoRelAy, and ViRelAy ,\n  author= Anders, Christopher J and Neumann, David and Samek, Wojciech and M \"u ller, Klaus-Robert and Lapuschkin, Sebastian ,\n  journal= arXiv preprint arXiv:2106.13200 ,\n  year= 2021 \n \n"
    },
    {
        "id": 91,
        "reference": "H. Park, N. Das, R. Duggal, A. P. Wright, O. Shaikh, F. Hohman, and D. H. P. Chau, “Neurocartography: Scalable automatic visual summarization of concepts in deep neural networks,” IEEE Transactions on Visualization and Computer Graphics, vol. 28, no. 1, pp. 813–823, 2021.",
        "bibtex": {
            "type": "article",
            "key": "park2021neurocartography",
            "title": "Neurocartography: Scalable automatic visual summarization of concepts in deep neural networks",
            "author": [
                "Park, Haekyu",
                "Das, Nilaksh",
                "Duggal, Rahul",
                "Wright, Austin P",
                "Shaikh, Omar",
                "Hohman, Fred",
                "Chau, Duen Horng Polo"
            ],
            "journal": "IEEE Transactions on Visualization and Computer Graphics",
            "volume": 28,
            "number": 1,
            "pages": "813--823",
            "year": 2021,
            "publisher": "IEEE"
        },
        "citation_count": 21,
        "abstract": "Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present Neurocartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. Neurocartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting “dog faces” of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting “dog face” and “dog tail” are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs' relationships, in time linear to the number of neurons instead of quadratic time. Neurocartography scales to large data, such as the ImageNet dataset with 1.2M images. The system's tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The Neurocartography visualization runs in modern browsers and is open-sourced.",
        "keywords": [
            "Deep learning interpretability",
            "visual analytics",
            "scalable summarization",
            "neuron clustering",
            "neuron embedding"
        ],
        "apa": "Park, H., Das, N., Duggal, R., Wright, A. P., Shaikh, O., Hohman, F., & Chau, D. H. P. (2021). Neurocartography: Scalable automatic visual summarization of concepts in deep neural networks. IEEE Transactions on Visualization and Computer Graphics, 28(1), 813-823.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=H.+Park%2C+N.+Das%2C+R.+Duggal%2C+A.+P.+Wright%2C+O.+Shaikh%2C+F.+Hohman%2C+and+D.+H.+P.+Chau%2C+%E2%80%9CNeurocartography%3A+Scalable+automatic+visual+summarization+of+concepts+in+deep+neural+networks%2C%E2%80%9D+IEEE+Transactions+on+Visualization+and+Computer+Graphics%2C+vol.+28%2C+no.+1%2C+pp.+813%E2%80%93823%2C+2021.&btnG=",
        "external_link": "https://ieeexplore.ieee.org/document/9552879",
        "doi": "https://ieeexplore.ieee.org/document/9552879",
        "group": "#f4cccc",
        "groupName": "Concept Based Paper",
        "bibTexContent": " \n@article park2021neurocartography,\n  title= Neurocartography: Scalable automatic visual summarization of concepts in deep neural networks ,\n  author= Park, Haekyu and Das, Nilaksh and Duggal, Rahul and Wright, Austin P and Shaikh, Omar and Hohman, Fred and Chau, Duen Horng Polo ,\n  journal= IEEE Transactions on Visualization and Computer Graphics ,\n  volume= 28 ,\n  number= 1 ,\n  pages= 813--823 ,\n  year= 2021 ,\n  publisher= IEEE \n \n"
    },
    {
        "id": 92,
        "reference": "Y. Li, J. Wang, P. Aboagye, C.-C. M. Yeh, Y. Zheng, L. Wang, W. Zhang, and K.-L. Ma, “Visual analytics for efficient image exploration and user-guided image captioning,” IEEE Transactions on Visualization and Computer Graphics, 2024.",
        "bibtex": {
            "type": "article",
            "key": "li2024visual",
            "title": "Visual Analytics for Efficient Image Exploration and User-Guided Image Captioning",
            "author": [
                "Li, Yiran",
                "Wang, Junpeng",
                "Aboagye, Prince",
                "Yeh, Chin-Chia Michael",
                "Zheng, Yan",
                "Wang, Liang",
                "Zhang, Wei",
                "Ma, Kwan-Liu"
            ],
            "journal": "IEEE Transactions on Visualization and Computer Graphics",
            "year": 2024,
            "publisher": "IEEE"
        },
        "citation_count": 0,
        "abstract": "Recent advancements in pre-trained language-image models have ushered in a new era of visual comprehension. Leveraging the power of these models, this paper tackles two issues within the realm of visual analytics: (1) the efficient exploration of large-scale image datasets and identification of data biases within them; (2) the evaluation of image captions and steering of their generation process. On the one hand, by visually examining the captions generated from language-image models for an image dataset, we gain deeper insights into the visual contents, unearthing data biases that may be entrenched within the dataset. On the other hand, by depicting the association between visual features and textual captions, we expose the weaknesses of pre-trained language-image models in their captioning capability and propose an interactive interface to steer caption generation. The two parts have been coalesced into a coordinated visual analytics system, fostering the mutual enrichment of visual and textual contents. We validate the effectiveness of the system with domain practitioners through concrete case studies with large-scale image datasets.",
        "ieee_keywords": [
            "Visual analytics",
            "Analytical models",
            "Training",
            "Image segmentation",
            "Transformers",
            "Snow",
            "Heating systems"
        ],
        "apa": "Li, Y., Wang, J., Aboagye, P., Yeh, C. C. M., Zheng, Y., Wang, L., ... & Ma, K. L. (2024). Visual Analytics for Efficient Image Exploration and User-Guided Image Captioning. IEEE Transactions on Visualization and Computer Graphics.",
        "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=Y.+Li%2C+J.+Wang%2C+P.+Aboagye%2C+C.-C.+M.+Yeh%2C+Y.+Zheng%2C+L.+Wang%2C+W.+Zhang%2C+and+K.-L.+Ma%2C+%E2%80%9CVisual+analytics+for+efficient+image+exploration+and+user-guided+image+captioning%2C%E2%80%9D+IEEE+Transactions+on+Visualization+and+Computer+Graphics%2C+2024.&btnG=",
        "external_link": "https://ieeexplore.ieee.org/document/10502235",
        "doi": "https://ieeexplore.ieee.org/document/10502235",
        "group": "#f4cccc",
        "groupName": "Concept Based Paper",
        "bibTexContent": " \n@article li2024visual,\n  title= Visual Analytics for Efficient Image Exploration and User-Guided Image Captioning ,\n  author= Li, Yiran and Wang, Junpeng and Aboagye, Prince and Yeh, Chin-Chia Michael and Zheng, Yan and Wang, Liang and Zhang, Wei and Ma, Kwan-Liu ,\n  journal= IEEE Transactions on Visualization and Computer Graphics ,\n  year= 2024 ,\n  publisher= IEEE \n \n"
    }
]